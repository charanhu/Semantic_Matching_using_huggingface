{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sqlite3\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # First element of model_output contains all token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(\n",
    "        -1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Query sentence\n",
    "query_sentence = input(\"Enter question: \")\n",
    "\n",
    "# Connect to database and fetch table names and column names\n",
    "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the filename that is connected above\n",
    "filename = conn.cursor().execute(\"PRAGMA database_list;\").fetchall()[0][2]\n",
    "\n",
    "# filename = '/content/Db-IMDB.db'\n",
    "# split the filename to get the database name\n",
    "database_name = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "table_names = [table_info[0] for table_info in cursor.execute(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
    "column_names = []\n",
    "for table_name in table_names:\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
    "\n",
    "# Tokenize query sentence, table names\n",
    "query_sentence_encoded = tokenizer(\n",
    "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
    "table_names_encoded = tokenizer(\n",
    "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings for query sentence, table names\n",
    "with torch.no_grad():\n",
    "    query_sentence_output = model(**query_sentence_encoded)\n",
    "    table_names_output = model(**table_names_encoded)\n",
    "\n",
    "# Perform pooling for query sentence, table names\n",
    "query_sentence_embedding = mean_pooling(\n",
    "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
    "table_names_embeddings = mean_pooling(\n",
    "    table_names_output, table_names_encoded['attention_mask'])\n",
    "\n",
    "# Normalize embeddings for query sentence, table names\n",
    "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
    "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
    "\n",
    "# Find the most similar table names by computing the cosine similarity between the query sentence embedding and the table names embeddings\n",
    "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
    "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
    "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
    "    descending=True)\n",
    "most_similar_table_names = [table_names[i]\n",
    "                            for i in most_similar_table_names_indices]\n",
    "\n",
    "# Print the most similar table names with there cosine similarity scores in descending order\n",
    "for i in range(len(most_similar_table_names)):\n",
    "    print(\n",
    "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
    "\n",
    "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
    "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
    "\n",
    "# Get the highest matching table name by using the index obtained above\n",
    "highest_matching_table_name = table_names[max_similarity_table_index]\n",
    "\n",
    "# Find the column names of the highest matching table by querying the database\n",
    "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
    "highest_matching_table_column_names = [\n",
    "    column_info[1] for column_info in cursor.fetchall()]\n",
    "\n",
    "# replace column names spaces with underscores\n",
    "highest_matching_table_column_names = [column_name.replace(\n",
    "    ' ', '_') for column_name in highest_matching_table_column_names]\n",
    "\n",
    "# store it in another list\n",
    "highest_matching_table_column_names_copy = highest_matching_table_column_names\n",
    "\n",
    "# Tokenize the column names of the highest matching table\n",
    "highest_matching_table_column_names_encoded = tokenizer(\n",
    "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute the token embeddings for the column names of the highest matching table\n",
    "with torch.no_grad():\n",
    "    highest_matching_table_column_names_output = model(\n",
    "        **highest_matching_table_column_names_encoded)\n",
    "\n",
    "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
    "highest_matching_table_column_names_embeddings = mean_pooling(\n",
    "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
    "\n",
    "# Normalize the embeddings for the column names of the highest matching table\n",
    "highest_matching_table_column_names_embeddings = F.normalize(\n",
    "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
    "\n",
    "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
    "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
    "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
    "\n",
    "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
    "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
    "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
    "    most_similar_highest_matching_table_column_name_index]\n",
    "\n",
    "# Print the most similar column name in the highest matching table\n",
    "print(\"----------------------------------\")\n",
    "print(\n",
    "    f\"Most similar column name in the highest matching table ({highest_matching_table_name}): {most_similar_highest_matching_table_column_name}\")\n",
    "\n",
    "\n",
    "highest_matching_table_column_names = \", \".join(\n",
    "    highest_matching_table_column_names)\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"tscholak/3vnuv1vf\")\n",
    "model_1 = AutoModelForSeq2SeqLM.from_pretrained(\"tscholak/3vnuv1vf\")\n",
    "\n",
    "# Make input text in this format. input_text = \"list names of film released in 2018 and rating more than 6? | IMDB | Movie: rating, year, title\"\n",
    "input_text_1 = query_sentence + \" | \" + database_name + \" | \" + \\\n",
    "    highest_matching_table_name + \": \" + highest_matching_table_column_names\n",
    "\n",
    "input_ids_1 = tokenizer_1.encode(input_text_1, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the output\n",
    "output_1 = model_1.generate(\n",
    "    input_ids_1, max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the output\n",
    "output_text_1 = tokenizer_1.decode(output_1[0], skip_special_tokens=True)\n",
    "# Output: IMDB | select title from movie where rating > 6 and year = 2018\n",
    "\n",
    "# split the output into two parts (sql and table name)\n",
    "output_text_1 = output_text_1.split(\"|\")\n",
    "sql_1 = output_text_1[1].strip()\n",
    "\n",
    "# split the sql into words\n",
    "sql_1 = sql_1.split()\n",
    "\n",
    "# convert the list to lower case\n",
    "highest_matching_table_column_names_copy = [x.lower() for x in highest_matching_table_column_names_copy]\n",
    "\n",
    "sql_query = \"\"\n",
    "\n",
    "for i in s1:\n",
    "    if i in lst:\n",
    "        # write it in duble quotes\n",
    "        s2 = s2 + \" \" + '\"' + i + '\"'\n",
    "    else:\n",
    "        s2 = s2 + \" \" + i\n",
    "\n",
    "print(s2)\n",
    "\n",
    "# remove underscore from the string\n",
    "s2 = s2.replace(\"_\", \" \")\n",
    "\n",
    "print(s2)\n",
    "\n",
    "\n",
    "# Execute the sql\n",
    "cursor.execute(sql_1)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "# print the result\n",
    "print(result)\n",
    "\n",
    "# Close database connection\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
