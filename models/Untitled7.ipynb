{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56f7ec8f7fe344fea4186d807b4641ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6979c4b2eede4aaf9d6a7ccd4d1d4d90",
              "IPY_MODEL_e81b733a46a2457c9a2329e7111d5f1a",
              "IPY_MODEL_64f957b5765d406484225b40dbdf1c2e"
            ],
            "layout": "IPY_MODEL_0829812637fd4780814ea9745c12299d"
          }
        },
        "6979c4b2eede4aaf9d6a7ccd4d1d4d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c06f7b878424d6886b75b728d9b25ca",
            "placeholder": "​",
            "style": "IPY_MODEL_ac49c4394d40410b9709e117663dc219",
            "value": "Downloading: 100%"
          }
        },
        "e81b733a46a2457c9a2329e7111d5f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7334508cbc04489a3ac02d8e1ca3703",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_902b40365fd244348358136fb0f5b9a9",
            "value": 665
          }
        },
        "64f957b5765d406484225b40dbdf1c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd8850c003904f2495f0a4d1f80ad506",
            "placeholder": "​",
            "style": "IPY_MODEL_9564feba93e74a679b6c73609a2aa6da",
            "value": " 665/665 [00:00&lt;00:00, 13.9kB/s]"
          }
        },
        "0829812637fd4780814ea9745c12299d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c06f7b878424d6886b75b728d9b25ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac49c4394d40410b9709e117663dc219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7334508cbc04489a3ac02d8e1ca3703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "902b40365fd244348358136fb0f5b9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd8850c003904f2495f0a4d1f80ad506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9564feba93e74a679b6c73609a2aa6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db04a71bf01e4825a7fe113b9db21ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57881d264ddb423fad029f3bc95e86ab",
              "IPY_MODEL_9143ede7bc1b4eca953a674733a16fe7",
              "IPY_MODEL_4687504556294908a0f721ad1ab4a484"
            ],
            "layout": "IPY_MODEL_4b4e917211d7467d938405070af092d7"
          }
        },
        "57881d264ddb423fad029f3bc95e86ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_438dc7a31aae46ee88ab3e02044470e8",
            "placeholder": "​",
            "style": "IPY_MODEL_318246cf645b41beb61c703cee606ad2",
            "value": "Downloading: 100%"
          }
        },
        "9143ede7bc1b4eca953a674733a16fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e97231d88b8946c8a560b0bff2cd9c3e",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3feaeb873ed4e64b6978dfb5c353f39",
            "value": 548118077
          }
        },
        "4687504556294908a0f721ad1ab4a484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baab95d6adc643c38d914287931983a4",
            "placeholder": "​",
            "style": "IPY_MODEL_1ce0e8e93ccb4bc785ce814938651208",
            "value": " 548M/548M [00:19&lt;00:00, 32.8MB/s]"
          }
        },
        "4b4e917211d7467d938405070af092d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438dc7a31aae46ee88ab3e02044470e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "318246cf645b41beb61c703cee606ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e97231d88b8946c8a560b0bff2cd9c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3feaeb873ed4e64b6978dfb5c353f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baab95d6adc643c38d914287931983a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce0e8e93ccb4bc785ce814938651208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "463055764eba466899d4f6da2c8ddc28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb91cfe6749d47809f7759278579a1b7",
              "IPY_MODEL_366ae2b3245c42c59df1ddf1b7999d9b",
              "IPY_MODEL_9c0c382a8c734d8391aa6874e983349d"
            ],
            "layout": "IPY_MODEL_21094ff6737d48eea20c59afbbf58165"
          }
        },
        "bb91cfe6749d47809f7759278579a1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd05aeee258a443587b0c69c41079573",
            "placeholder": "​",
            "style": "IPY_MODEL_044d75eabd4d42f3a89e735d81ecff48",
            "value": "Downloading: 100%"
          }
        },
        "366ae2b3245c42c59df1ddf1b7999d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f87b16439546a599fcea3be96ba05c",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e585127a3a774612b7dc970ca990cb72",
            "value": 1042301
          }
        },
        "9c0c382a8c734d8391aa6874e983349d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff31ce7c93004311aba2f791a0c4e192",
            "placeholder": "​",
            "style": "IPY_MODEL_f522127a4fef49b684836e1b65cafdd9",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 2.99MB/s]"
          }
        },
        "21094ff6737d48eea20c59afbbf58165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd05aeee258a443587b0c69c41079573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "044d75eabd4d42f3a89e735d81ecff48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31f87b16439546a599fcea3be96ba05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e585127a3a774612b7dc970ca990cb72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff31ce7c93004311aba2f791a0c4e192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f522127a4fef49b684836e1b65cafdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8a769de54a94fc0ac1e4f1593cffdb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a467b8a7e564ea1a23938ac9a17441c",
              "IPY_MODEL_209e1a67288d4f36819aefd95c1842bd",
              "IPY_MODEL_e20a69b858284e71b80bcba1b0f7514d"
            ],
            "layout": "IPY_MODEL_942c51985bdc47d4b98d8d8436601d7c"
          }
        },
        "5a467b8a7e564ea1a23938ac9a17441c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fdba29680a4def9deed78cc2a92418",
            "placeholder": "​",
            "style": "IPY_MODEL_a748ad2fe7ed46e78eba5ed0d3f162ff",
            "value": "Downloading: 100%"
          }
        },
        "209e1a67288d4f36819aefd95c1842bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7d9ee6dfeaa4d028a1dc428a01f8263",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6087eca9c8846a2bf33e80723b68753",
            "value": 456318
          }
        },
        "e20a69b858284e71b80bcba1b0f7514d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e46aa868444344bb266e55b2dbe283",
            "placeholder": "​",
            "style": "IPY_MODEL_05cecc765f344108beecdb36c3c64d31",
            "value": " 456k/456k [00:00&lt;00:00, 552kB/s]"
          }
        },
        "942c51985bdc47d4b98d8d8436601d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fdba29680a4def9deed78cc2a92418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a748ad2fe7ed46e78eba5ed0d3f162ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7d9ee6dfeaa4d028a1dc428a01f8263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6087eca9c8846a2bf33e80723b68753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24e46aa868444344bb266e55b2dbe283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05cecc765f344108beecdb36c3c64d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b954b918af64befbb99d659ecdde8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_785d31be913e4608b3a2aa019bc82403",
              "IPY_MODEL_f16cf505dca14298a01db031c40ca62b",
              "IPY_MODEL_29ab502ac790408a9202bb12b44595fa"
            ],
            "layout": "IPY_MODEL_d09b3d908b3145bf98ad1cbb07bf6def"
          }
        },
        "785d31be913e4608b3a2aa019bc82403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e84a81dea0684215b3dffc7ad175ec76",
            "placeholder": "​",
            "style": "IPY_MODEL_7159447b181e444c8ba3e39bb14cc21f",
            "value": "Downloading: 100%"
          }
        },
        "f16cf505dca14298a01db031c40ca62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9271ef926739455f99157ea2a8c95313",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_460e98a060fa4b13b5e5412dbd3aa6a1",
            "value": 1355256
          }
        },
        "29ab502ac790408a9202bb12b44595fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ea6c3adb4c4d93b25d664566648399",
            "placeholder": "​",
            "style": "IPY_MODEL_9d9b4a78b18140deabeb392a961f8722",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.98MB/s]"
          }
        },
        "d09b3d908b3145bf98ad1cbb07bf6def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84a81dea0684215b3dffc7ad175ec76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7159447b181e444c8ba3e39bb14cc21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9271ef926739455f99157ea2a8c95313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460e98a060fa4b13b5e5412dbd3aa6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6ea6c3adb4c4d93b25d664566648399": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d9b4a78b18140deabeb392a961f8722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a486cc174d394e7189a0ab8e867a4f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e74fb494a084da8811097ca4cacc811",
              "IPY_MODEL_e0866ef9f7744e9c9750ea93e512d228",
              "IPY_MODEL_ed8e467c30014db1a237ef9f1af1219b"
            ],
            "layout": "IPY_MODEL_6fa25c401aa5499080ecd49664227a7e"
          }
        },
        "7e74fb494a084da8811097ca4cacc811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_241de83b953b467984d8a49b57188642",
            "placeholder": "​",
            "style": "IPY_MODEL_63e8c15180fc4fdbb234db6fd59b9274",
            "value": "Downloading: 100%"
          }
        },
        "e0866ef9f7744e9c9750ea93e512d228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c6c1100598a4ae4a226df7e2f8e5f6e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f796888ebee45b1b4809d2b491acf7a",
            "value": 570
          }
        },
        "ed8e467c30014db1a237ef9f1af1219b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b30f71f3da714f1888d030ac7244660d",
            "placeholder": "​",
            "style": "IPY_MODEL_6f5474aa234d4e349952c5aed4017311",
            "value": " 570/570 [00:00&lt;00:00, 15.0kB/s]"
          }
        },
        "6fa25c401aa5499080ecd49664227a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241de83b953b467984d8a49b57188642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63e8c15180fc4fdbb234db6fd59b9274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c6c1100598a4ae4a226df7e2f8e5f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f796888ebee45b1b4809d2b491acf7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b30f71f3da714f1888d030ac7244660d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5474aa234d4e349952c5aed4017311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a58de913f0bf427b91da05eaa93a8e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9498f3a919804e6e85431be6f399a4ba",
              "IPY_MODEL_c5d2efc5c4a347539a4443bfcabab2f2",
              "IPY_MODEL_1ea69daf7c0a4724b903a48fc4b79383"
            ],
            "layout": "IPY_MODEL_ef904123bb1d412dbdb198b2260aa4f9"
          }
        },
        "9498f3a919804e6e85431be6f399a4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f0f9927b5764073a4bbd6aa75b133f3",
            "placeholder": "​",
            "style": "IPY_MODEL_96caa72c25524137a10401d0e3ebdbb7",
            "value": "Downloading: 100%"
          }
        },
        "c5d2efc5c4a347539a4443bfcabab2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f53375975fee4f61959fd2493e44eb31",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a2c5477f010458f9380dac8d2eb972e",
            "value": 435779157
          }
        },
        "1ea69daf7c0a4724b903a48fc4b79383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d5e2ea828db4318a4e267932f6be5b7",
            "placeholder": "​",
            "style": "IPY_MODEL_e98057658c3345e8badd94a2a476471f",
            "value": " 436M/436M [00:13&lt;00:00, 31.2MB/s]"
          }
        },
        "ef904123bb1d412dbdb198b2260aa4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0f9927b5764073a4bbd6aa75b133f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96caa72c25524137a10401d0e3ebdbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53375975fee4f61959fd2493e44eb31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a2c5477f010458f9380dac8d2eb972e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d5e2ea828db4318a4e267932f6be5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98057658c3345e8badd94a2a476471f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46608e8ccab9423cb10f5705702bcbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59cb9efd153b442780f7a61972df6bf7",
              "IPY_MODEL_63539f7d5e364614a2846cf3dcafb4e5",
              "IPY_MODEL_a1568a18595d411eb484c31fadbaeb95"
            ],
            "layout": "IPY_MODEL_e149f1f6fe0a4c51a45ed1414893227d"
          }
        },
        "59cb9efd153b442780f7a61972df6bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bb887b2f41c462d823d57d5e9f9a981",
            "placeholder": "​",
            "style": "IPY_MODEL_959b8a99273a4bf1bac34d678fa93ab4",
            "value": "Downloading: 100%"
          }
        },
        "63539f7d5e364614a2846cf3dcafb4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f04f156192eb440a8ee2f4f46c71d03a",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82165193dda2411cac6ed75c121d92ba",
            "value": 213450
          }
        },
        "a1568a18595d411eb484c31fadbaeb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d152a8b8f95142aa9054e0d8c156fa53",
            "placeholder": "​",
            "style": "IPY_MODEL_f016f586469d4cdfb5abf70b8ac00404",
            "value": " 213k/213k [00:00&lt;00:00, 550kB/s]"
          }
        },
        "e149f1f6fe0a4c51a45ed1414893227d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bb887b2f41c462d823d57d5e9f9a981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959b8a99273a4bf1bac34d678fa93ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f04f156192eb440a8ee2f4f46c71d03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82165193dda2411cac6ed75c121d92ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d152a8b8f95142aa9054e0d8c156fa53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f016f586469d4cdfb5abf70b8ac00404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "966fd06c693e41e1bd685630ccb48154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c8039f306d543bea974c1d5cfb89214",
              "IPY_MODEL_4f8233fe343e4d5a867722b4cd345f11",
              "IPY_MODEL_ca4fe1cc8b6e404786995e5e7cf3e8ae"
            ],
            "layout": "IPY_MODEL_14b519f89c3a4f709a1abc5941a475d5"
          }
        },
        "4c8039f306d543bea974c1d5cfb89214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f6f11638f44db3a1135b2b2f4c8a11",
            "placeholder": "​",
            "style": "IPY_MODEL_d4f99af487024c5a80a7383d38a7fe21",
            "value": "Downloading: 100%"
          }
        },
        "4f8233fe343e4d5a867722b4cd345f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57191384d5d74fe1a72a586b9ae897a7",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_346e2bebe6184411ade03d7fcbdc3124",
            "value": 29
          }
        },
        "ca4fe1cc8b6e404786995e5e7cf3e8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b15f7d5dbb1a475b80309af9861d16b8",
            "placeholder": "​",
            "style": "IPY_MODEL_9a656b11e5844b6aae698db9430c5e44",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.24kB/s]"
          }
        },
        "14b519f89c3a4f709a1abc5941a475d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f6f11638f44db3a1135b2b2f4c8a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f99af487024c5a80a7383d38a7fe21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57191384d5d74fe1a72a586b9ae897a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346e2bebe6184411ade03d7fcbdc3124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b15f7d5dbb1a475b80309af9861d16b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a656b11e5844b6aae698db9430c5e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5030bc8118c743958f32a9e76336ffc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d84011a8b7594ca694ff37e8f64d78a1",
              "IPY_MODEL_7b1a4963110a496eb61a362ae1b766a0",
              "IPY_MODEL_4be47353540a4c6e841c15194a0a8ae2"
            ],
            "layout": "IPY_MODEL_354a3e36bac540bdb01a6ce04e3ade14"
          }
        },
        "d84011a8b7594ca694ff37e8f64d78a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bc5d3ba03834bdabe4d9f9be532adf3",
            "placeholder": "​",
            "style": "IPY_MODEL_364c0595dc9f49d0b62f741c319341a1",
            "value": "Downloading: 100%"
          }
        },
        "7b1a4963110a496eb61a362ae1b766a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f54c7938e5b4ee68332d3f4319f496b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a752ea9e529045089a470ac979c1f1c4",
            "value": 570
          }
        },
        "4be47353540a4c6e841c15194a0a8ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ee1c54cd3d440fae2d7c801541cbd4",
            "placeholder": "​",
            "style": "IPY_MODEL_9504af680c434ad3b17b66b62006fc5f",
            "value": " 570/570 [00:00&lt;00:00, 13.3kB/s]"
          }
        },
        "354a3e36bac540bdb01a6ce04e3ade14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc5d3ba03834bdabe4d9f9be532adf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364c0595dc9f49d0b62f741c319341a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f54c7938e5b4ee68332d3f4319f496b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a752ea9e529045089a470ac979c1f1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13ee1c54cd3d440fae2d7c801541cbd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9504af680c434ad3b17b66b62006fc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a6ea9327b634858b7f043d77e983458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15288bc0ff064f849d6bb491577dee49",
              "IPY_MODEL_8d3f5b9b569249bba284f2776710cd9d",
              "IPY_MODEL_bc4d16a7a011449d84a8c5a75a001db8"
            ],
            "layout": "IPY_MODEL_a35653cfb8ec47e3ac4899d6fd6ba3b3"
          }
        },
        "15288bc0ff064f849d6bb491577dee49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb09f2523ae242948019346dfed16228",
            "placeholder": "​",
            "style": "IPY_MODEL_083ccbaf1fdf47ff95638e7c7e61ff12",
            "value": "Downloading: 100%"
          }
        },
        "8d3f5b9b569249bba284f2776710cd9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0d01a0cb3af4af8994f3477c3c6521a",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b5d2ae421ae4d738ba47061577f7f1a",
            "value": 440473133
          }
        },
        "bc4d16a7a011449d84a8c5a75a001db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d45c9546a01470aa3e1964e2c280bef",
            "placeholder": "​",
            "style": "IPY_MODEL_159d61fe1c5441ce98a012af1c178721",
            "value": " 440M/440M [00:13&lt;00:00, 29.9MB/s]"
          }
        },
        "a35653cfb8ec47e3ac4899d6fd6ba3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb09f2523ae242948019346dfed16228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083ccbaf1fdf47ff95638e7c7e61ff12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0d01a0cb3af4af8994f3477c3c6521a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5d2ae421ae4d738ba47061577f7f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d45c9546a01470aa3e1964e2c280bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159d61fe1c5441ce98a012af1c178721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca85b8e80fb7429f8efdaedaf61806e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cf274e14eaf4e35bf1c7c3a0e213eac",
              "IPY_MODEL_c69862baf5d8435fae75762e90f14ecc",
              "IPY_MODEL_12d5479b783c4d8b857a9e7a1341edbb"
            ],
            "layout": "IPY_MODEL_1bffa87f2dfc4a4d971d40b5beca9d42"
          }
        },
        "3cf274e14eaf4e35bf1c7c3a0e213eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_103533b527bf46fbb1cbb18dac58e3fc",
            "placeholder": "​",
            "style": "IPY_MODEL_cdfa0f406cb54f499975ca60911a6f7c",
            "value": "Downloading: 100%"
          }
        },
        "c69862baf5d8435fae75762e90f14ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd755acb61fb4511aa30879193e6da5e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b81792cc8c84b13b87015252ab93e57",
            "value": 231508
          }
        },
        "12d5479b783c4d8b857a9e7a1341edbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e88cff068b2646f4b33046e8ccd32fbf",
            "placeholder": "​",
            "style": "IPY_MODEL_97add7d7ffcf4183a346720a73237599",
            "value": " 232k/232k [00:00&lt;00:00, 567kB/s]"
          }
        },
        "1bffa87f2dfc4a4d971d40b5beca9d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "103533b527bf46fbb1cbb18dac58e3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfa0f406cb54f499975ca60911a6f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd755acb61fb4511aa30879193e6da5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b81792cc8c84b13b87015252ab93e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e88cff068b2646f4b33046e8ccd32fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97add7d7ffcf4183a346720a73237599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24b99b821fa948d29d8e783c053f070a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3d627bc58c6437298b36cd277bda49f",
              "IPY_MODEL_757a877b524e497883cc5a42f095ae17",
              "IPY_MODEL_8f1bdabba0f84e429c356ef06583b539"
            ],
            "layout": "IPY_MODEL_287df84a5882497abc96d6c82e1a7356"
          }
        },
        "e3d627bc58c6437298b36cd277bda49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c866db67664840a2ec135fedbd2cac",
            "placeholder": "​",
            "style": "IPY_MODEL_92e2db6ec2a944d683195299d07316e5",
            "value": "Downloading: 100%"
          }
        },
        "757a877b524e497883cc5a42f095ae17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89344cefc0af4d92b75900790f795820",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4f6195cdc554e0ca60ab59a77352f3f",
            "value": 28
          }
        },
        "8f1bdabba0f84e429c356ef06583b539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1498fc88dbb442f5b0fa4d2d9f422e78",
            "placeholder": "​",
            "style": "IPY_MODEL_c9dd0a4d000242ac9631d9d45caae116",
            "value": " 28.0/28.0 [00:00&lt;00:00, 439B/s]"
          }
        },
        "287df84a5882497abc96d6c82e1a7356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c866db67664840a2ec135fedbd2cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e2db6ec2a944d683195299d07316e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89344cefc0af4d92b75900790f795820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4f6195cdc554e0ca60ab59a77352f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1498fc88dbb442f5b0fa4d2d9f422e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9dd0a4d000242ac9631d9d45caae116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za9AuD47g5BM",
        "outputId": "dc50f8f2-d9ec-4018-c509-fb1cd23b5dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3n2_aecgTfk",
        "outputId": "0f839cfb-83d5-46c2-a893-def69ee6907f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Paris is the captial of France', 'France is a country in Europe.', 'capital of France is Paris.']\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import nltk\n",
        "import transformers\n",
        "import torch\n",
        "import scipy.spatial.distance\n",
        "import numpy\n",
        "\n",
        "# Preprocess and clean the text data\n",
        "def preprocess_text(text):\n",
        "  # Remove stop words and stem or lemmatize the words\n",
        "  processed_text = [stemmer.stem(word) for word in text if word not in stopwords]\n",
        "  return processed_text\n",
        "\n",
        "# Tokenize the text data\n",
        "def tokenize_text(text):\n",
        "  # Use the BERT tokenizer to tokenize the text\n",
        "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  return tokenized_text\n",
        "\n",
        "# Generate word embeddings for the text data\n",
        "def generate_embeddings(text):\n",
        "  # Convert the text to a sequence of tokens\n",
        "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
        "\n",
        "  # Generate word embeddings for the tokens\n",
        "  model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "  with torch.no_grad():\n",
        "    embeddings = model(input_ids)[0]\n",
        "  return embeddings.flatten()\n",
        "\n",
        "# Compute the similarity between the query and the text data\n",
        "def compute_similarity(query_embeddings, text_embeddings):\n",
        "  # Pad the shorter embeddings with zeros to make them the same length\n",
        "  if len(query_embeddings) > len(text_embeddings):\n",
        "    text_embeddings = numpy.pad(text_embeddings, (0, len(query_embeddings) - len(text_embeddings)), 'constant')\n",
        "  elif len(text_embeddings) > len(query_embeddings):\n",
        "    query_embeddings = numpy.pad(query_embeddings, (0, len(text_embeddings) - len(query_embeddings)), 'constant')\n",
        "\n",
        "  # Compute the cosine similarity between the query and text embeddings\n",
        "  similarity = scipy.spatial.distance.cosine(query_embeddings, text_embeddings)\n",
        "  return similarity\n",
        "\n",
        "# Rank the results based on similarity\n",
        "def rank_results(query, results):\n",
        "  # Generate word embeddings for the query\n",
        "  query_embeddings = generate_embeddings(query)\n",
        "\n",
        "  # Compute the similarity between the query and each result\n",
        "  similarities = []\n",
        "  for result in results:\n",
        "    result_embeddings = generate_embeddings(result)\n",
        "    similarity = compute_similarity(query_embeddings, result_embeddings)\n",
        "    similarities.append(similarity)\n",
        "\n",
        "  # Sort the results by their similarity to the query\n",
        "  sorted_results = [result for _, result in sorted(zip(similarities, results), key=lambda x: x[0])]\n",
        "  return sorted_results\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the capital of France?\"\n",
        "results = [\"Paris is the captial of France\", \"capital of France is Paris.\", \"France is a country in Europe.\"]\n",
        "ranked_results = rank_results(query, results)\n",
        "print(ranked_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"What is the capital of France?\"\n",
        "results = [\"CAPITAL of France is Paris.\", \"Paris is in France.\", \"France is a country in Europe.\"]\n",
        "ranked_results = rank_results(query, results)\n",
        "print(ranked_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "1yPw9SRXiiDU",
        "outputId": "12bca805-eda6-4c7d-f567-ca447c1f6b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cf944ef0e501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the capital of France?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CAPITAL of France is Paris.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Paris is in France.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"France is a country in Europe.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mranked_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-71922252a7ba>\u001b[0m in \u001b[0;36mrank_results\u001b[0;34m(query, results)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mresult_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-71922252a7ba>\u001b[0m in \u001b[0;36mcompute_similarity\u001b[0;34m(query_embeddings, text_embeddings)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# Compute the cosine similarity between the query and text embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# clamp the result to 0-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0muv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m     \u001b[0muu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6912,) (6144,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the BERT model\n",
        "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the CSV file as a Pandas DataFrame\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Get the list of column names in the DataFrame\n",
        "column_names = df.columns.tolist()\n",
        "\n",
        "# Preprocess the natural language question\n",
        "def preprocess_text(text):\n",
        "  # Tokenize the text using the BERT tokenizer\n",
        "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "  # Convert the tokenized text to a sequence of input IDs\n",
        "  input_ids = torch.tensor([tokenizer.encode(tokenized_text, add_special_tokens=True)])\n",
        "\n",
        "  # Generate word embeddings for the input tokens\n",
        "  with torch.no_grad():\n",
        "    embeddings = model(input_ids)[0]\n",
        "\n",
        "  # Average the word embeddings to get a single embedding for the question\n",
        "  question_embedding = embeddings.mean(dim=1)\n",
        "\n",
        "  return question_embedding\n",
        "\n",
        "# Find the matching attributes for the natural language question\n",
        "def find_matching_attributes(question, column_names):\n",
        "  # Preprocess the question\n",
        "  question_embedding = preprocess_text(question)\n",
        "\n",
        "  # Compute the cosine similarity between the question embedding and the embeddings of the column names\n",
        "  similarities = []\n",
        "  for column_name in column_names:\n",
        "    column_name_embedding = preprocess_text(column_name)\n",
        "    similarity = torch.nn.functional.cosine_similarity(question_embedding, column_name_embedding, dim=0)\n",
        "    similarities.append(similarity)\n",
        "\n",
        "  # Sort the column names by their similarity to the question\n",
        "  sorted_column_names = [column_name for _, column_name in sorted(zip(similarities, column_names), key=lambda x: x[0], reverse=True)]\n",
        "\n",
        "  return sorted_column_names\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the total sales?\"\n",
        "matching_attributes = find_matching_attributes(question, column_names)\n",
        "print(matching_attributes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "AxegujM_owpv",
        "outputId": "3513addf-cece-40f3-9ffe-5e14ec4e48eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-86e92181aaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the total sales?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmatching_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_matching_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_attributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-86e92181aaeb>\u001b[0m in \u001b[0;36mfind_matching_attributes\u001b[0;34m(question, column_names)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;31m# Sort the column names by their similarity to the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0msorted_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted_column_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import transformers\n",
        "\n",
        "# Preprocess and clean the text data\n",
        "def preprocess_text(text):\n",
        "  # Remove punctuation and make the text lowercase\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "# Tokenize the text data\n",
        "def tokenize_text(text):\n",
        "  # Use the BERT tokenizer to tokenize the text\n",
        "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  return tokenized_text\n",
        "\n",
        "# Generate word embeddings for the text data\n",
        "def generate_embeddings(text):\n",
        "  # Convert the text to a sequence of tokens\n",
        "  tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
        "\n",
        "  # Generate word embeddings for the tokens\n",
        "  model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "  with torch.no_grad():\n",
        "    embeddings = model(input_ids)[0]\n",
        "  return embeddings.flatten()\n",
        "\n",
        "import scipy.spatial.distance\n",
        "\n",
        "# Compute the similarity between two sets of word embeddings\n",
        "def compute_similarity(embeddings1, embeddings2):\n",
        "  # Pad the shorter embeddings with zeros to make them the same length\n",
        "  if len(embeddings1) > len(embeddings2):\n",
        "    embeddings2 = numpy.pad(embeddings2, (0, len(embeddings1) - len(embeddings2)), 'constant')\n",
        "  elif len(embeddings2) > len(embeddings1):\n",
        "    embeddings1 = numpy.pad(embeddings1, (0, len(embeddings2) - len(embeddings1)), 'constant')\n",
        "\n",
        "  # Compute the cosine similarity between the embeddings\n",
        "  similarity = scipy.spatial.distance.cosine(embeddings1, embeddings2)\n",
        "  return similarity\n",
        "\n",
        "\n",
        "# Search for the matching attributes in the CSV file\n",
        "def search_attributes(question, csv_file):\n",
        "  # Preprocess and tokenize the question\n",
        "  question = preprocess_text(question)\n",
        "  question_tokens = tokenize_text(question)\n",
        "\n",
        "  # Generate word embeddings for the question\n",
        "  question_embeddings = generate_embeddings(question)\n",
        "\n",
        "  # Initialize a list to store the matching attributes\n",
        "  matching_attributes = []\n",
        "\n",
        "  # Open the CSV file and read the header row\n",
        "  with open(csv_file) as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "\n",
        "  # Iterate over the header row and compute the similarity between each attribute and the question\n",
        "  for attribute in header:\n",
        "    # Preprocess and tokenize the attribute\n",
        "    attribute = preprocess_text(attribute)\n",
        "    attribute_tokens = tokenize_text(attribute)\n",
        "\n",
        "    # Generate word embeddings for the attribute\n",
        "    attribute_embeddings = generate_embeddings(attribute)\n",
        "\n",
        "    # Compute the similarity between the attribute and the question\n",
        "    similarity = compute_similarity(question_embeddings, attribute_embeddings)\n",
        "\n",
        "    # If the similarity is above a certain threshold, add the attribute to the list of matching attributes\n",
        "    if similarity > 0.5:\n",
        "      matching_attributes.append(attribute)\n",
        "\n",
        "  return matching_attributes\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the total sales by city?\"\n",
        "csv_file = \"/content/train.csv\"\n",
        "attributes = search_attributes(question, csv_file)\n",
        "print(attributes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG_XDvE3o5s-",
        "outputId": "45589a95-2b5b-41d2-bf81-b62e9402c835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['row id', 'order id', 'order date', 'ship date', 'ship mode', 'customer id', 'customer name', 'segment', 'country', 'city', 'state', 'postal code', 'region', 'product id', 'category', 'subcategory', 'product name', 'sales']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import sqlite3\n",
        "\n",
        "# Connect to the database\n",
        "try:\n",
        "    conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "except sqlite3.Error as e:\n",
        "    print(f\"Error connecting to the database: {e}\")\n",
        "\n",
        "# Execute a SELECT statement to retrieve the data\n",
        "try:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT * FROM Person\")\n",
        "except sqlite3.Error as e:\n",
        "    print(f\"Error executing the SELECT statement: {e}\")\n",
        "\n",
        "# Use the CSV library to write the results to a CSV file\n",
        "try:\n",
        "    with open(\"dataset.csv\", \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(cursor)\n",
        "except Exception as e:\n",
        "    print(f\"Error writing to the CSV file: {e}\")\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "RF4bQEeAqPmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Create a temporary feature vector by obtaining the unique values of each attribute\n",
        "df['feature'] = df['attribute_name'].map(str) + '_' + df['data_value'].map(str)\n",
        "\n",
        "# Split, tokenize, and one-hot encode the feature vector\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Split the feature vector into words and n-grams\n",
        "# You can adjust the value of n based on the length of the words in the feature vector\n",
        "df['feature'] = df['feature'].apply(lambda x: [x[i:i+n] for i in range(len(x)-n+1)])\n",
        "\n",
        "# Flatten the list of n-grams into a single list\n",
        "df['feature'] = df['feature'].apply(lambda x: [item for sublist in x for item in sublist])\n",
        "\n",
        "# Tokenize the list of words and n-grams\n",
        "label_encoder = LabelEncoder()\n",
        "df['feature'] = df['feature'].apply(lambda x: label_encoder.fit_transform(x))\n",
        "\n",
        "# One-hot encode the tokenized list\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "df['feature'] = df['feature'].apply(lambda x: one_hot_encoder.fit_transform(x.reshape(-1, 1)))\n",
        "\n",
        "# Randomly sample from the feature vector for augmentation\n",
        "from random import sample\n",
        "\n",
        "# Determine the range of tokens to randomly select\n",
        "# You can adjust this value based on the length of\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "JVsFakjqKPAz",
        "outputId": "bc6210ef-f6ef-4192-91bc-5957789d3496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'attribute_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d3b75b30cee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a temporary feature vector by obtaining the unique values of each attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attribute_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Split, tokenize, and one-hot encode the feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'attribute_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Load the English language model in Spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a custom Spacy matcher to identify key entities and relationships in the question\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add('AVG', None, [{'LOWER': 'average'}, {'LOWER': 'of'}, {'LOWER': 'the'}, {'LOWER': 'sales'}])\n",
        "matcher.add('SALES_BY_CATEGORY', None, [{'LOWER': 'total'}, {'LOWER': 'sales'}, {'LOWER': 'by'}, {'LOWER': 'category'}])\n",
        "\n",
        "# Example question: \"What is the average of the sales?\"\n",
        "question = \"What is the average of the sales?\"\n",
        "\n",
        "# Tokenize and parse the question using the Spacy model\n",
        "doc = nlp(question)\n",
        "\n",
        "# Use the matcher to find the relevant entities and relationships in the question\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Extract the most relevant match and generate the appropriate SQL query\n",
        "if matches[0][1] == 'AVG':\n",
        "  query = \"SELECT AVG(Sales) FROM data\"\n",
        "elif matches[0][1] == 'SALES_BY_CATEGORY':\n",
        "  query = \"SELECT Category, SUM(Sales) FROM data GROUP BY Category\"\n",
        "\n",
        "# Execute the query and print the results\n",
        "result = pd.read_sql(query, con)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "g3nyFoTnKtBj",
        "outputId": "d83ca548-55b9-47eb-a385-7548f321e1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6e98f7feaf8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Define a custom Spacy matcher to identify key entities and relationships in the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AVG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'average'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'of'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'sales'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SALES_BY_CATEGORY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'total'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'sales'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'by'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOWER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/matcher/matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: add() takes exactly 2 positional arguments (3 given)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weCDJzXWXr59",
        "outputId": "4fa74624-10ae-4548-d57c-2c483d9ee809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 80.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Instantiate the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set the device to use for PyTorch tensors\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Preprocess the natural language question\n",
        "question = \"What is the column name for the product ID?\"\n",
        "input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n",
        "\n",
        "# Iterate over the column names and compute the similarity scores\n",
        "scores = {}\n",
        "for col in df.columns:\n",
        "  # Preprocess the column name\n",
        "  col_input_ids = tokenizer.encode(col, return_tensors='pt').to(device)\n",
        "  # Compute the similarity score\n",
        "  score = model(col_input_ids, input_ids).squeeze()\n",
        "  scores[col] = score\n",
        "\n",
        "# Select the column name with the highest score\n",
        "best_match = max(scores, key=scores.get)\n",
        "print(f\"The column name that best matches the question is: {best_match}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "YNG-KYzjXnG2",
        "outputId": "e4d77e1d-e7c2-4207-f3d8-0eb886a1123b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d164a23ed78c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mcol_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# Compute the similarity score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to search the database for matching column names\n",
        "cursor.execute(\"SELECT name FROM PRAGMA_TABLE_INFO('tablename')\")\n",
        "columns = cursor.fetchall()\n",
        "\n",
        "print(\"\\nMatching columns:\")\n",
        "for column in columns:\n",
        "    print(column[0])\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7WSZwryYLFs",
        "outputId": "963bfb4b-ecb7-44bc-8b31-b825789e4dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "\n",
            "Matching columns:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to search the database for matching column names and rows in the \"movies\" table\n",
        "cursor.execute(\"SELECT * FROM movie WHERE Genre='comedy'\")\n",
        "movies = cursor.fetchall()\n",
        "\n",
        "print(\"\\nComedy movies:\")\n",
        "for movie in movies:\n",
        "    print(movie)\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "wBW-hePmbiIQ",
        "outputId": "9b157a78-3dfa-477b-f8f6-7480f893e3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3daa61d261df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Use the semantic representation to search the database for matching column names and rows in the \"movies\" table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM movie WHERE Genre='comedy'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such column: Genre"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJXWGHX8cX0u",
        "outputId": "8432a788-07ab-469a-f269-e28f8cb25f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# for the question print only semantically matching all the matching tables and columns in the tables\n",
        "print(\"Matching tables and columns for the question:\")\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    for column in columns:\n",
        "        input_ids = torch.tensor(tokenizer.encode(column)).unsqueeze(0)  # Batch size 1\n",
        "        output = model(input_ids)\n",
        "        print(\"Column \" + column + \" in table \" + table[0] + \" is semantically matching the question\")\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHoWKsvXcYbd",
        "outputId": "448c7137-4ba6-4e1f-fbaa-ac0006d1d1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Matching tables and columns for the question:\n",
            "Column index in table Movie is semantically matching the question\n",
            "Column MID in table Movie is semantically matching the question\n",
            "Column title in table Movie is semantically matching the question\n",
            "Column year in table Movie is semantically matching the question\n",
            "Column rating in table Movie is semantically matching the question\n",
            "Column num_votes in table Movie is semantically matching the question\n",
            "Column index in table Genre is semantically matching the question\n",
            "Column Name in table Genre is semantically matching the question\n",
            "Column GID in table Genre is semantically matching the question\n",
            "Column index in table Language is semantically matching the question\n",
            "Column Name in table Language is semantically matching the question\n",
            "Column LAID in table Language is semantically matching the question\n",
            "Column index in table Country is semantically matching the question\n",
            "Column Name in table Country is semantically matching the question\n",
            "Column CID in table Country is semantically matching the question\n",
            "Column index in table Location is semantically matching the question\n",
            "Column Name in table Location is semantically matching the question\n",
            "Column LID in table Location is semantically matching the question\n",
            "Column index in table M_Location is semantically matching the question\n",
            "Column MID in table M_Location is semantically matching the question\n",
            "Column LID in table M_Location is semantically matching the question\n",
            "Column ID in table M_Location is semantically matching the question\n",
            "Column index in table M_Country is semantically matching the question\n",
            "Column MID in table M_Country is semantically matching the question\n",
            "Column CID in table M_Country is semantically matching the question\n",
            "Column ID in table M_Country is semantically matching the question\n",
            "Column index in table M_Language is semantically matching the question\n",
            "Column MID in table M_Language is semantically matching the question\n",
            "Column LAID in table M_Language is semantically matching the question\n",
            "Column ID in table M_Language is semantically matching the question\n",
            "Column index in table M_Genre is semantically matching the question\n",
            "Column MID in table M_Genre is semantically matching the question\n",
            "Column GID in table M_Genre is semantically matching the question\n",
            "Column ID in table M_Genre is semantically matching the question\n",
            "Column index in table Person is semantically matching the question\n",
            "Column PID in table Person is semantically matching the question\n",
            "Column Name in table Person is semantically matching the question\n",
            "Column Gender in table Person is semantically matching the question\n",
            "Column index in table M_Producer is semantically matching the question\n",
            "Column MID in table M_Producer is semantically matching the question\n",
            "Column PID in table M_Producer is semantically matching the question\n",
            "Column ID in table M_Producer is semantically matching the question\n",
            "Column index in table M_Director is semantically matching the question\n",
            "Column MID in table M_Director is semantically matching the question\n",
            "Column PID in table M_Director is semantically matching the question\n",
            "Column ID in table M_Director is semantically matching the question\n",
            "Column index in table M_Cast is semantically matching the question\n",
            "Column MID in table M_Cast is semantically matching the question\n",
            "Column PID in table M_Cast is semantically matching the question\n",
            "Column ID in table M_Cast is semantically matching the question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# Use semantic serach to find only matching columns. Example: \"list all comedy movies\"\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    for column in columns:\n",
        "        input_ids = torch.tensor(tokenizer.encode(column)).unsqueeze(0)  # Batch size 1\n",
        "        output = model(input_ids)\n",
        "        if output[0][0][0][0] == output[0][0][0][1]:\n",
        "            print(\"Column \" + column + \" in table \" + table[0] + \" matches the question.\")\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_MWxV2GdKL9",
        "outputId": "f73076d8-1981-48c7-f868-f2339f8d7d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# for english question generate sql query using BERT model and semantic search in database\n",
        "print(\"SQL query:\")\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    for column in columns:\n",
        "        if column == \"genre\":\n",
        "            print(\"SELECT * FROM \" + table[0] + \" WHERE \" + column + \" = 'Comedy';\")\n",
        "\n",
        "            \n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuUjn6LHd8ym",
        "outputId": "0b761e4a-a74e-4280-bc79-16f086a03607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "SQL query:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# function to convert question into sql query using semantic representation, bert model, tokenizer and database connection.\n",
        "def question_to_sql(question, model, tokenizer, conn):\n",
        "    # Preprocess the English question\n",
        "    input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "    # Generate a semantic representation of the question\n",
        "    output = model(input_ids)\n",
        "\n",
        "    # Use the semantic representation to search the database for matching table names\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = cursor.fetchall()\n",
        "\n",
        "    # Use the semantic representation to print all columns in the matching tables\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "\n",
        "    # Use the semantic representation to generate a SQL query\n",
        "    sql_query = \"SELECT * FROM \" + tables[0][0] + \" WHERE \" + columns[0] + \" = \" + question\n",
        "    print(\"SQL query:\")\n",
        "    print(sql_query)\n",
        "\n",
        "    # Use the SQL query to retrieve the results from the database\n",
        "    cursor.execute(sql_query)\n",
        "    results = cursor.fetchall()\n",
        "    print(\"Results:\")\n",
        "    print(results)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Use the function to convert the question into a SQL query\n",
        "results = question_to_sql(question, model, tokenizer, conn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ad6Zn7-ke7z2",
        "outputId": "a43ad1de-9bbd-4630-89bb-5336834324c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "SQL query:\n",
            "SELECT * FROM Movie WHERE index = What are the names of the tables in the database and list all comedy movies?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ac0c69066051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Use the function to convert the question into a SQL query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_to_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ac0c69066051>\u001b[0m in \u001b[0;36mquestion_to_sql\u001b[0;34m(question, model, tokenizer, conn)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Use the SQL query to retrieve the results from the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: near \"index\": syntax error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# function to convert question into enriched question using semantic representation of question and table names and columns in the database \n",
        "def enrich_question(question, output, tables, columns):\n",
        "    # Get the semantic representation of the question\n",
        "    question_embedding = output[0][0][0]\n",
        "\n",
        "    # Get the semantic representation of the table names\n",
        "    table_embeddings = []\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        table_embeddings.append(table_embedding)\n",
        "\n",
        "    # Get the semantic representation of the columns\n",
        "    column_embeddings = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        column_embeddings.append(column_embedding)\n",
        "\n",
        "    # Find the table name that is most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_table = \"\"\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, table_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_table = table[0]\n",
        "\n",
        "    # Find the columns that are most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_columns = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, column_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_columns = [column]\n",
        "        elif similarity == max_similarity:\n",
        "            max_similarity_columns.append(column)\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in max_similarity_columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "# Use enriched question to search the database for matching rows\n",
        "enriched_question = enrich_question(question, output, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43BvCxWZfm9O",
        "outputId": "e36321f4-4b62-4309-9dda-4df0f0a4fd28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enriched question:\n",
            "SELECT index, MID, ID FROM Movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# function to convert question into enriched question using semantic representation of question and table names and columns in the database \n",
        "def enrich_question(question, output, tables, columns):\n",
        "    # Get the semantic representation of the question\n",
        "    question_embedding = output[0][0][0]\n",
        "\n",
        "    # Get the semantic representation of the table names\n",
        "    table_embeddings = []\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        table_embeddings.append(table_embedding)\n",
        "\n",
        "    # Get the semantic representation of the columns\n",
        "    column_embeddings = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        column_embeddings.append(column_embedding)\n",
        "\n",
        "    # Find the table name that is most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_table = \"\"\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, table_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_table = table[0]\n",
        "\n",
        "    # Find the columns that are most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_columns = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, column_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_columns = [column]\n",
        "        elif similarity == max_similarity:\n",
        "            max_similarity_columns.append(column)\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in max_similarity_columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "# Use enriched question to search the database for matching rows\n",
        "enriched_question = enrich_question(question, output, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "# LSTM function to generate more accurate enriched question using enriched question and question\n",
        "def lstm(question, enriched_question):\n",
        "    # Tokenize the question and enriched question\n",
        "    tokenized_question = tokenizer.tokenize(question)\n",
        "    tokenized_enriched_question = tokenizer.tokenize(enriched_question)\n",
        "\n",
        "    # Create the input and output sequences\n",
        "    input_sequence = [\"[CLS]\"] + tokenized_question + [\"[SEP]\"]\n",
        "    output_sequence = tokenized_enriched_question + [\"[SEP]\"]\n",
        "\n",
        "    # Create the input and output sequences' indices\n",
        "    input_sequence_indices = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "    output_sequence_indices = tokenizer.convert_tokens_to_ids(output_sequence)\n",
        "\n",
        "    # Create the input and output sequences' tensors\n",
        "    input_sequence_tensor = torch.tensor([input_sequence_indices])\n",
        "    output_sequence_tensor = torch.tensor([output_sequence_indices])\n",
        "\n",
        "    # Create the attention mask\n",
        "    attention_mask = torch.ones(input_sequence_tensor.shape, dtype=torch.long)\n",
        "    attention_mask[attention_mask == 0] = tokenizer.convert_tokens_to_ids(['[PAD]'])\n",
        "\n",
        "    # Generate the enriched question\n",
        "    outputs = model(input_sequence_tensor, attention_mask=attention_mask, decoder_input_ids=output_sequence_tensor)\n",
        "    next_token_logits = outputs[0][0, -1, :]\n",
        "\n",
        "    # Get the most likely next token\n",
        "    next_token = torch.argmax(next_token_logits)\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = tokenizer.decode(input_sequence_tensor[0].tolist() + [next_token])\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "# Use LSTM to generate more accurate enriched question\n",
        "enriched_question = lstm(question, enriched_question)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6hlU7gWAgPLd",
        "outputId": "5a1444f3-f4fc-48b6-bf53-9d0addebe48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enriched question:\n",
            "SELECT PID FROM M_Location\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-029583df19e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Use LSTM to generate more accurate enriched question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0menriched_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menriched_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enriched question:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menriched_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-029583df19e9>\u001b[0m in \u001b[0;36mlstm\u001b[0;34m(question, enriched_question)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Create the attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Generate the enriched question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't assign a list to a torch.LongTensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# function to create a enriched natural language question using question, Matching tables and columns in the database\n",
        "\n",
        "\n",
        "def enrich_question(question, tables, columns):\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" FROM \"\n",
        "    for table in tables:\n",
        "        enriched_question += table[0] + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" WHERE \" + question\n",
        "    return enriched_question\n",
        "\n",
        "# Display the enriched question\n",
        "enriched_question = enrich_question(question, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "\n",
        "# # function to convert question into enriched question using semantic representation of question and table names and columns in the database\n",
        "# def enrich_question(question, output, tables, columns):\n",
        "#     # Get the semantic representation of the question\n",
        "#     question_embedding = output[0][0][0]\n",
        "\n",
        "#     # Get the semantic representation of the table names\n",
        "#     table_embeddings = []\n",
        "#     for table in tables:\n",
        "#         table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "#         table_embeddings.append(table_embedding)\n",
        "\n",
        "#     # Get the semantic representation of the columns\n",
        "#     column_embeddings = []\n",
        "#     for column in columns:\n",
        "#         column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "#         column_embeddings.append(column_embedding)\n",
        "\n",
        "#     # Find the table name that is most similar to the semantic representation of the question\n",
        "#     max_similarity = 0\n",
        "#     max_similarity_table = \"\"\n",
        "#     for table in tables:\n",
        "#         table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "#         similarity = torch.cosine_similarity(question_embedding, table_embedding, dim=0)\n",
        "#         if similarity > max_similarity:\n",
        "#             max_similarity = similarity\n",
        "#             max_similarity_table = table[0]\n",
        "\n",
        "#     # Find the columns that are most similar to the semantic representation of the question\n",
        "#     max_similarity = 0\n",
        "#     max_similarity_columns = []\n",
        "#     for column in columns:\n",
        "#         column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "#         similarity = torch.cosine_similarity(question_embedding, column_embedding, dim=0)\n",
        "#         if similarity > max_similarity:\n",
        "#             max_similarity = similarity\n",
        "#             max_similarity_columns = [column]\n",
        "#         elif similarity == max_similarity:\n",
        "#             max_similarity_columns.append(column)\n",
        "\n",
        "#     # Create the enriched question\n",
        "#     enriched_question = \"SELECT \"\n",
        "#     for column in max_similarity_columns:\n",
        "#         enriched_question += column + \", \"\n",
        "#     enriched_question = enriched_question[:-2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "#     return enriched_question\n",
        "\n",
        "\n",
        "# Use enriched question to search the database for matching rows\n",
        "#enriched_question = enrich_question(question, output, tables, columns)\n",
        "#print(\"Enriched question:\")\n",
        "#print(enriched_question)\n",
        "\n",
        "# LSTM function to generate more accurate enriched question using enriched question and question\n",
        "\n",
        "\n",
        "# def lstm(question, enriched_question):\n",
        "#     # Tokenize the question and enriched question\n",
        "#     tokenized_question = tokenizer.tokenize(question)\n",
        "#     tokenized_enriched_question = tokenizer.tokenize(enriched_question)\n",
        "\n",
        "#     # Create the input and output sequences\n",
        "#     input_sequence = [\"[CLS]\"] + tokenized_question + [\"[SEP]\"]\n",
        "#     output_sequence = tokenized_enriched_question + [\"[SEP]\"]\n",
        "\n",
        "#     # Create the input and output sequences' indices\n",
        "#     input_sequence_indices = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "#     output_sequence_indices = tokenizer.convert_tokens_to_ids(output_sequence)\n",
        "\n",
        "#     # Create the input and output sequences' tensors\n",
        "#     input_sequence_tensor = torch.tensor([input_sequence_indices])\n",
        "#     output_sequence_tensor = torch.tensor([output_sequence_indices])\n",
        "\n",
        "#     # Create the attention mask\n",
        "#     attention_mask = torch.ones(input_sequence_tensor.shape, dtype=torch.long)\n",
        "#     attention_mask[attention_mask == 0] = tokenizer.convert_tokens_to_ids([\n",
        "#                                                                           '[PAD]'])\n",
        "\n",
        "#     # Generate the enriched question\n",
        "#     outputs = model(input_sequence_tensor, attention_mask=attention_mask,\n",
        "#                     decoder_input_ids=output_sequence_tensor)\n",
        "#     next_token_logits = outputs[0][0, -1, :]\n",
        "\n",
        "#     # Get the most likely next token\n",
        "#     next_token = torch.argmax(next_token_logits)\n",
        "\n",
        "#     # Create the enriched question\n",
        "#     enriched_question = tokenizer.decode(\n",
        "#         input_sequence_tensor[0].tolist() + [next_token])\n",
        "\n",
        "#     return enriched_question\n",
        "\n",
        "\n",
        "# # Use LSTM to generate more accurate enriched question\n",
        "# enriched_question = lstm(question, enriched_question)\n",
        "# print(\"Enriched question:\")\n",
        "# print(enriched_question)\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwq6EEEFh7AO",
        "outputId": "bd8f31f5-b2ec-4e8e-80de-bf2d1fe08af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enriched question:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"List all the comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Get all the tables in the database and their columns and print them\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "for table in tables:\n",
        "    cursor.execute(\"PRAGMA table_info({})\".format(table[0]))\n",
        "    print(table[0], cursor.fetchall())\n",
        "    \n",
        "\n",
        "# close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25kzJ22nlL9I",
        "outputId": "3416b231-878b-49d3-9cb7-28e49909c818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'title', 'TEXT', 0, None, 0), (3, 'year', 'TEXT', 0, None, 0), (4, 'rating', 'REAL', 0, None, 0), (5, 'num_votes', 'INTEGER', 0, None, 0)]\n",
            "Genre [(0, 'index', 'INTEGER', 0, None, 0), (1, 'Name', 'TEXT', 0, None, 0), (2, 'GID', 'INTEGER', 0, None, 0)]\n",
            "Language [(0, 'index', 'INTEGER', 0, None, 0), (1, 'Name', 'TEXT', 0, None, 0), (2, 'LAID', 'INTEGER', 0, None, 0)]\n",
            "Country [(0, 'index', 'INTEGER', 0, None, 0), (1, 'Name', 'TEXT', 0, None, 0), (2, 'CID', 'INTEGER', 0, None, 0)]\n",
            "Location [(0, 'index', 'INTEGER', 0, None, 0), (1, 'Name', 'TEXT', 0, None, 0), (2, 'LID', 'INTEGER', 0, None, 0)]\n",
            "M_Location [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'LID', 'REAL', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n",
            "M_Country [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'CID', 'REAL', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n",
            "M_Language [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'LAID', 'INTEGER', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n",
            "M_Genre [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'GID', 'INTEGER', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n",
            "Person [(0, 'index', 'INTEGER', 0, None, 0), (1, 'PID', 'TEXT', 0, None, 0), (2, 'Name', 'TEXT', 0, None, 0), (3, 'Gender', 'TEXT', 0, None, 0)]\n",
            "M_Producer [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'PID', 'TEXT', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n",
            "M_Director [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'PID', 'TEXT', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n",
            "M_Cast [(0, 'index', 'INTEGER', 0, None, 0), (1, 'MID', 'TEXT', 0, None, 0), (2, 'PID', 'TEXT', 0, None, 0), (3, 'ID', 'INTEGER', 0, None, 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUfX4H2Oo3VI",
        "outputId": "b6b0dea6-a0ec-4d23-b87d-871492f5bce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# function to print all the table names and columns names in the database by sematically matching the question\n",
        "def print_tables_and_columns_by_question(tables, question):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        for column in columns:\n",
        "            input_ids = torch.tensor(tokenizer.encode(\n",
        "                question + \" \" + column)).unsqueeze(0)  # Batch size 1\n",
        "            output = model(input_ids)\n",
        "            if output[0][0][0][0] > 0.5:\n",
        "                print(\"Columns in table \" + table[0] + \":\")\n",
        "                print(columns)\n",
        "                print(\"\")\n",
        "                return\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database by sematically matching the question\n",
        "print_tables_and_columns_by_question(tables, question)\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkkRlaw_qaW4",
        "outputId": "59484583-c75e-4d52-bea3-b667d4b41c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"What are the names of the tables in the database and list all comedy movies?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# semantic serach function to print all the table names and columns names in the database by sematically matching the question\n",
        "def semantic_search(question, tables):\n",
        "    # Preprocess the English question\n",
        "    input_ids = torch.tensor(tokenizer.encode(\n",
        "        question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "    # Generate a semantic representation of the question\n",
        "    output = model(input_ids)\n",
        "\n",
        "    # Compare the semantic representation of the question with the semantic representation of the tables\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        for column in columns:\n",
        "            cursor.execute(\"SELECT \" + column + \" FROM \" + table[0])\n",
        "            rows = cursor.fetchall()\n",
        "            for row in rows:\n",
        "                input_ids = torch.tensor(tokenizer.encode(\n",
        "                    row[0])).unsqueeze(0)\n",
        "                output_row = model(input_ids)\n",
        "                similarity = torch.cosine_similarity(\n",
        "                    output[0], output_row[0], dim=1)\n",
        "                if similarity > 0.9:\n",
        "                    print(\"Similarity between question and \" + row[0] + \":\")\n",
        "                    print(similarity)\n",
        "                    print(\"\")\n",
        "    return\n",
        "# call the function to print all the table names and columns names in the database by sematically matching the question\n",
        "semantic_search(question, tables)\n",
        "\n",
        "# close the database connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "URKOsiZ2qye_",
        "outputId": "8fce5318-2b90-484f-8082-7a2fe9745e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9496c7b7dffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# call the function to print all the table names and columns names in the database by sematically matching the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0msemantic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# close the database connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-9496c7b7dffe>\u001b[0m in \u001b[0;36msemantic_search\u001b[0;34m(question, tables)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" FROM \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: near \"index\": syntax error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"List all movies where ?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "print(output)\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc7ofQFsrmD3",
        "outputId": "22695da0-7da0-4de6-f669-d4ae91167889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2753,  0.2062, -0.2812,  ..., -0.3618,  0.2308,  0.6460],\n",
            "         [ 0.4098,  0.6940,  0.1491,  ...,  0.2076,  0.2634,  0.5377],\n",
            "         [-0.3510, -0.1538,  1.1815,  ..., -0.2900,  0.2516, -0.3691],\n",
            "         ...,\n",
            "         [ 0.6622, -0.2652,  0.8352,  ...,  0.5237,  0.1432,  0.0804],\n",
            "         [-0.1971, -0.9702, -0.8535,  ...,  0.2307,  0.0697,  0.5020],\n",
            "         [ 0.9780,  0.1651, -0.1899,  ...,  0.2267, -0.7277, -0.1333]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.5900e-01, -3.8488e-01, -7.6583e-01,  6.6878e-01,  4.0596e-01,\n",
            "         -1.3221e-01,  8.1544e-01,  2.4634e-01, -6.8579e-01, -9.9995e-01,\n",
            "         -4.1075e-01,  8.3950e-01,  9.6768e-01,  5.4491e-01,  8.8112e-01,\n",
            "         -7.2483e-01, -3.2834e-01, -5.8837e-01,  2.2613e-01, -4.3926e-01,\n",
            "          6.0478e-01,  9.9984e-01,  7.8702e-02,  3.3994e-01,  4.6214e-01,\n",
            "          9.5503e-01, -8.0216e-01,  8.9470e-01,  9.3628e-01,  7.1864e-01,\n",
            "         -6.1425e-01,  2.3023e-01, -9.7908e-01, -1.2490e-01, -7.3957e-01,\n",
            "         -9.8260e-01,  3.0598e-01, -6.9021e-01,  9.1302e-02,  9.7757e-02,\n",
            "         -8.3237e-01,  2.2061e-01,  9.9984e-01, -2.4251e-01,  1.9757e-01,\n",
            "         -2.6131e-01, -1.0000e+00,  1.7733e-01, -8.3344e-01,  7.9151e-01,\n",
            "          6.8483e-01,  6.7198e-01,  1.5860e-01,  4.2062e-01,  4.8191e-01,\n",
            "         -3.4386e-02, -1.0448e-01,  7.0494e-02, -1.8104e-01, -5.0238e-01,\n",
            "         -6.6711e-01,  3.3271e-01, -7.5833e-01, -8.4032e-01,  8.5865e-01,\n",
            "          6.6386e-01, -1.2417e-01, -2.9124e-01, -5.9515e-02, -4.8576e-02,\n",
            "          8.5806e-01,  1.5317e-01, -1.5416e-02, -8.1039e-01,  4.7420e-01,\n",
            "          2.6651e-01, -7.1319e-01,  1.0000e+00, -5.4236e-01, -9.6190e-01,\n",
            "          6.9809e-01,  7.4244e-01,  6.5813e-01, -2.3364e-01,  5.0668e-01,\n",
            "         -1.0000e+00,  3.9374e-01, -1.1856e-01, -9.8229e-01,  1.8554e-01,\n",
            "          4.4119e-01, -2.5839e-01,  5.5465e-01,  6.4171e-01, -6.8899e-01,\n",
            "         -3.2046e-01, -9.6763e-02, -6.6400e-01, -1.3992e-01, -3.6377e-01,\n",
            "         -3.5211e-02, -1.5807e-01, -2.2354e-01, -3.3380e-01,  2.4912e-01,\n",
            "         -4.5997e-01, -3.9001e-01,  5.6834e-01,  2.8859e-01,  6.3226e-01,\n",
            "          4.8309e-01, -3.3397e-01,  3.9549e-01, -9.1327e-01,  6.1159e-01,\n",
            "         -3.0575e-01, -9.7529e-01, -6.8116e-01, -9.7961e-01,  5.1594e-01,\n",
            "         -1.2992e-01, -2.3902e-01,  9.1423e-01, -1.0382e-01,  3.2662e-01,\n",
            "          2.3869e-02, -7.1992e-01, -1.0000e+00, -5.7135e-01, -4.2205e-01,\n",
            "         -2.7487e-01, -2.9106e-01, -9.6200e-01, -9.4140e-01,  5.6570e-01,\n",
            "          9.2579e-01,  2.0965e-01,  9.9950e-01, -1.9494e-01,  9.2104e-01,\n",
            "         -2.6122e-01, -6.4066e-01,  2.7315e-01, -4.1902e-01,  6.9783e-01,\n",
            "          2.8653e-01, -5.5679e-01,  2.2350e-01, -2.5950e-01,  9.2960e-02,\n",
            "         -6.4425e-01, -1.3253e-01, -6.6572e-01, -9.0379e-01, -3.0916e-01,\n",
            "          9.1426e-01, -3.7135e-01, -8.8335e-01, -8.9732e-03, -2.2529e-01,\n",
            "         -3.0621e-01,  7.5024e-01,  6.4425e-01,  2.5413e-01, -5.8194e-01,\n",
            "          3.6188e-01,  2.5435e-01,  4.2178e-01, -7.6361e-01,  2.5127e-03,\n",
            "          4.0589e-01, -3.4851e-01, -7.2749e-01, -9.6788e-01, -3.5200e-01,\n",
            "          3.8979e-01,  9.7717e-01,  6.9500e-01,  1.9879e-01,  5.0050e-01,\n",
            "         -1.9348e-01,  5.8929e-01, -9.4322e-01,  9.6702e-01, -1.1323e-01,\n",
            "          3.1723e-01,  1.4977e-01,  3.8335e-01, -7.8161e-01,  1.0153e-02,\n",
            "          7.2418e-01, -4.4184e-01, -6.9163e-01,  3.1417e-02, -4.9806e-01,\n",
            "         -3.7380e-01, -5.2747e-01,  3.5705e-01, -2.5173e-01, -3.6779e-01,\n",
            "         -5.9758e-03,  8.8264e-01,  9.6410e-01,  5.6277e-01,  5.3215e-02,\n",
            "          5.9420e-01, -8.0078e-01, -4.7295e-01,  4.7071e-02,  1.6642e-01,\n",
            "          1.7531e-01,  9.8984e-01, -4.9712e-01, -6.2304e-02, -8.6701e-01,\n",
            "         -9.7751e-01, -9.0540e-02, -8.7179e-01, -2.0823e-01, -5.9875e-01,\n",
            "          5.4616e-01,  2.2040e-01,  4.9082e-01,  4.0205e-01, -9.5958e-01,\n",
            "         -7.5561e-01,  3.8222e-01, -4.3572e-01,  3.7537e-01, -2.4005e-01,\n",
            "          6.3960e-01,  8.8497e-01, -5.7364e-01,  6.3785e-01,  9.0545e-01,\n",
            "         -7.3398e-01, -6.7624e-01,  7.6185e-01, -2.5714e-01,  8.5734e-01,\n",
            "         -5.3398e-01,  9.7586e-01,  7.9967e-01,  6.7942e-01, -8.1552e-01,\n",
            "         -6.8485e-01, -8.0846e-01, -6.9241e-01, -3.8596e-02,  6.2879e-02,\n",
            "          8.3973e-01,  6.6068e-01,  3.7939e-01,  2.7626e-01, -5.5838e-01,\n",
            "          9.8977e-01, -7.2276e-01, -9.3461e-01, -4.5659e-02, -2.2461e-01,\n",
            "         -9.7844e-01,  6.8609e-01,  3.0941e-01, -1.7765e-01, -4.1321e-01,\n",
            "         -5.6982e-01, -9.4097e-01,  7.4455e-01,  5.7711e-02,  9.7453e-01,\n",
            "          8.0157e-03, -8.8509e-01, -5.6160e-01, -8.9839e-01, -1.4759e-01,\n",
            "         -2.4898e-01, -3.1377e-01, -1.2936e-01, -9.0835e-01,  5.0878e-01,\n",
            "          4.3310e-01,  3.9424e-01, -7.9382e-01,  9.9602e-01,  1.0000e+00,\n",
            "          9.5245e-01,  8.4109e-01,  8.0244e-01, -9.9874e-01, -3.2360e-01,\n",
            "          9.9998e-01, -9.6776e-01, -1.0000e+00, -8.8880e-01, -6.0914e-01,\n",
            "          2.7088e-01, -1.0000e+00, -2.2071e-01,  1.0517e-01, -8.4837e-01,\n",
            "          5.6723e-01,  9.6705e-01,  9.7781e-01, -1.0000e+00,  6.8940e-01,\n",
            "          8.6722e-01, -7.1431e-01,  8.7280e-01, -2.9252e-01,  9.4111e-01,\n",
            "          6.3468e-01,  4.1002e-01, -1.9012e-01,  4.6847e-01, -8.8796e-01,\n",
            "         -8.2432e-01, -4.2940e-01, -6.0775e-01,  9.8621e-01,  1.7961e-01,\n",
            "         -7.7898e-01, -8.1631e-01,  1.4381e-01, -1.4196e-01, -2.3166e-01,\n",
            "         -9.3327e-01, -1.2324e-01,  4.2289e-01,  7.0317e-01,  1.3200e-01,\n",
            "          2.7618e-01, -5.5381e-01,  3.1122e-01,  3.4376e-02,  2.1134e-01,\n",
            "          7.1082e-01, -9.3041e-01, -3.6215e-01, -3.8172e-01, -1.9514e-01,\n",
            "         -5.9952e-01, -9.5049e-01,  9.3432e-01, -3.6959e-01,  7.0540e-01,\n",
            "          1.0000e+00,  3.7116e-02, -7.5438e-01,  6.0746e-01,  2.4891e-01,\n",
            "         -2.3751e-01,  1.0000e+00,  7.4264e-01, -9.6079e-01, -5.6530e-01,\n",
            "          5.5700e-01, -5.3237e-01, -5.1653e-01,  9.9855e-01, -2.5820e-01,\n",
            "         -5.5988e-01, -1.2083e-01,  9.6830e-01, -9.8462e-01,  9.8111e-01,\n",
            "         -8.7215e-01, -9.3477e-01,  9.4769e-01,  9.0761e-01, -6.0728e-01,\n",
            "         -6.1761e-01,  1.0381e-01, -5.9603e-01,  3.1517e-01, -9.1735e-01,\n",
            "          6.4288e-01,  4.1378e-01, -6.8670e-02,  8.3497e-01, -7.6346e-01,\n",
            "         -6.3192e-01,  3.1900e-01, -6.8065e-01, -1.1784e-01,  7.8374e-01,\n",
            "          3.9167e-01, -1.5166e-01,  4.9973e-02, -2.4735e-01, -1.6765e-01,\n",
            "         -9.6289e-01,  3.4555e-01,  1.0000e+00, -1.5404e-01,  3.0300e-01,\n",
            "         -5.7469e-01,  2.7801e-02, -1.3671e-01,  4.9113e-01,  5.2386e-01,\n",
            "         -2.9460e-01, -7.3183e-01,  5.8344e-01, -9.2578e-01, -9.7962e-01,\n",
            "          6.5851e-01,  1.0609e-01, -2.5491e-01,  9.9996e-01,  5.2129e-01,\n",
            "          1.4081e-01,  3.4170e-01,  9.5069e-01, -5.6508e-02,  3.7950e-01,\n",
            "          7.8255e-01,  9.6319e-01, -2.2584e-01,  6.2753e-01,  7.6293e-01,\n",
            "         -8.4740e-01, -2.5078e-01, -6.3800e-01,  1.4074e-02, -9.1049e-01,\n",
            "         -1.0135e-02, -9.2835e-01,  9.4665e-01,  8.3821e-01,  3.2532e-01,\n",
            "          1.9391e-01,  3.4983e-01,  1.0000e+00, -2.9179e-01,  5.8002e-01,\n",
            "         -2.2972e-01,  7.8801e-01, -9.9818e-01, -7.2788e-01, -4.0517e-01,\n",
            "         -9.3169e-03, -7.5802e-01, -2.6841e-01,  2.5151e-01, -9.5268e-01,\n",
            "          6.8801e-01,  3.8769e-01, -9.6473e-01, -9.7987e-01,  3.2263e-02,\n",
            "          8.1859e-01, -5.0876e-04, -9.3983e-01, -7.1178e-01, -5.0813e-01,\n",
            "          5.6428e-01, -2.2012e-01, -8.8430e-01, -8.4963e-02, -2.4899e-01,\n",
            "          4.7391e-01, -1.2268e-01,  6.0350e-01,  7.3306e-01,  7.0726e-01,\n",
            "         -3.7088e-01, -2.4042e-01, -8.7777e-02, -8.4211e-01,  8.1211e-01,\n",
            "         -7.6578e-01, -7.1522e-01, -1.7149e-01,  1.0000e+00, -4.1774e-01,\n",
            "          8.1285e-01,  6.6102e-01,  6.8110e-01, -2.6242e-01,  2.3120e-01,\n",
            "          8.5636e-01,  2.1762e-01, -7.7492e-01, -7.4420e-01, -5.2510e-01,\n",
            "         -3.5941e-01,  6.9278e-01,  1.2678e-01,  5.5210e-01,  6.7256e-01,\n",
            "          5.3323e-01,  1.6970e-01, -9.6091e-02, -4.2258e-02,  9.9775e-01,\n",
            "         -7.3341e-02,  9.6401e-02, -4.9697e-01, -6.1205e-02, -3.6946e-01,\n",
            "         -2.1871e-01,  1.0000e+00,  2.7168e-01,  2.8050e-01, -9.8257e-01,\n",
            "         -6.5992e-01, -8.1694e-01,  1.0000e+00,  7.7425e-01, -6.0469e-01,\n",
            "          6.5110e-01,  5.9129e-01, -2.0706e-01,  6.7702e-01, -9.0044e-02,\n",
            "         -2.3750e-01,  1.7722e-01,  6.6230e-02,  9.1450e-01, -5.5422e-01,\n",
            "         -9.5594e-01, -5.0111e-01,  3.6847e-01, -9.2490e-01,  9.9939e-01,\n",
            "         -4.9343e-01, -1.7408e-01, -3.3296e-01,  1.6941e-01,  3.8241e-01,\n",
            "         -1.2439e-01, -9.6872e-01, -6.3149e-02,  1.5582e-01,  9.3621e-01,\n",
            "          1.5705e-01, -6.4400e-01, -8.7957e-01,  6.1742e-01,  5.3237e-01,\n",
            "         -7.7372e-01, -8.6361e-01,  9.4161e-01, -9.5923e-01,  5.7405e-01,\n",
            "          1.0000e+00,  3.7094e-01, -1.7306e-01,  7.9963e-02, -3.9396e-01,\n",
            "          3.1142e-01, -8.7436e-02,  7.0702e-01, -9.1762e-01, -3.2774e-01,\n",
            "         -1.4072e-01,  2.7499e-01, -9.5225e-02, -1.4650e-01,  4.5673e-01,\n",
            "          2.1400e-01, -5.8613e-01, -6.0321e-01,  2.1221e-02,  4.3617e-01,\n",
            "          8.2449e-01, -2.0850e-01, -2.0545e-01,  1.0026e-01, -1.8228e-01,\n",
            "         -8.5740e-01, -3.3210e-01, -2.7354e-01, -9.9992e-01,  7.5763e-01,\n",
            "         -1.0000e+00,  3.1728e-01,  4.7565e-02, -2.6251e-01,  7.7458e-01,\n",
            "          4.3903e-01,  4.9778e-01, -5.2780e-01, -7.6360e-01,  4.3647e-01,\n",
            "          6.6805e-01, -2.6040e-01, -2.7907e-01, -5.7336e-01,  1.8033e-01,\n",
            "         -4.6165e-02,  1.3828e-01, -4.3469e-01,  7.3874e-01, -1.7003e-01,\n",
            "          1.0000e+00,  8.6386e-02, -5.6536e-01, -9.3624e-01,  1.1160e-01,\n",
            "         -2.3715e-01,  1.0000e+00, -8.7559e-01, -9.1917e-01,  1.8034e-01,\n",
            "         -7.0080e-01, -8.1092e-01,  2.8168e-01,  1.2339e-01, -6.9002e-01,\n",
            "         -8.5592e-01,  8.6854e-01,  8.4506e-01, -6.1576e-01,  4.6282e-01,\n",
            "         -2.1541e-01, -4.9473e-01,  5.1503e-02,  6.4686e-01,  9.7857e-01,\n",
            "          3.6949e-01,  8.4366e-01,  2.8202e-01, -5.3935e-02,  9.3807e-01,\n",
            "          2.4602e-01,  2.8025e-01,  4.5850e-02,  1.0000e+00,  2.7059e-01,\n",
            "         -8.7412e-01,  2.3813e-01, -9.6224e-01, -2.2715e-01, -9.2264e-01,\n",
            "          2.9120e-01,  1.8292e-01,  8.6693e-01, -1.8177e-01,  9.2659e-01,\n",
            "         -4.7637e-01, -6.8871e-02, -5.4586e-01, -2.4365e-01,  4.2545e-01,\n",
            "         -8.6545e-01, -9.7657e-01, -9.7776e-01,  5.5217e-01, -3.7072e-01,\n",
            "          6.0653e-02,  2.1490e-01,  7.3455e-02,  3.6796e-01,  4.0605e-01,\n",
            "         -1.0000e+00,  9.0233e-01,  4.5619e-01,  8.3404e-01,  9.3134e-01,\n",
            "          5.7464e-01,  4.4743e-01,  3.0123e-01, -9.7559e-01, -9.3872e-01,\n",
            "         -3.5483e-01, -2.4198e-01,  5.9316e-01,  6.2790e-01,  8.2456e-01,\n",
            "          3.1971e-01, -4.8041e-01, -2.5634e-01, -2.7267e-01, -6.8504e-01,\n",
            "         -9.8924e-01,  4.7887e-01, -4.7964e-01, -9.3076e-01,  9.2861e-01,\n",
            "         -2.7311e-02, -1.2122e-01, -7.9458e-02, -6.6343e-01,  8.9503e-01,\n",
            "          6.5545e-01,  3.3552e-01,  8.7883e-02,  4.1916e-01,  8.4033e-01,\n",
            "          9.0870e-01,  9.6879e-01, -8.0143e-01,  7.3063e-01, -3.9865e-01,\n",
            "          4.0345e-01,  7.1130e-01, -9.1435e-01,  1.6566e-01,  3.0330e-01,\n",
            "         -4.9729e-01,  2.0404e-01, -2.4658e-01, -9.5108e-01,  4.1415e-01,\n",
            "         -3.2222e-01,  5.0986e-01, -2.7750e-01,  8.1052e-02, -4.0525e-01,\n",
            "         -1.7207e-01, -5.9100e-01, -6.3392e-01,  7.1289e-01,  4.0591e-01,\n",
            "          8.6402e-01,  6.9539e-01, -3.7200e-02, -6.6443e-01, -2.4792e-01,\n",
            "         -6.7366e-01, -8.7735e-01,  8.4942e-01, -2.0344e-02, -2.7994e-01,\n",
            "          4.0380e-01, -1.2168e-01,  6.6802e-01,  1.9362e-01, -3.7592e-01,\n",
            "         -3.6824e-01, -6.6590e-01,  7.3509e-01, -3.0879e-01, -5.2517e-01,\n",
            "         -5.6904e-01,  5.4254e-01,  3.6642e-01,  9.9985e-01, -7.1439e-01,\n",
            "         -8.2738e-01, -3.5307e-01, -2.8901e-01,  3.5036e-01, -4.1199e-01,\n",
            "         -1.0000e+00,  2.4182e-01, -3.9162e-01,  7.0332e-01, -5.7382e-01,\n",
            "          7.6179e-01, -5.2311e-01, -9.5800e-01, -1.8075e-01,  3.2770e-01,\n",
            "          5.0088e-01, -4.8978e-01, -6.4578e-01,  6.2903e-01, -1.2420e-03,\n",
            "          8.9740e-01,  7.6366e-01, -5.5009e-01, -1.4125e-01,  7.2514e-01,\n",
            "         -5.5231e-01, -6.2722e-01,  8.8008e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Get all the table names and columns names in the database by using tables names, columns names, question and by maching semantic similarity\n",
        "def get_table_and_column_names(tables, question):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        for column in columns:\n",
        "            input_text = question + \" \" + table[0] + \" \" + column\n",
        "            tokenized_text = tokenizer.tokenize(input_text)\n",
        "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "            segments_ids = [1] * len(tokenized_text)\n",
        "            tokens_tensor = torch.tensor([indexed_tokens])\n",
        "            segments_tensors = torch.tensor([segments_ids])\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "            token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "            token_embeddings = token_embeddings.permute(1, 0, 2)\n",
        "            token_vecs_sum = []\n",
        "            for token in token_embeddings:\n",
        "                sum_vec = torch.sum(token[-4:], dim=0)\n",
        "                token_vecs_sum.append(sum_vec)\n",
        "            sentence_embedding = torch.mean(torch.stack(token_vecs_sum), dim=0)\n",
        "            cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "            cos_sim = cos(sentence_embedding, question_embedding)\n",
        "            if cos_sim > 0.7:\n",
        "                return table[0], column\n",
        "    return None, None\n",
        "\n",
        "# Get the table name and column name by using the function get_table_and_column_names\n",
        "table_name, column_name = get_table_and_column_names(tables, question)\n",
        "print(\"Table Name: \" + table_name)\n",
        "print(\"Column Name: \" + column_name)\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vGrEHruiscow",
        "outputId": "c548bdfb-ec9d-41c6-e027-dceb4f9615a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: list all comedy movies?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-275f652a271a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Get the table name and column name by using the function get_table_and_column_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_table_and_column_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Table Name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column Name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-275f652a271a>\u001b[0m in \u001b[0;36mget_table_and_column_names\u001b[0;34m(tables, question)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the question using the BERT tokenizer\n",
        "question_tokens = tokenizer.tokenize(question)\n",
        "question_tokens = ['[CLS]'] + question_tokens + ['[SEP]']\n",
        "question_ids = tokenizer.convert_tokens_to_ids(question_tokens)\n",
        "question_ids = torch.tensor([question_ids])\n",
        "\n",
        "# Get the output from the BERT model\n",
        "with torch.no_grad():\n",
        "    question_output = model(question_ids)\n",
        "\n",
        "# print the output from the BERT model\n",
        "print(question_output)\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8q_N6wCvNys",
        "outputId": "b6b78134-7995-4350-d242-97a0d4a3cd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: list all comedy movies?\n",
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3056,  0.0352, -0.2608,  ..., -0.2744,  0.5498,  0.6035],\n",
            "         [-0.0530,  0.4415, -0.0239,  ...,  0.1702,  0.3903,  0.5585],\n",
            "         [-0.4942, -0.2925,  0.8274,  ..., -0.0826,  0.4571, -0.1149],\n",
            "         ...,\n",
            "         [ 0.9046, -0.0937, -0.4343,  ..., -0.0011,  0.2638, -0.3769],\n",
            "         [-0.6290, -0.8845, -0.5948,  ...,  0.3529,  0.3621,  0.4311],\n",
            "         [ 0.7883,  0.0472, -0.3031,  ...,  0.5006, -0.6100, -0.0095]]]), pooler_output=tensor([[-0.7661, -0.2946, -0.6372,  0.4657,  0.3241, -0.1456,  0.6348,  0.1857,\n",
            "         -0.5196, -0.9997, -0.3470,  0.7453,  0.9530,  0.4820,  0.6245, -0.7090,\n",
            "         -0.3386, -0.5473,  0.1657,  0.0794,  0.3490,  0.9994,  0.0586,  0.2986,\n",
            "          0.3381,  0.8406, -0.7840,  0.7395,  0.8926,  0.6877, -0.4189,  0.2458,\n",
            "         -0.9723, -0.0198, -0.6471, -0.9713,  0.2366, -0.5033,  0.1034,  0.1243,\n",
            "         -0.6616,  0.2012,  0.9993, -0.1876,  0.2311, -0.1733, -1.0000,  0.1041,\n",
            "         -0.7215,  0.6627,  0.5321,  0.5867,  0.0160,  0.2862,  0.3893, -0.0736,\n",
            "         -0.1064,  0.0865, -0.0830, -0.3668, -0.5945,  0.2361, -0.6401, -0.7018,\n",
            "          0.8067,  0.5631, -0.1155, -0.2535,  0.0499, -0.1234,  0.7217,  0.0457,\n",
            "         -0.0652, -0.6353,  0.3112,  0.2509, -0.6158,  1.0000, -0.4805, -0.9317,\n",
            "          0.6684,  0.6484,  0.5272, -0.1497,  0.3643, -1.0000,  0.3144, -0.0485,\n",
            "         -0.9694,  0.0298,  0.4181, -0.2514,  0.5533,  0.5332, -0.5904, -0.2235,\n",
            "         -0.0067, -0.6259, -0.1671, -0.3455, -0.1192, -0.0667, -0.1243, -0.2849,\n",
            "          0.1642, -0.3398, -0.0658,  0.4834,  0.1726,  0.6208,  0.4204, -0.2700,\n",
            "          0.3225, -0.8638,  0.4747, -0.1887, -0.9619, -0.5087, -0.9653,  0.3685,\n",
            "         -0.0904, -0.2648,  0.7859, -0.0589,  0.2586,  0.0783, -0.6444, -1.0000,\n",
            "         -0.4380, -0.4025, -0.2524, -0.2367, -0.9541, -0.9139,  0.4676,  0.8589,\n",
            "          0.1216,  0.9973, -0.1130,  0.8775, -0.1990, -0.6008,  0.1156, -0.2645,\n",
            "          0.5742,  0.1878, -0.2101,  0.2092, -0.3604,  0.0132, -0.6658, -0.1941,\n",
            "         -0.5261, -0.7745, -0.3012,  0.7877, -0.2307, -0.7997, -0.0244, -0.2012,\n",
            "         -0.2197,  0.6043,  0.5744,  0.1671, -0.5043,  0.2717, -0.0369,  0.2474,\n",
            "         -0.6626,  0.0099,  0.2573, -0.2939, -0.6762, -0.9497, -0.3083,  0.2635,\n",
            "          0.9599,  0.5907,  0.0565,  0.4216, -0.1654,  0.4574, -0.9215,  0.9475,\n",
            "          0.0159,  0.3441,  0.0828,  0.1558, -0.6448, -0.1255,  0.5851, -0.1446,\n",
            "         -0.6045,  0.0108, -0.4044, -0.2745, -0.4713,  0.3756, -0.1327, -0.2535,\n",
            "          0.0446,  0.7908,  0.8724,  0.2870,  0.0392,  0.5141, -0.6097, -0.3731,\n",
            "          0.0151,  0.0819,  0.1240,  0.9824, -0.4239, -0.0258, -0.6895, -0.9568,\n",
            "         -0.0723, -0.7971, -0.1810, -0.5296,  0.3963,  0.2294,  0.2323,  0.3221,\n",
            "         -0.7753, -0.6370,  0.3453, -0.3391,  0.3315, -0.0999,  0.7642,  0.7803,\n",
            "         -0.5472,  0.2723,  0.8796, -0.6564, -0.6421,  0.3999, -0.2018,  0.7825,\n",
            "         -0.3991,  0.9310,  0.6934,  0.4766, -0.6727, -0.6016, -0.6769, -0.5183,\n",
            "         -0.0178, -0.1783,  0.7015,  0.4964,  0.3340,  0.4115, -0.4166,  0.9284,\n",
            "         -0.8326, -0.9116, -0.2217, -0.1553, -0.9704,  0.6071,  0.2208, -0.0542,\n",
            "         -0.3095, -0.4923, -0.8944,  0.3566,  0.0269,  0.9181,  0.0920, -0.7352,\n",
            "         -0.4933, -0.8357, -0.1120, -0.1474, -0.1365, -0.0805, -0.7792,  0.4552,\n",
            "          0.3059,  0.3538, -0.6994,  0.9717,  1.0000,  0.9311,  0.6990,  0.5369,\n",
            "         -0.9965, -0.5287,  0.9999, -0.9268, -1.0000, -0.7908, -0.5131,  0.1117,\n",
            "         -1.0000, -0.1955,  0.0869, -0.7667,  0.4281,  0.9350,  0.8690, -1.0000,\n",
            "          0.6100,  0.7394, -0.5863,  0.7560, -0.2472,  0.9107,  0.6149,  0.4549,\n",
            "         -0.0767,  0.4052, -0.8416, -0.6988, -0.3640, -0.5730,  0.9710,  0.0797,\n",
            "         -0.7078, -0.5402,  0.1857, -0.1151, -0.3062, -0.8714, -0.0483,  0.3810,\n",
            "          0.6072,  0.1291,  0.3013, -0.3668,  0.2468, -0.0661, -0.1705,  0.5692,\n",
            "         -0.8667, -0.0801, -0.3282, -0.1384, -0.4672, -0.9148,  0.8662, -0.2642,\n",
            "          0.6247,  1.0000,  0.1672, -0.4806,  0.4769,  0.1166, -0.4299,  1.0000,\n",
            "          0.6904, -0.9493, -0.4016,  0.4418, -0.3887, -0.3565,  0.9946, -0.1341,\n",
            "         -0.4451, -0.1839,  0.9702, -0.9793,  0.9628, -0.7872, -0.8794,  0.8777,\n",
            "          0.8324, -0.5867, -0.5188, -0.0077, -0.3994,  0.2616, -0.7489,  0.6090,\n",
            "          0.2580,  0.0105,  0.6920, -0.6117, -0.4885,  0.2725, -0.6248, -0.1365,\n",
            "          0.6422,  0.2617, -0.0944, -0.0225, -0.1775, -0.3258, -0.9070,  0.3178,\n",
            "          1.0000, -0.1978,  0.2961, -0.4595, -0.0049, -0.1747,  0.4271,  0.4007,\n",
            "         -0.2044, -0.5309,  0.4836, -0.6751, -0.9751,  0.2955,  0.0730, -0.1803,\n",
            "          0.9993,  0.4410,  0.0796,  0.3352,  0.8767, -0.1419,  0.0684,  0.6191,\n",
            "          0.9410, -0.2618,  0.5017,  0.4770, -0.7526, -0.2721, -0.5061, -0.0323,\n",
            "         -0.8539,  0.0035, -0.8710,  0.9113,  0.7259,  0.2615,  0.1355,  0.3060,\n",
            "          1.0000, -0.6032,  0.4583,  0.0285,  0.3920, -0.9950, -0.3778, -0.3103,\n",
            "          0.0509, -0.6422, -0.1874,  0.2334, -0.9305,  0.5898,  0.2637, -0.7803,\n",
            "         -0.9627,  0.1160,  0.7122, -0.0425, -0.9212, -0.5898, -0.4419,  0.5621,\n",
            "         -0.1058, -0.7883, -0.0485, -0.2349,  0.4227, -0.1145,  0.5033,  0.6117,\n",
            "          0.7855, -0.4152, -0.2635, -0.1232, -0.7832,  0.7460, -0.6148, -0.5569,\n",
            "         -0.0749,  1.0000, -0.4343,  0.7554,  0.3978,  0.5624, -0.2126,  0.1529,\n",
            "          0.7738,  0.2152, -0.6183, -0.6917, -0.2525, -0.2214,  0.6323,  0.1586,\n",
            "          0.3288,  0.5606,  0.5446,  0.1385, -0.1272, -0.1052,  0.9756,  0.0764,\n",
            "          0.1623, -0.4369, -0.0641, -0.3062,  0.0779,  1.0000,  0.1940,  0.1338,\n",
            "         -0.9740, -0.5938, -0.6137,  1.0000,  0.7275, -0.4483,  0.5786,  0.5921,\n",
            "         -0.1909,  0.4148, -0.0138, -0.2474,  0.1801,  0.0409,  0.8602, -0.4552,\n",
            "         -0.9406, -0.3383,  0.2313, -0.8344,  0.9982, -0.4302, -0.1757, -0.3147,\n",
            "          0.1261, -0.2094, -0.2204, -0.9234, -0.0450,  0.1050,  0.8889,  0.1774,\n",
            "         -0.5341, -0.7951,  0.5354,  0.5293, -0.6113, -0.7657,  0.8940, -0.8626,\n",
            "          0.5434,  1.0000,  0.4237, -0.0599,  0.0822, -0.3456,  0.2022, -0.0754,\n",
            "          0.5532, -0.7979, -0.2218, -0.0927,  0.1678, -0.0236, -0.3082,  0.3331,\n",
            "          0.1153, -0.4539, -0.5601,  0.0633,  0.4192,  0.7207, -0.0921, -0.1415,\n",
            "          0.1375, -0.0743, -0.5089, -0.2714, -0.1760, -0.9996,  0.6832, -1.0000,\n",
            "          0.3150, -0.0799, -0.1855,  0.6229,  0.5173,  0.3044, -0.3546, -0.6717,\n",
            "          0.3958,  0.3647, -0.1092, -0.1981, -0.2708,  0.1008,  0.0238,  0.1285,\n",
            "         -0.3347,  0.4941, -0.0560,  1.0000,  0.0753, -0.5213, -0.7358,  0.0340,\n",
            "         -0.1547,  1.0000, -0.7299, -0.9006,  0.0766, -0.6337, -0.6582,  0.2276,\n",
            "          0.0948, -0.6205, -0.7527,  0.5156,  0.7218, -0.5235,  0.3950, -0.1750,\n",
            "         -0.3880,  0.0543,  0.5416,  0.9624,  0.2920,  0.6612,  0.1004, -0.0970,\n",
            "          0.8821,  0.2103, -0.0598,  0.0374,  1.0000,  0.2401, -0.8360,  0.1933,\n",
            "         -0.9015, -0.2945, -0.8137,  0.2252,  0.1041,  0.8225, -0.0962,  0.8501,\n",
            "         -0.3270, -0.1664, -0.4987, -0.1647,  0.3844, -0.7733, -0.9676, -0.9674,\n",
            "          0.5105, -0.3325,  0.2062,  0.2360, -0.0254,  0.2767,  0.2651, -1.0000,\n",
            "          0.8586,  0.3735,  0.7356,  0.9060,  0.4376,  0.4988,  0.2813, -0.9599,\n",
            "         -0.6435, -0.2205, -0.2111,  0.3522,  0.5876,  0.7380,  0.2778, -0.3930,\n",
            "         -0.2355, -0.0524, -0.7689, -0.9820,  0.4478, -0.3719, -0.8002,  0.8822,\n",
            "         -0.1838, -0.0591,  0.0142, -0.5180,  0.6429,  0.3678,  0.1813,  0.0061,\n",
            "          0.3092,  0.6811,  0.7905,  0.9507, -0.7058,  0.5409, -0.2198,  0.2780,\n",
            "          0.8184, -0.8714,  0.0700,  0.2379, -0.4432,  0.1978, -0.2175, -0.7584,\n",
            "          0.4620, -0.3392,  0.5179, -0.1815,  0.1225, -0.3116, -0.1346, -0.5285,\n",
            "         -0.6166,  0.5854,  0.2866,  0.8056,  0.5662, -0.0352, -0.4968, -0.1580,\n",
            "         -0.4718, -0.8545,  0.5685, -0.0142,  0.0154,  0.3793, -0.0988,  0.7853,\n",
            "          0.2340, -0.3100, -0.2734, -0.6181,  0.4131, -0.4773, -0.4529, -0.4691,\n",
            "          0.4710,  0.2748,  0.9995, -0.6292, -0.7033, -0.4368, -0.1255,  0.3550,\n",
            "         -0.4168, -1.0000,  0.1335, -0.3169,  0.6597, -0.5096,  0.7472, -0.4527,\n",
            "         -0.8345, -0.0706,  0.3498,  0.3431, -0.3514, -0.5078,  0.4795,  0.1235,\n",
            "          0.7970,  0.6313, -0.7268, -0.1071,  0.5337, -0.5210, -0.4779,  0.6664]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the question using the BERT tokenizer\n",
        "question_tokens = tokenizer.tokenize(question)\n",
        "question_tokens = ['[CLS]'] + question_tokens + ['[SEP]']\n",
        "question_ids = tokenizer.convert_tokens_to_ids(question_tokens)\n",
        "question_ids = torch.tensor([question_ids])\n",
        "\n",
        "# Get the output from the BERT model\n",
        "with torch.no_grad():\n",
        "    question_output = model(question_ids)\n",
        "\n",
        "# print the output from the BERT model\n",
        "print(question_output)\n",
        "\n",
        "# Get semantically matching table names and columns names from the database using the BERT model output and question\n",
        "def get_semantically_matching_tables_and_columns(question_output, tables):\n",
        "    # Get the output from the BERT model\n",
        "    question_output = question_output[0][0][0]\n",
        "    # Get the output from the BERT model for each table and column in the database\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        for column in columns:\n",
        "            cursor.execute(\"SELECT \" +\n",
        "                            column +\n",
        "                            \" FROM \" +\n",
        "                            table[0])\n",
        "            column_output = cursor.fetchall()\n",
        "            # Get the output from the BERT model for each row in the column\n",
        "            for row in column_output:\n",
        "                # Get the output from the BERT model for each word in the row\n",
        "                for word in row:\n",
        "                    word_tokens = tokenizer.tokenize(word)\n",
        "                    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
        "                    word_ids = torch.tensor([word_ids])\n",
        "                    with torch.no_grad():\n",
        "                        word_output = model(word_ids)\n",
        "                    # Get the cosine similarity between the output from the BERT model for the question and the output from the BERT model for the word\n",
        "                    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "                    cos_sim = cos(question_output, word_output[0][0][0])\n",
        "                    # If the cosine similarity is greater than 0.5, print the table name and column name\n",
        "                    if cos_sim > 0.5:\n",
        "                        print(\"Table: \" + table[0] + \", Column: \" + column)\n",
        "    return\n",
        "\n",
        "# call the function to get semantically matching table names and columns names from the database using the BERT model output and question\n",
        "get_semantically_matching_tables_and_columns(question_output, tables)\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YGjzuvqFvOWx",
        "outputId": "48c4305b-992e-4e95-e0a4-e044b819b7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: list all comedy movies?\n",
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3056,  0.0352, -0.2608,  ..., -0.2744,  0.5498,  0.6035],\n",
            "         [-0.0530,  0.4415, -0.0239,  ...,  0.1702,  0.3903,  0.5585],\n",
            "         [-0.4942, -0.2925,  0.8274,  ..., -0.0826,  0.4571, -0.1149],\n",
            "         ...,\n",
            "         [ 0.9046, -0.0937, -0.4343,  ..., -0.0011,  0.2638, -0.3769],\n",
            "         [-0.6290, -0.8845, -0.5948,  ...,  0.3529,  0.3621,  0.4311],\n",
            "         [ 0.7883,  0.0472, -0.3031,  ...,  0.5006, -0.6100, -0.0095]]]), pooler_output=tensor([[-0.7661, -0.2946, -0.6372,  0.4657,  0.3241, -0.1456,  0.6348,  0.1857,\n",
            "         -0.5196, -0.9997, -0.3470,  0.7453,  0.9530,  0.4820,  0.6245, -0.7090,\n",
            "         -0.3386, -0.5473,  0.1657,  0.0794,  0.3490,  0.9994,  0.0586,  0.2986,\n",
            "          0.3381,  0.8406, -0.7840,  0.7395,  0.8926,  0.6877, -0.4189,  0.2458,\n",
            "         -0.9723, -0.0198, -0.6471, -0.9713,  0.2366, -0.5033,  0.1034,  0.1243,\n",
            "         -0.6616,  0.2012,  0.9993, -0.1876,  0.2311, -0.1733, -1.0000,  0.1041,\n",
            "         -0.7215,  0.6627,  0.5321,  0.5867,  0.0160,  0.2862,  0.3893, -0.0736,\n",
            "         -0.1064,  0.0865, -0.0830, -0.3668, -0.5945,  0.2361, -0.6401, -0.7018,\n",
            "          0.8067,  0.5631, -0.1155, -0.2535,  0.0499, -0.1234,  0.7217,  0.0457,\n",
            "         -0.0652, -0.6353,  0.3112,  0.2509, -0.6158,  1.0000, -0.4805, -0.9317,\n",
            "          0.6684,  0.6484,  0.5272, -0.1497,  0.3643, -1.0000,  0.3144, -0.0485,\n",
            "         -0.9694,  0.0298,  0.4181, -0.2514,  0.5533,  0.5332, -0.5904, -0.2235,\n",
            "         -0.0067, -0.6259, -0.1671, -0.3455, -0.1192, -0.0667, -0.1243, -0.2849,\n",
            "          0.1642, -0.3398, -0.0658,  0.4834,  0.1726,  0.6208,  0.4204, -0.2700,\n",
            "          0.3225, -0.8638,  0.4747, -0.1887, -0.9619, -0.5087, -0.9653,  0.3685,\n",
            "         -0.0904, -0.2648,  0.7859, -0.0589,  0.2586,  0.0783, -0.6444, -1.0000,\n",
            "         -0.4380, -0.4025, -0.2524, -0.2367, -0.9541, -0.9139,  0.4676,  0.8589,\n",
            "          0.1216,  0.9973, -0.1130,  0.8775, -0.1990, -0.6008,  0.1156, -0.2645,\n",
            "          0.5742,  0.1878, -0.2101,  0.2092, -0.3604,  0.0132, -0.6658, -0.1941,\n",
            "         -0.5261, -0.7745, -0.3012,  0.7877, -0.2307, -0.7997, -0.0244, -0.2012,\n",
            "         -0.2197,  0.6043,  0.5744,  0.1671, -0.5043,  0.2717, -0.0369,  0.2474,\n",
            "         -0.6626,  0.0099,  0.2573, -0.2939, -0.6762, -0.9497, -0.3083,  0.2635,\n",
            "          0.9599,  0.5907,  0.0565,  0.4216, -0.1654,  0.4574, -0.9215,  0.9475,\n",
            "          0.0159,  0.3441,  0.0828,  0.1558, -0.6448, -0.1255,  0.5851, -0.1446,\n",
            "         -0.6045,  0.0108, -0.4044, -0.2745, -0.4713,  0.3756, -0.1327, -0.2535,\n",
            "          0.0446,  0.7908,  0.8724,  0.2870,  0.0392,  0.5141, -0.6097, -0.3731,\n",
            "          0.0151,  0.0819,  0.1240,  0.9824, -0.4239, -0.0258, -0.6895, -0.9568,\n",
            "         -0.0723, -0.7971, -0.1810, -0.5296,  0.3963,  0.2294,  0.2323,  0.3221,\n",
            "         -0.7753, -0.6370,  0.3453, -0.3391,  0.3315, -0.0999,  0.7642,  0.7803,\n",
            "         -0.5472,  0.2723,  0.8796, -0.6564, -0.6421,  0.3999, -0.2018,  0.7825,\n",
            "         -0.3991,  0.9310,  0.6934,  0.4766, -0.6727, -0.6016, -0.6769, -0.5183,\n",
            "         -0.0178, -0.1783,  0.7015,  0.4964,  0.3340,  0.4115, -0.4166,  0.9284,\n",
            "         -0.8326, -0.9116, -0.2217, -0.1553, -0.9704,  0.6071,  0.2208, -0.0542,\n",
            "         -0.3095, -0.4923, -0.8944,  0.3566,  0.0269,  0.9181,  0.0920, -0.7352,\n",
            "         -0.4933, -0.8357, -0.1120, -0.1474, -0.1365, -0.0805, -0.7792,  0.4552,\n",
            "          0.3059,  0.3538, -0.6994,  0.9717,  1.0000,  0.9311,  0.6990,  0.5369,\n",
            "         -0.9965, -0.5287,  0.9999, -0.9268, -1.0000, -0.7908, -0.5131,  0.1117,\n",
            "         -1.0000, -0.1955,  0.0869, -0.7667,  0.4281,  0.9350,  0.8690, -1.0000,\n",
            "          0.6100,  0.7394, -0.5863,  0.7560, -0.2472,  0.9107,  0.6149,  0.4549,\n",
            "         -0.0767,  0.4052, -0.8416, -0.6988, -0.3640, -0.5730,  0.9710,  0.0797,\n",
            "         -0.7078, -0.5402,  0.1857, -0.1151, -0.3062, -0.8714, -0.0483,  0.3810,\n",
            "          0.6072,  0.1291,  0.3013, -0.3668,  0.2468, -0.0661, -0.1705,  0.5692,\n",
            "         -0.8667, -0.0801, -0.3282, -0.1384, -0.4672, -0.9148,  0.8662, -0.2642,\n",
            "          0.6247,  1.0000,  0.1672, -0.4806,  0.4769,  0.1166, -0.4299,  1.0000,\n",
            "          0.6904, -0.9493, -0.4016,  0.4418, -0.3887, -0.3565,  0.9946, -0.1341,\n",
            "         -0.4451, -0.1839,  0.9702, -0.9793,  0.9628, -0.7872, -0.8794,  0.8777,\n",
            "          0.8324, -0.5867, -0.5188, -0.0077, -0.3994,  0.2616, -0.7489,  0.6090,\n",
            "          0.2580,  0.0105,  0.6920, -0.6117, -0.4885,  0.2725, -0.6248, -0.1365,\n",
            "          0.6422,  0.2617, -0.0944, -0.0225, -0.1775, -0.3258, -0.9070,  0.3178,\n",
            "          1.0000, -0.1978,  0.2961, -0.4595, -0.0049, -0.1747,  0.4271,  0.4007,\n",
            "         -0.2044, -0.5309,  0.4836, -0.6751, -0.9751,  0.2955,  0.0730, -0.1803,\n",
            "          0.9993,  0.4410,  0.0796,  0.3352,  0.8767, -0.1419,  0.0684,  0.6191,\n",
            "          0.9410, -0.2618,  0.5017,  0.4770, -0.7526, -0.2721, -0.5061, -0.0323,\n",
            "         -0.8539,  0.0035, -0.8710,  0.9113,  0.7259,  0.2615,  0.1355,  0.3060,\n",
            "          1.0000, -0.6032,  0.4583,  0.0285,  0.3920, -0.9950, -0.3778, -0.3103,\n",
            "          0.0509, -0.6422, -0.1874,  0.2334, -0.9305,  0.5898,  0.2637, -0.7803,\n",
            "         -0.9627,  0.1160,  0.7122, -0.0425, -0.9212, -0.5898, -0.4419,  0.5621,\n",
            "         -0.1058, -0.7883, -0.0485, -0.2349,  0.4227, -0.1145,  0.5033,  0.6117,\n",
            "          0.7855, -0.4152, -0.2635, -0.1232, -0.7832,  0.7460, -0.6148, -0.5569,\n",
            "         -0.0749,  1.0000, -0.4343,  0.7554,  0.3978,  0.5624, -0.2126,  0.1529,\n",
            "          0.7738,  0.2152, -0.6183, -0.6917, -0.2525, -0.2214,  0.6323,  0.1586,\n",
            "          0.3288,  0.5606,  0.5446,  0.1385, -0.1272, -0.1052,  0.9756,  0.0764,\n",
            "          0.1623, -0.4369, -0.0641, -0.3062,  0.0779,  1.0000,  0.1940,  0.1338,\n",
            "         -0.9740, -0.5938, -0.6137,  1.0000,  0.7275, -0.4483,  0.5786,  0.5921,\n",
            "         -0.1909,  0.4148, -0.0138, -0.2474,  0.1801,  0.0409,  0.8602, -0.4552,\n",
            "         -0.9406, -0.3383,  0.2313, -0.8344,  0.9982, -0.4302, -0.1757, -0.3147,\n",
            "          0.1261, -0.2094, -0.2204, -0.9234, -0.0450,  0.1050,  0.8889,  0.1774,\n",
            "         -0.5341, -0.7951,  0.5354,  0.5293, -0.6113, -0.7657,  0.8940, -0.8626,\n",
            "          0.5434,  1.0000,  0.4237, -0.0599,  0.0822, -0.3456,  0.2022, -0.0754,\n",
            "          0.5532, -0.7979, -0.2218, -0.0927,  0.1678, -0.0236, -0.3082,  0.3331,\n",
            "          0.1153, -0.4539, -0.5601,  0.0633,  0.4192,  0.7207, -0.0921, -0.1415,\n",
            "          0.1375, -0.0743, -0.5089, -0.2714, -0.1760, -0.9996,  0.6832, -1.0000,\n",
            "          0.3150, -0.0799, -0.1855,  0.6229,  0.5173,  0.3044, -0.3546, -0.6717,\n",
            "          0.3958,  0.3647, -0.1092, -0.1981, -0.2708,  0.1008,  0.0238,  0.1285,\n",
            "         -0.3347,  0.4941, -0.0560,  1.0000,  0.0753, -0.5213, -0.7358,  0.0340,\n",
            "         -0.1547,  1.0000, -0.7299, -0.9006,  0.0766, -0.6337, -0.6582,  0.2276,\n",
            "          0.0948, -0.6205, -0.7527,  0.5156,  0.7218, -0.5235,  0.3950, -0.1750,\n",
            "         -0.3880,  0.0543,  0.5416,  0.9624,  0.2920,  0.6612,  0.1004, -0.0970,\n",
            "          0.8821,  0.2103, -0.0598,  0.0374,  1.0000,  0.2401, -0.8360,  0.1933,\n",
            "         -0.9015, -0.2945, -0.8137,  0.2252,  0.1041,  0.8225, -0.0962,  0.8501,\n",
            "         -0.3270, -0.1664, -0.4987, -0.1647,  0.3844, -0.7733, -0.9676, -0.9674,\n",
            "          0.5105, -0.3325,  0.2062,  0.2360, -0.0254,  0.2767,  0.2651, -1.0000,\n",
            "          0.8586,  0.3735,  0.7356,  0.9060,  0.4376,  0.4988,  0.2813, -0.9599,\n",
            "         -0.6435, -0.2205, -0.2111,  0.3522,  0.5876,  0.7380,  0.2778, -0.3930,\n",
            "         -0.2355, -0.0524, -0.7689, -0.9820,  0.4478, -0.3719, -0.8002,  0.8822,\n",
            "         -0.1838, -0.0591,  0.0142, -0.5180,  0.6429,  0.3678,  0.1813,  0.0061,\n",
            "          0.3092,  0.6811,  0.7905,  0.9507, -0.7058,  0.5409, -0.2198,  0.2780,\n",
            "          0.8184, -0.8714,  0.0700,  0.2379, -0.4432,  0.1978, -0.2175, -0.7584,\n",
            "          0.4620, -0.3392,  0.5179, -0.1815,  0.1225, -0.3116, -0.1346, -0.5285,\n",
            "         -0.6166,  0.5854,  0.2866,  0.8056,  0.5662, -0.0352, -0.4968, -0.1580,\n",
            "         -0.4718, -0.8545,  0.5685, -0.0142,  0.0154,  0.3793, -0.0988,  0.7853,\n",
            "          0.2340, -0.3100, -0.2734, -0.6181,  0.4131, -0.4773, -0.4529, -0.4691,\n",
            "          0.4710,  0.2748,  0.9995, -0.6292, -0.7033, -0.4368, -0.1255,  0.3550,\n",
            "         -0.4168, -1.0000,  0.1335, -0.3169,  0.6597, -0.5096,  0.7472, -0.4527,\n",
            "         -0.8345, -0.0706,  0.3498,  0.3431, -0.3514, -0.5078,  0.4795,  0.1235,\n",
            "          0.7970,  0.6313, -0.7268, -0.1071,  0.5337, -0.5210, -0.4779,  0.6664]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8a8737e75f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# call the function to get semantically matching table names and columns names from the database using the BERT model output and question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mget_semantically_matching_tables_and_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# close the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-8a8737e75f1e>\u001b[0m in \u001b[0;36mget_semantically_matching_tables_and_columns\u001b[0;34m(question_output, tables)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             cursor.execute(\"SELECT \" +\n\u001b[0m\u001b[1;32m     56\u001b[0m                             \u001b[0mcolumn\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                             \u001b[0;34m\" FROM \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: near \"index\": syntax error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the question using the BERT tokenizer\n",
        "question_tokens = tokenizer.tokenize(question)\n",
        "question_tokens = ['[CLS]'] + question_tokens + ['[SEP]']\n",
        "question_ids = tokenizer.convert_tokens_to_ids(question_tokens)\n",
        "question_ids = torch.tensor([question_ids])\n",
        "\n",
        "# Get the output from the BERT model\n",
        "with torch.no_grad():\n",
        "    question_output = model(question_ids)\n",
        "\n",
        "# print the output from the BERT model\n",
        "print(question_output)\n",
        "\n",
        "# Semantically match table names and columns names using the BERT model output and question\n",
        "def get_semantically_matching_tables_and_columns(question_output, tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        for column in columns:\n",
        "            column_tokens = tokenizer.tokenize(column)\n",
        "            column_tokens = ['[CLS]'] + column_tokens + ['[SEP]']\n",
        "            column_ids = tokenizer.convert_tokens_to_ids(column_tokens)\n",
        "            column_ids = torch.tensor([column_ids])\n",
        "            with torch.no_grad():\n",
        "                column_output = model(column_ids)\n",
        "            print(\"Similarity between \" + question + \" and \" + column + \" is \" + str(torch.cosine_similarity(question_output[0], column_output[0], dim=1)))\n",
        "    return\n",
        "\n",
        "\n",
        "# call the function to get semantically matching table names and columns names from the database using the BERT model output and question\n",
        "get_semantically_matching_tables_and_columns(question_output, tables)\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SHX5ABrYvzmZ",
        "outputId": "22d6acca-539c-4d1c-e533-6eb5870379da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: list all the comedy movies?\n",
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.5184e-01,  2.1112e-04, -3.6048e-01,  ..., -1.0900e-01,\n",
            "           4.9850e-01,  5.1213e-01],\n",
            "         [ 4.5390e-03,  3.0934e-01,  3.4192e-01,  ...,  1.3862e-01,\n",
            "           2.6549e-01,  2.8119e-01],\n",
            "         [-6.0266e-01, -7.8740e-01,  1.0702e+00,  ...,  2.4208e-01,\n",
            "           6.8055e-01,  1.7610e-01],\n",
            "         ...,\n",
            "         [ 8.2531e-01, -2.4109e-02, -3.5690e-01,  ...,  2.6998e-01,\n",
            "           3.4618e-01, -3.6203e-01],\n",
            "         [-6.0513e-01, -1.0469e+00, -5.1820e-01,  ...,  4.7873e-01,\n",
            "           3.7486e-01,  1.0885e-01],\n",
            "         [ 8.0018e-01,  7.7958e-02, -2.0348e-01,  ...,  4.7889e-01,\n",
            "          -6.1196e-01, -5.6337e-02]]]), pooler_output=tensor([[-7.1108e-01, -2.5426e-01, -6.1647e-01,  4.0977e-01,  3.6481e-01,\n",
            "         -9.9603e-02,  6.3000e-01,  1.1508e-01, -5.1598e-01, -9.9967e-01,\n",
            "         -4.9319e-01,  7.0477e-01,  9.4596e-01,  4.3248e-01,  6.8819e-01,\n",
            "         -7.0238e-01, -1.4738e-01, -5.4471e-01,  8.5916e-02,  1.0044e-01,\n",
            "          3.1007e-01,  9.9891e-01,  9.4269e-03,  2.7826e-01,  3.4432e-01,\n",
            "          8.3637e-01, -7.7500e-01,  7.5296e-01,  8.6786e-01,  6.4482e-01,\n",
            "         -3.5570e-01,  1.3276e-01, -9.6836e-01,  5.5270e-02, -5.9446e-01,\n",
            "         -9.6795e-01,  1.8288e-01, -5.2711e-01,  1.4665e-01,  2.4575e-01,\n",
            "         -6.8883e-01,  1.5432e-01,  9.9935e-01, -1.6167e-01,  1.7071e-01,\n",
            "         -8.9012e-02, -9.9992e-01,  1.8057e-02, -7.3741e-01,  6.9657e-01,\n",
            "          6.2834e-01,  5.5479e-01, -4.3228e-02,  2.2110e-01,  3.2443e-01,\n",
            "          7.1354e-02, -2.4552e-01, -5.3942e-02, -7.4667e-02, -2.8975e-01,\n",
            "         -5.7738e-01,  2.3168e-01, -7.0775e-01, -6.9339e-01,  8.2895e-01,\n",
            "          5.4453e-01, -1.4491e-01, -2.5843e-01,  1.5577e-01, -1.8757e-01,\n",
            "          7.2646e-01,  2.0400e-02,  8.8272e-03, -6.8744e-01,  2.5874e-01,\n",
            "          1.6649e-01, -6.6219e-01,  1.0000e+00, -3.6345e-01, -9.4153e-01,\n",
            "          6.4561e-01,  6.1970e-01,  6.0242e-01, -1.8684e-01,  3.6791e-01,\n",
            "         -1.0000e+00,  3.8348e-01, -3.2861e-02, -9.6839e-01, -3.9354e-02,\n",
            "          4.3084e-01, -2.1842e-01,  4.0748e-01,  5.7921e-01, -6.5295e-01,\n",
            "         -2.2152e-01,  6.4773e-02, -6.7545e-01, -6.4622e-02, -3.5777e-01,\n",
            "         -1.9668e-01, -2.5734e-03, -3.1064e-02, -3.0819e-01,  2.0039e-01,\n",
            "         -3.2611e-01, -2.7745e-02,  3.0442e-01,  2.0332e-01,  5.9444e-01,\n",
            "          4.6746e-01, -1.9955e-01,  2.9860e-01, -8.6102e-01,  4.8757e-01,\n",
            "         -1.6255e-01, -9.6138e-01, -5.6716e-01, -9.6071e-01,  4.1107e-01,\n",
            "         -9.7454e-02, -2.4426e-01,  7.9492e-01, -5.3850e-02,  2.1452e-01,\n",
            "          1.8702e-01, -6.3765e-01, -1.0000e+00, -5.2696e-01, -4.7050e-01,\n",
            "         -2.0727e-01, -2.0902e-01, -9.4851e-01, -9.1197e-01,  3.9638e-01,\n",
            "          8.6126e-01,  4.9645e-02,  9.9749e-01, -3.4668e-02,  8.8139e-01,\n",
            "         -1.6068e-01, -5.2212e-01,  1.6907e-01, -2.6948e-01,  6.0415e-01,\n",
            "          1.4466e-01, -1.7323e-01,  2.0375e-01, -2.5372e-01,  1.6346e-01,\n",
            "         -6.2434e-01, -1.1575e-01, -5.5981e-01, -8.1754e-01, -1.7136e-01,\n",
            "          7.8971e-01, -2.7698e-01, -7.7591e-01, -1.9746e-02, -1.3613e-01,\n",
            "         -1.6881e-01,  6.5003e-01,  5.7155e-01,  1.8255e-01, -5.1494e-01,\n",
            "          1.3206e-01,  9.3698e-02,  2.9460e-01, -6.7640e-01,  1.5969e-02,\n",
            "          1.8117e-01, -2.4216e-01, -6.7829e-01, -9.5357e-01, -2.8659e-01,\n",
            "          2.1231e-01,  9.6256e-01,  4.9640e-01,  1.0972e-02,  3.7878e-01,\n",
            "         -1.5442e-01,  6.0315e-01, -9.0397e-01,  9.4915e-01,  1.0161e-01,\n",
            "          2.7741e-01,  1.2999e-01,  5.7133e-02, -6.8242e-01, -3.4316e-02,\n",
            "          5.4810e-01, -2.5840e-01, -5.6630e-01,  7.8609e-02, -3.6824e-01,\n",
            "         -1.9255e-01, -4.3827e-01,  2.2899e-01, -1.2594e-01, -1.9039e-01,\n",
            "          1.0330e-01,  8.2528e-01,  8.5216e-01,  2.9570e-01, -3.2052e-02,\n",
            "          5.0069e-01, -5.8678e-01, -2.7780e-01, -4.3806e-02,  4.0292e-02,\n",
            "          3.6919e-02,  9.8200e-01, -5.2235e-01,  4.1790e-02, -7.1161e-01,\n",
            "         -9.5199e-01, -1.8443e-01, -8.2247e-01, -1.2485e-01, -4.1709e-01,\n",
            "          3.8312e-01,  1.3752e-01,  3.1198e-01,  3.2266e-01, -7.6625e-01,\n",
            "         -6.6214e-01,  2.7972e-01, -3.1089e-01,  2.9517e-01, -9.0211e-02,\n",
            "          8.2463e-01,  7.8153e-01, -4.9501e-01,  2.9576e-01,  8.8305e-01,\n",
            "         -6.1629e-01, -6.7444e-01,  3.2780e-01, -1.6576e-01,  7.3856e-01,\n",
            "         -3.5954e-01,  9.2015e-01,  7.1523e-01,  4.6242e-01, -6.7507e-01,\n",
            "         -5.5896e-01, -6.6492e-01, -5.2886e-01,  1.0127e-02, -1.3083e-01,\n",
            "          7.2672e-01,  5.3457e-01,  2.6115e-01,  3.8808e-01, -4.0041e-01,\n",
            "          9.1850e-01, -8.6934e-01, -9.1095e-01, -4.5688e-01, -8.1332e-02,\n",
            "         -9.7060e-01,  6.6835e-01,  2.4018e-01, -2.3272e-03, -2.6560e-01,\n",
            "         -4.6508e-01, -9.1282e-01,  2.6982e-01, -7.8031e-02,  8.7721e-01,\n",
            "         -3.9851e-03, -7.2624e-01, -4.9917e-01, -8.5242e-01, -1.9163e-01,\n",
            "         -1.3097e-01, -2.1517e-01, -1.9375e-01, -8.0720e-01,  4.5357e-01,\n",
            "          2.8432e-01,  2.4185e-01, -6.6742e-01,  9.6573e-01,  9.9998e-01,\n",
            "          9.3021e-01,  7.2556e-01,  5.2677e-01, -9.9574e-01, -5.6478e-01,\n",
            "          9.9985e-01, -9.0718e-01, -1.0000e+00, -7.9558e-01, -5.1865e-01,\n",
            "          7.9035e-02, -1.0000e+00, -1.8060e-01,  1.0884e-01, -7.6903e-01,\n",
            "          4.5003e-01,  9.3393e-01,  8.5579e-01, -1.0000e+00,  5.4175e-01,\n",
            "          7.2312e-01, -6.3156e-01,  7.6729e-01, -2.5585e-01,  9.0828e-01,\n",
            "          6.0744e-01,  4.8553e-01, -2.0359e-03,  4.0524e-01, -8.4604e-01,\n",
            "         -6.8491e-01, -3.2227e-01, -5.1543e-01,  9.6009e-01,  4.4804e-02,\n",
            "         -6.5639e-01, -5.7275e-01,  1.1709e-01, -1.0803e-01, -3.7503e-01,\n",
            "         -8.6713e-01, -3.1502e-02,  4.3811e-01,  6.2163e-01,  4.3794e-02,\n",
            "          2.7576e-01, -3.3046e-01,  2.2912e-01, -7.7090e-02, -1.2111e-01,\n",
            "          6.3190e-01, -8.5843e-01,  8.4236e-02, -4.3655e-01, -2.3148e-01,\n",
            "         -4.9286e-01, -9.1069e-01,  8.6572e-01, -2.0373e-01,  6.7564e-01,\n",
            "          1.0000e+00,  1.3097e-01, -4.6676e-01,  5.2293e-01,  8.5840e-02,\n",
            "         -4.8880e-01,  1.0000e+00,  6.7043e-01, -9.4883e-01, -4.2900e-01,\n",
            "          4.7165e-01, -3.7119e-01, -3.2580e-01,  9.9499e-01, -1.2923e-01,\n",
            "         -4.8132e-01, -2.0769e-01,  9.6874e-01, -9.7920e-01,  9.6258e-01,\n",
            "         -7.5474e-01, -8.7541e-01,  8.7862e-01,  8.2970e-01, -6.1189e-01,\n",
            "         -5.4643e-01, -9.4839e-02, -3.8934e-01,  2.1436e-01, -7.7527e-01,\n",
            "          5.1527e-01,  2.7361e-01,  3.0680e-02,  7.1157e-01, -5.0059e-01,\n",
            "         -5.5945e-01,  2.1258e-01, -6.1338e-01, -5.5492e-02,  6.7398e-01,\n",
            "          1.9126e-01, -5.0282e-02, -1.3445e-01, -2.0726e-01, -5.0245e-01,\n",
            "         -9.0151e-01,  3.5394e-01,  1.0000e+00, -2.1650e-01,  3.1793e-01,\n",
            "         -4.2444e-01,  5.9178e-02, -1.8732e-01,  3.9001e-01,  3.6780e-01,\n",
            "         -1.2621e-01, -5.3320e-01,  5.3999e-01, -7.5813e-01, -9.7184e-01,\n",
            "          2.7842e-01, -9.7130e-03, -9.6670e-02,  9.9892e-01,  5.0458e-01,\n",
            "         -2.2103e-03,  3.5107e-01,  8.8119e-01, -2.1016e-01,  7.1217e-02,\n",
            "          6.8513e-01,  9.3983e-01, -2.0730e-01,  5.7626e-01,  4.4697e-01,\n",
            "         -7.6783e-01, -1.9753e-01, -4.8650e-01, -9.6194e-02, -8.3298e-01,\n",
            "          1.4078e-01, -8.7860e-01,  9.2415e-01,  7.3551e-01,  2.1927e-01,\n",
            "          1.0640e-01,  2.7476e-01,  1.0000e+00, -6.6528e-01,  4.1291e-01,\n",
            "          1.2495e-01,  4.3975e-01, -9.9286e-01, -3.4445e-01, -3.2516e-01,\n",
            "          7.3681e-02, -6.2850e-01, -2.3524e-01,  1.5371e-01, -9.2570e-01,\n",
            "          6.3344e-01,  2.9594e-01, -8.3220e-01, -9.6519e-01,  1.6349e-01,\n",
            "          6.1003e-01, -6.3679e-02, -9.0778e-01, -5.8537e-01, -4.2400e-01,\n",
            "          5.8383e-01, -7.4695e-02, -8.1676e-01, -8.2715e-02, -1.4914e-01,\n",
            "          3.6018e-01, -4.9656e-02,  5.3372e-01,  6.2540e-01,  7.6789e-01,\n",
            "         -4.0139e-01, -3.5125e-01, -6.6431e-02, -7.8260e-01,  7.7157e-01,\n",
            "         -5.9458e-01, -5.6939e-01, -9.4887e-03,  1.0000e+00, -2.9348e-01,\n",
            "          7.5511e-01,  3.8142e-01,  5.2469e-01, -1.4734e-01,  1.5221e-01,\n",
            "          7.5597e-01,  1.7148e-01, -6.2536e-01, -6.7235e-01, -1.0737e-01,\n",
            "         -1.6067e-01,  6.3756e-01,  1.9387e-01,  3.2735e-01,  5.7008e-01,\n",
            "          5.7435e-01,  2.0523e-02, -5.6928e-02, -2.1889e-01,  9.7114e-01,\n",
            "          1.6660e-01,  2.5235e-01, -3.5053e-01, -4.0507e-02, -2.1047e-01,\n",
            "          2.3118e-01,  1.0000e+00,  4.7894e-02,  7.7283e-02, -9.7363e-01,\n",
            "         -6.7005e-01, -5.4437e-01,  9.9995e-01,  6.8491e-01, -4.3707e-01,\n",
            "          4.9285e-01,  5.8671e-01, -1.2336e-01,  4.0067e-01,  2.5705e-02,\n",
            "         -1.6539e-01,  1.2062e-01,  3.4357e-02,  8.3973e-01, -4.5903e-01,\n",
            "         -9.4741e-01, -3.8182e-01,  2.2837e-01, -8.5735e-01,  9.9785e-01,\n",
            "         -3.5189e-01, -9.5144e-02, -3.2952e-01,  1.0156e-01, -3.5378e-01,\n",
            "         -3.1064e-01, -9.3277e-01, -3.2652e-02,  1.1534e-01,  8.9445e-01,\n",
            "          1.6979e-01, -5.9186e-01, -7.8454e-01,  5.6049e-01,  6.1561e-01,\n",
            "         -6.4435e-01, -7.9978e-01,  8.9693e-01, -8.5389e-01,  5.5446e-01,\n",
            "          9.9999e-01,  4.1167e-01,  1.5251e-02,  2.8735e-02, -2.6557e-01,\n",
            "          1.5534e-01, -1.6695e-01,  5.1033e-01, -8.2452e-01, -1.9764e-01,\n",
            "         -4.8220e-02,  5.9540e-02,  1.3721e-01, -1.6338e-01,  3.7198e-01,\n",
            "          1.0035e-01, -5.1669e-01, -4.9756e-01,  1.7668e-01,  3.5680e-01,\n",
            "          6.8914e-01, -3.3896e-02, -6.0830e-02,  6.4042e-02, -7.1251e-02,\n",
            "         -5.9442e-01, -2.3089e-01, -7.9352e-02, -9.9941e-01,  6.8448e-01,\n",
            "         -1.0000e+00,  2.4732e-01, -8.8023e-02, -1.1025e-01,  6.4409e-01,\n",
            "          5.7990e-01,  2.0938e-01, -3.8242e-01, -6.5488e-01,  4.3847e-01,\n",
            "          3.9243e-01,  6.5031e-04, -3.4017e-01, -2.4770e-01,  9.5371e-02,\n",
            "          1.3634e-01,  5.1877e-02, -2.3358e-01,  5.2954e-01, -2.1276e-02,\n",
            "          1.0000e+00,  1.5087e-02, -4.8782e-01, -7.3923e-01, -4.7690e-02,\n",
            "         -5.6746e-02,  9.9999e-01, -6.7138e-01, -8.9480e-01,  8.7296e-02,\n",
            "         -5.4652e-01, -6.2224e-01,  2.1223e-01,  1.3434e-01, -6.0200e-01,\n",
            "         -7.7646e-01,  5.3168e-01,  6.9772e-01, -5.8482e-01,  4.2242e-01,\n",
            "         -8.4050e-02, -3.5102e-01, -1.8029e-02,  5.9790e-01,  9.5908e-01,\n",
            "          4.2043e-01,  6.7185e-01,  2.4436e-01,  2.7755e-02,  8.9088e-01,\n",
            "          1.4525e-01, -1.2372e-01, -7.3104e-02,  1.0000e+00,  1.3315e-01,\n",
            "         -8.3421e-01,  1.9348e-01, -9.1947e-01, -2.3921e-01, -8.3022e-01,\n",
            "          1.9877e-01,  7.8801e-02,  8.1223e-01, -1.9549e-02,  8.3314e-01,\n",
            "         -4.0831e-01, -2.5039e-01, -6.2919e-01, -1.2806e-01,  3.4521e-01,\n",
            "         -7.8399e-01, -9.6761e-01, -9.6705e-01,  4.3297e-01, -2.9919e-01,\n",
            "          2.8581e-01,  1.4164e-01, -8.2169e-02,  1.9780e-01,  3.0432e-01,\n",
            "         -1.0000e+00,  8.4570e-01,  3.8058e-01,  7.6800e-01,  9.0219e-01,\n",
            "          4.6636e-01,  4.4327e-01,  2.3800e-01, -9.6165e-01, -6.9558e-01,\n",
            "         -1.9920e-01, -1.4854e-01,  2.8880e-01,  4.6528e-01,  7.3396e-01,\n",
            "          2.4209e-01, -3.7560e-01, -1.5192e-01, -8.3633e-02, -8.0371e-01,\n",
            "         -9.8287e-01,  4.2324e-01, -3.4677e-01, -7.4999e-01,  8.7639e-01,\n",
            "         -2.6166e-01,  7.6618e-03,  2.4812e-02, -5.6862e-01,  5.0785e-01,\n",
            "          3.8895e-01,  1.2391e-01, -3.3086e-02,  2.3726e-01,  7.1853e-01,\n",
            "          8.2323e-01,  9.4423e-01, -6.6337e-01,  5.5639e-01, -1.4366e-01,\n",
            "          2.3200e-01,  8.6397e-01, -8.6899e-01,  7.6784e-02,  2.5245e-01,\n",
            "         -4.8728e-01,  1.1646e-01, -1.9686e-01, -7.3059e-01,  5.3488e-01,\n",
            "         -3.1996e-01,  4.6444e-01, -2.0987e-01,  2.1104e-01, -2.7378e-01,\n",
            "         -9.6330e-02, -5.4171e-01, -5.5118e-01,  6.5166e-01,  2.7540e-01,\n",
            "          8.0225e-01,  6.5623e-01,  3.0969e-02, -4.8690e-01, -1.4706e-01,\n",
            "         -5.2283e-01, -8.5914e-01,  5.2251e-01,  7.7876e-02, -6.0653e-03,\n",
            "          4.0463e-01, -1.5603e-01,  8.2713e-01,  3.0309e-01, -2.6406e-01,\n",
            "         -2.3227e-01, -5.8362e-01,  4.2719e-01, -4.8961e-01, -4.2118e-01,\n",
            "         -3.4990e-01,  4.3355e-01,  2.1706e-01,  9.9925e-01, -6.7501e-01,\n",
            "         -7.5686e-01, -4.1882e-01, -1.0328e-01,  3.0527e-01, -4.4992e-01,\n",
            "         -1.0000e+00,  1.0808e-02, -2.7007e-01,  6.2181e-01, -6.2234e-01,\n",
            "          7.4255e-01, -5.1941e-01, -8.4516e-01, -4.0100e-02,  3.4621e-01,\n",
            "          3.2283e-01, -3.4101e-01, -5.5714e-01,  5.4548e-01,  6.3813e-02,\n",
            "          8.4215e-01,  6.4200e-01, -7.0526e-01, -9.6538e-02,  6.1748e-01,\n",
            "         -4.9544e-01, -4.7459e-01,  7.1131e-01]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-10263504db3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# call the function to get semantically matching table names and columns names from the database using the BERT model output and question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mget_semantically_matching_tables_and_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# close the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-10263504db3c>\u001b[0m in \u001b[0;36mget_semantically_matching_tables_and_columns\u001b[0;34m(question_output, tables)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mcolumn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Similarity between \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the question and get the table names and columns names by semantic parsing using BERT model and tokenizer \n",
        "question = question.lower()\n",
        "question = question.replace(\"?\", \"\")\n",
        "\n",
        "# tokenize the question\n",
        "tokenized_question = tokenizer.tokenize(question)\n",
        "\n",
        "# get the table names and columns names by semantic parsing using BERT model and tokenizer\n",
        "table_names = []\n",
        "column_names = []\n",
        "for token in tokenized_question:\n",
        "    if token in tables:\n",
        "        table_names.append(token)\n",
        "    else:\n",
        "        cursor.execute(\"SELECT * FROM \" + table_names[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        if token in columns:\n",
        "            column_names.append(token)\n",
        "\n",
        "# print the table names and columns names\n",
        "print(\"Table names: \" + str(table_names))\n",
        "print(\"Column names: \" + str(column_names))\n",
        "\n",
        "\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3CHbsTvcwN4H",
        "outputId": "fd5f9348-0b2a-4bc2-b9a9-3c76297e8965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: list all the comedy movies\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-dfbcb1b67943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtable_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input English Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"List all movies where ?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Get the semantic representation of the question\n",
        "question_representation = output[0][0][0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GalHAddVy1z5",
        "outputId": "ef1ecb15-96dd-4655-8a1a-17298305560c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: list all the comedy movies?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input English Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the English question\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Generate enriched question using the semantic representation of the question and the table name and column names by semantic search and elastic search\n",
        "def generate_enriched_question(question, table_name, column_names):\n",
        "    # Generate a semantic representation of the table name and column names\n",
        "    input_ids = torch.tensor(tokenizer.encode(\n",
        "        table_name + \" \" + \" \".join(column_names))).unsqueeze(0)  # Batch size 1\n",
        "    output = model(input_ids)\n",
        "\n",
        "    # Get the semantic representation of the question\n",
        "    question_embedding = output[0][0][0]\n",
        "\n",
        "    # Get the semantic representation of the table name and column names\n",
        "    table_embedding = output[0][0][1:]\n",
        "\n",
        "    # Get the semantic representation of the table name and column names by semantic search and elastic search\n",
        "    enriched_question_embedding = question_embedding + table_embedding\n",
        "\n",
        "    # Generate enriched question using the semantic representation of the question and the table name and column names by semantic search and elastic search\n",
        "    enriched_question = tokenizer.decode(\n",
        "        torch.argmax(enriched_question_embedding, dim=0).tolist())\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "# Print the enriched question\n",
        "print(\"Enriched Question: \" + generate_enriched_question(\n",
        "    question, tables[0][0], tables[0][1:]))\n",
        "    \n",
        "\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A70AwVEmzM_5",
        "outputId": "a6c2348a-bd91-4614-c564-6dbc1d64e71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: List all the comedy movies?\n",
            "Enriched Question: [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [PAD] [PAD] [PAD] [PAD] [unused0] [PAD] [unused0] [unused0] [PAD] [unused0] [unused0] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [PAD] [unused0] [unused0] [unused0] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [PAD] [unused0] [PAD] [unused0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def print_tables_and_columns(tables):\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        print(\"Columns in table \" + table[0] + \":\")\n",
        "        print(columns)\n",
        "        print(\"\")\n",
        "    return\n",
        "\n",
        "# call the function to print all the table names and columns names in the database\n",
        "print_tables_and_columns(tables)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input English Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the English question\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# match the semantic representation of the question with the semantic representation of tables and columns in the database\n",
        "# and return the table name and column name with the highest similarity score\n",
        "def match_question_with_table_and_column(question, tables):\n",
        "    max_score = 0\n",
        "    table_name = \"\"\n",
        "    column_name = \"\"\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        for column in columns:\n",
        "            cursor.execute(\"SELECT \" + column + \" FROM \" + table[0])\n",
        "            rows = cursor.fetchall()\n",
        "            for row in rows:\n",
        "                input_ids = torch.tensor(tokenizer.encode(\n",
        "                    row[0])).unsqueeze(0)\n",
        "                output = model(input_ids)\n",
        "                score = torch.cosine_similarity(\n",
        "                    output[0][0], question[0][0], dim=0)\n",
        "                if score > max_score:\n",
        "                    max_score = score\n",
        "                    table_name = table[0]\n",
        "                    column_name = column\n",
        "    return table_name, column_name\n",
        "\n",
        "# Print matched table name and column name\n",
        "table_name, column_name = match_question_with_table_and_column(\n",
        "    output, tables)\n",
        "print(\"Table name: \" + table_name)\n",
        "print(\"Column name: \" + column_name)\n",
        "\n",
        "\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BIZjp1dq0FTB",
        "outputId": "c2fb1c53-f8fb-4992-d9ec-02588637dcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: What is the last comedy movie?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-772932049c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Print matched table name and column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m table_name, column_name = match_question_with_table_and_column(\n\u001b[0m\u001b[1;32m     66\u001b[0m     output, tables)\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Table name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-772932049c11>\u001b[0m in \u001b[0;36mmatch_question_with_table_and_column\u001b[0;34m(question, tables)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" FROM \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: near \"index\": syntax error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# function to print all the table names and columns names in the database\n",
        "def get_tables_and_columns(tables):\n",
        "    tables_and_columns = {}\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        tables_and_columns[table[0]] = columns\n",
        "    return tables_and_columns\n",
        "\n",
        "tables_and_columns = get_tables_and_columns(tables)\n",
        "print(tables_and_columns)\n",
        "\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input English Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the English question\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "\n",
        "# close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "5bqyJSZT0uFq",
        "outputId": "72188cf4-2056-413f-c9eb-470755ef98fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Movie': ['index', 'MID', 'title', 'year', 'rating', 'num_votes'], 'Genre': ['index', 'Name', 'GID'], 'Language': ['index', 'Name', 'LAID'], 'Country': ['index', 'Name', 'CID'], 'Location': ['index', 'Name', 'LID'], 'M_Location': ['index', 'MID', 'LID', 'ID'], 'M_Country': ['index', 'MID', 'CID', 'ID'], 'M_Language': ['index', 'MID', 'LAID', 'ID'], 'M_Genre': ['index', 'MID', 'GID', 'ID'], 'Person': ['index', 'PID', 'Name', 'Gender'], 'M_Producer': ['index', 'MID', 'PID', 'ID'], 'M_Director': ['index', 'MID', 'PID', 'ID'], 'M_Cast': ['index', 'MID', 'PID', 'ID']}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-1ea2aa2ba61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Get Input English Question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Preprocess the English question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "def get_tables_and_columns(cursor):\n",
        "    # Get all tables in the database\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = cursor.fetchall()\n",
        "\n",
        "    tables_and_columns = {}\n",
        "    for table in tables:\n",
        "        cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "        columns = [description[0] for description in cursor.description]\n",
        "        tables_and_columns[table[0]] = columns\n",
        "    return tables_and_columns\n",
        "\n",
        "def get_matching_tables_and_columns(output, tables_and_columns):\n",
        "    # TODO: Use the output of the BERT model to search the tables_and_columns dictionary\n",
        "    # for semantically matching table names and column names\n",
        "    matching_tables = []\n",
        "    matching_columns = []\n",
        "    return matching_tables, matching_columns\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables and columns in the database\n",
        "tables_and_columns = get_tables_and_columns(cursor)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input English Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the English question\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Get matching table names and column names\n",
        "matching_tables, matching_columns = get_matching_tables_and_columns(output, tables_and_columns)\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "print(matching_tables)\n",
        "\n",
        "print(\"Matching columns:\")\n",
        "print(matching_columns)\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4SCsvIv1cpq",
        "outputId": "b73ee30f-840d-407e-ea74-d85723e64ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: list all comedy movies?\n",
            "Matching tables:\n",
            "[]\n",
            "Matching columns:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"list all movie names?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Connect to the database and create a cursor\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all the table names in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Print the matching tables\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Get all the columns in the matching tables\n",
        "columns = []\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    cols = [description[0] for description in cursor.description]\n",
        "    columns.extend(cols)\n",
        "\n",
        "# Print the columns in the matching tables\n",
        "print(\"Columns in matching tables:\")\n",
        "print(columns)\n",
        "\n",
        "# Define a function to convert the question into an enriched question using the semantic representation of the question and the table names and columns in the database\n",
        "\n",
        "\n",
        "def enrich_question(question, output, tables, columns):\n",
        "    # Get the semantic representation of the question\n",
        "    question_embedding = output[0][0][0]\n",
        "\n",
        "    # Get the semantic representation of the table names\n",
        "    table_embeddings = []\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        table_embeddings.append(table_embedding)\n",
        "\n",
        "    # Get the semantic representation of the columns\n",
        "    column_embeddings = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        column_embeddings.append(column_embedding)\n",
        "\n",
        "    # Find the table name that is most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_table = \"\"\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        similarity = torch.cosine_similarity(\n",
        "            question_embedding, table_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_table = table[0]\n",
        "\n",
        "    # Find the columns that are most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_columns = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        similarity = torch.cosine_similarity(\n",
        "            question_embedding, column_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_columns = [column]\n",
        "        elif similarity == max_similarity:\n",
        "            max_similarity_columns.append(column)\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in max_similarity_columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-\n",
        "                                          2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "\n",
        "# Use the enriched question to search the database for matching rows\n",
        "enriched_question = enrich_question(question, output, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "# Execute the enriched question\n",
        "cursor.execute(enriched_question)\n",
        "results = cursor.fetchall()\n",
        "\n",
        "# Print the results\n",
        "print(\"Results:\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "4M7S2ugC6Fec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98069b91-ffdd-4453-af9b-527cd0203d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in matching tables:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes', 'index', 'Name', 'GID', 'index', 'Name', 'LAID', 'index', 'Name', 'CID', 'index', 'Name', 'LID', 'index', 'MID', 'LID', 'ID', 'index', 'MID', 'CID', 'ID', 'index', 'MID', 'LAID', 'ID', 'index', 'MID', 'GID', 'ID', 'index', 'PID', 'Name', 'Gender', 'index', 'MID', 'PID', 'ID', 'index', 'MID', 'PID', 'ID', 'index', 'MID', 'PID', 'ID']\n",
            "Enriched question:\n",
            "SELECT num_votes FROM Movie\n",
            "Results:\n",
            "(21967,)\n",
            "(110861,)\n",
            "(142585,)\n",
            "(1137529,)\n",
            "(7483,)\n",
            "(1970,)\n",
            "(536641,)\n",
            "(18160,)\n",
            "(170216,)\n",
            "(681,)\n",
            "(72375,)\n",
            "(729879,)\n",
            "(320472,)\n",
            "(2205,)\n",
            "(163278,)\n",
            "(1457,)\n",
            "(518138,)\n",
            "(184585,)\n",
            "(116089,)\n",
            "(12996,)\n",
            "(11188,)\n",
            "(8418,)\n",
            "(160,)\n",
            "(161266,)\n",
            "(475,)\n",
            "(97876,)\n",
            "(111130,)\n",
            "(2770,)\n",
            "(71047,)\n",
            "(64493,)\n",
            "(1606,)\n",
            "(101213,)\n",
            "(169683,)\n",
            "(287848,)\n",
            "(133783,)\n",
            "(27161,)\n",
            "(130977,)\n",
            "(66899,)\n",
            "(66292,)\n",
            "(109930,)\n",
            "(84600,)\n",
            "(50603,)\n",
            "(35436,)\n",
            "(23051,)\n",
            "(6333,)\n",
            "(1165,)\n",
            "(12749,)\n",
            "(746,)\n",
            "(49705,)\n",
            "(84136,)\n",
            "(77402,)\n",
            "(51491,)\n",
            "(20603,)\n",
            "(13771,)\n",
            "(10592,)\n",
            "(90179,)\n",
            "(6885,)\n",
            "(88262,)\n",
            "(275,)\n",
            "(2370,)\n",
            "(1345,)\n",
            "(45291,)\n",
            "(97421,)\n",
            "(4847,)\n",
            "(15869,)\n",
            "(90115,)\n",
            "(21616,)\n",
            "(61978,)\n",
            "(3171,)\n",
            "(376,)\n",
            "(34025,)\n",
            "(7678,)\n",
            "(3493,)\n",
            "(13599,)\n",
            "(4446,)\n",
            "(27282,)\n",
            "(21636,)\n",
            "(40342,)\n",
            "(1000,)\n",
            "(41224,)\n",
            "(16385,)\n",
            "(6814,)\n",
            "(13693,)\n",
            "(66919,)\n",
            "(9261,)\n",
            "(1123,)\n",
            "(18984,)\n",
            "(4273,)\n",
            "(18827,)\n",
            "(7520,)\n",
            "(6025,)\n",
            "(15181,)\n",
            "(589,)\n",
            "(38633,)\n",
            "(17190,)\n",
            "(5008,)\n",
            "(939,)\n",
            "(907,)\n",
            "(30352,)\n",
            "(11177,)\n",
            "(2971,)\n",
            "(12432,)\n",
            "(1099,)\n",
            "(9462,)\n",
            "(12186,)\n",
            "(8660,)\n",
            "(23970,)\n",
            "(12127,)\n",
            "(35263,)\n",
            "(50219,)\n",
            "(408,)\n",
            "(56040,)\n",
            "(47517,)\n",
            "(60969,)\n",
            "(3875,)\n",
            "(7530,)\n",
            "(19962,)\n",
            "(8962,)\n",
            "(21805,)\n",
            "(8455,)\n",
            "(21488,)\n",
            "(30225,)\n",
            "(2850,)\n",
            "(9769,)\n",
            "(15508,)\n",
            "(46484,)\n",
            "(33456,)\n",
            "(7561,)\n",
            "(17713,)\n",
            "(5538,)\n",
            "(30231,)\n",
            "(10180,)\n",
            "(51007,)\n",
            "(12486,)\n",
            "(12285,)\n",
            "(9901,)\n",
            "(177,)\n",
            "(39578,)\n",
            "(14820,)\n",
            "(66187,)\n",
            "(16086,)\n",
            "(2967,)\n",
            "(32109,)\n",
            "(13815,)\n",
            "(213,)\n",
            "(56133,)\n",
            "(25358,)\n",
            "(10354,)\n",
            "(28878,)\n",
            "(17804,)\n",
            "(56080,)\n",
            "(18959,)\n",
            "(151,)\n",
            "(21640,)\n",
            "(8813,)\n",
            "(28043,)\n",
            "(21795,)\n",
            "(2378,)\n",
            "(35343,)\n",
            "(18393,)\n",
            "(15433,)\n",
            "(33455,)\n",
            "(34131,)\n",
            "(41863,)\n",
            "(40758,)\n",
            "(9511,)\n",
            "(15884,)\n",
            "(44746,)\n",
            "(2807,)\n",
            "(11886,)\n",
            "(316,)\n",
            "(43753,)\n",
            "(23791,)\n",
            "(23651,)\n",
            "(25928,)\n",
            "(3918,)\n",
            "(34920,)\n",
            "(21247,)\n",
            "(6096,)\n",
            "(56579,)\n",
            "(39110,)\n",
            "(5443,)\n",
            "(28578,)\n",
            "(3823,)\n",
            "(23703,)\n",
            "(32668,)\n",
            "(7913,)\n",
            "(8489,)\n",
            "(72397,)\n",
            "(1576,)\n",
            "(4221,)\n",
            "(47551,)\n",
            "(4179,)\n",
            "(1122,)\n",
            "(6809,)\n",
            "(1621,)\n",
            "(45175,)\n",
            "(572,)\n",
            "(721,)\n",
            "(4956,)\n",
            "(125,)\n",
            "(4020,)\n",
            "(64275,)\n",
            "(53328,)\n",
            "(43370,)\n",
            "(15500,)\n",
            "(13113,)\n",
            "(13768,)\n",
            "(20855,)\n",
            "(17893,)\n",
            "(1486,)\n",
            "(3719,)\n",
            "(5106,)\n",
            "(168,)\n",
            "(2737,)\n",
            "(5404,)\n",
            "(14292,)\n",
            "(5188,)\n",
            "(1052,)\n",
            "(62149,)\n",
            "(46737,)\n",
            "(2890,)\n",
            "(1472,)\n",
            "(24031,)\n",
            "(7505,)\n",
            "(1549,)\n",
            "(668,)\n",
            "(2328,)\n",
            "(7924,)\n",
            "(202,)\n",
            "(73,)\n",
            "(7599,)\n",
            "(10035,)\n",
            "(15535,)\n",
            "(14881,)\n",
            "(19118,)\n",
            "(7352,)\n",
            "(43542,)\n",
            "(10704,)\n",
            "(18433,)\n",
            "(515,)\n",
            "(14094,)\n",
            "(37639,)\n",
            "(847,)\n",
            "(684,)\n",
            "(17080,)\n",
            "(10537,)\n",
            "(41975,)\n",
            "(99,)\n",
            "(6134,)\n",
            "(2781,)\n",
            "(23993,)\n",
            "(12356,)\n",
            "(30993,)\n",
            "(2793,)\n",
            "(11149,)\n",
            "(13979,)\n",
            "(6586,)\n",
            "(1232,)\n",
            "(23969,)\n",
            "(14871,)\n",
            "(11855,)\n",
            "(30754,)\n",
            "(10670,)\n",
            "(3596,)\n",
            "(4815,)\n",
            "(12019,)\n",
            "(1294,)\n",
            "(2688,)\n",
            "(3086,)\n",
            "(615,)\n",
            "(4380,)\n",
            "(12040,)\n",
            "(6715,)\n",
            "(59380,)\n",
            "(5409,)\n",
            "(9661,)\n",
            "(12356,)\n",
            "(8131,)\n",
            "(3143,)\n",
            "(3018,)\n",
            "(9236,)\n",
            "(661,)\n",
            "(15972,)\n",
            "(25626,)\n",
            "(4917,)\n",
            "(28435,)\n",
            "(4462,)\n",
            "(33976,)\n",
            "(18773,)\n",
            "(19424,)\n",
            "(12722,)\n",
            "(3344,)\n",
            "(22562,)\n",
            "(2428,)\n",
            "(2418,)\n",
            "(6568,)\n",
            "(16627,)\n",
            "(3794,)\n",
            "(2635,)\n",
            "(52802,)\n",
            "(5276,)\n",
            "(19300,)\n",
            "(2830,)\n",
            "(4150,)\n",
            "(347,)\n",
            "(6783,)\n",
            "(365,)\n",
            "(18342,)\n",
            "(5486,)\n",
            "(46252,)\n",
            "(4125,)\n",
            "(8419,)\n",
            "(9847,)\n",
            "(1243,)\n",
            "(14922,)\n",
            "(469,)\n",
            "(23879,)\n",
            "(21230,)\n",
            "(5403,)\n",
            "(20300,)\n",
            "(16374,)\n",
            "(7610,)\n",
            "(2389,)\n",
            "(9888,)\n",
            "(17500,)\n",
            "(5458,)\n",
            "(26011,)\n",
            "(4612,)\n",
            "(11935,)\n",
            "(4358,)\n",
            "(28078,)\n",
            "(13733,)\n",
            "(37942,)\n",
            "(9077,)\n",
            "(27042,)\n",
            "(6944,)\n",
            "(11406,)\n",
            "(2073,)\n",
            "(1495,)\n",
            "(8426,)\n",
            "(4128,)\n",
            "(146,)\n",
            "(11244,)\n",
            "(2798,)\n",
            "(422,)\n",
            "(1261,)\n",
            "(2902,)\n",
            "(741,)\n",
            "(291,)\n",
            "(4936,)\n",
            "(2631,)\n",
            "(4028,)\n",
            "(15105,)\n",
            "(22497,)\n",
            "(44368,)\n",
            "(13191,)\n",
            "(18497,)\n",
            "(2275,)\n",
            "(5134,)\n",
            "(10433,)\n",
            "(4376,)\n",
            "(9858,)\n",
            "(2811,)\n",
            "(97,)\n",
            "(4341,)\n",
            "(22636,)\n",
            "(6143,)\n",
            "(20584,)\n",
            "(6577,)\n",
            "(29331,)\n",
            "(2184,)\n",
            "(8592,)\n",
            "(8859,)\n",
            "(245,)\n",
            "(1081,)\n",
            "(537,)\n",
            "(3626,)\n",
            "(4091,)\n",
            "(1298,)\n",
            "(18668,)\n",
            "(2536,)\n",
            "(14131,)\n",
            "(3513,)\n",
            "(11397,)\n",
            "(554,)\n",
            "(9326,)\n",
            "(257,)\n",
            "(1550,)\n",
            "(5202,)\n",
            "(279,)\n",
            "(11186,)\n",
            "(425,)\n",
            "(376,)\n",
            "(1382,)\n",
            "(5456,)\n",
            "(17945,)\n",
            "(2363,)\n",
            "(897,)\n",
            "(23056,)\n",
            "(6208,)\n",
            "(11357,)\n",
            "(3144,)\n",
            "(1698,)\n",
            "(4382,)\n",
            "(3206,)\n",
            "(105,)\n",
            "(4107,)\n",
            "(4986,)\n",
            "(3514,)\n",
            "(6723,)\n",
            "(500,)\n",
            "(1403,)\n",
            "(11587,)\n",
            "(14876,)\n",
            "(1374,)\n",
            "(7275,)\n",
            "(2240,)\n",
            "(18311,)\n",
            "(7727,)\n",
            "(6771,)\n",
            "(3086,)\n",
            "(14778,)\n",
            "(2379,)\n",
            "(1295,)\n",
            "(63,)\n",
            "(9447,)\n",
            "(2523,)\n",
            "(1693,)\n",
            "(1721,)\n",
            "(5235,)\n",
            "(2834,)\n",
            "(5686,)\n",
            "(6041,)\n",
            "(3582,)\n",
            "(19070,)\n",
            "(6237,)\n",
            "(455,)\n",
            "(3594,)\n",
            "(2812,)\n",
            "(3243,)\n",
            "(9063,)\n",
            "(184,)\n",
            "(3403,)\n",
            "(2906,)\n",
            "(3740,)\n",
            "(2286,)\n",
            "(138,)\n",
            "(20733,)\n",
            "(3863,)\n",
            "(1326,)\n",
            "(13913,)\n",
            "(17299,)\n",
            "(2028,)\n",
            "(670,)\n",
            "(14881,)\n",
            "(11015,)\n",
            "(273,)\n",
            "(4446,)\n",
            "(5303,)\n",
            "(937,)\n",
            "(1236,)\n",
            "(692,)\n",
            "(6336,)\n",
            "(5615,)\n",
            "(790,)\n",
            "(3000,)\n",
            "(3087,)\n",
            "(776,)\n",
            "(243,)\n",
            "(802,)\n",
            "(3209,)\n",
            "(2746,)\n",
            "(3798,)\n",
            "(1485,)\n",
            "(10671,)\n",
            "(6493,)\n",
            "(1174,)\n",
            "(12189,)\n",
            "(21132,)\n",
            "(7540,)\n",
            "(7090,)\n",
            "(9482,)\n",
            "(1239,)\n",
            "(204,)\n",
            "(13515,)\n",
            "(2383,)\n",
            "(816,)\n",
            "(167,)\n",
            "(12953,)\n",
            "(11321,)\n",
            "(343,)\n",
            "(1297,)\n",
            "(11209,)\n",
            "(214,)\n",
            "(69,)\n",
            "(13162,)\n",
            "(146,)\n",
            "(14114,)\n",
            "(2838,)\n",
            "(499,)\n",
            "(7010,)\n",
            "(161,)\n",
            "(531,)\n",
            "(3365,)\n",
            "(4123,)\n",
            "(1904,)\n",
            "(319,)\n",
            "(17696,)\n",
            "(12145,)\n",
            "(853,)\n",
            "(415,)\n",
            "(5768,)\n",
            "(11052,)\n",
            "(2192,)\n",
            "(240,)\n",
            "(2950,)\n",
            "(9764,)\n",
            "(19832,)\n",
            "(4282,)\n",
            "(4465,)\n",
            "(296,)\n",
            "(9018,)\n",
            "(19048,)\n",
            "(2075,)\n",
            "(1764,)\n",
            "(36953,)\n",
            "(288,)\n",
            "(5755,)\n",
            "(1867,)\n",
            "(14620,)\n",
            "(1583,)\n",
            "(2854,)\n",
            "(2163,)\n",
            "(6495,)\n",
            "(15677,)\n",
            "(5459,)\n",
            "(4463,)\n",
            "(4218,)\n",
            "(5264,)\n",
            "(5466,)\n",
            "(12292,)\n",
            "(11746,)\n",
            "(7835,)\n",
            "(6483,)\n",
            "(6427,)\n",
            "(37578,)\n",
            "(4074,)\n",
            "(4572,)\n",
            "(1228,)\n",
            "(1270,)\n",
            "(6418,)\n",
            "(8684,)\n",
            "(3388,)\n",
            "(3720,)\n",
            "(19770,)\n",
            "(4697,)\n",
            "(8137,)\n",
            "(8969,)\n",
            "(936,)\n",
            "(3269,)\n",
            "(1230,)\n",
            "(9989,)\n",
            "(9964,)\n",
            "(11244,)\n",
            "(9535,)\n",
            "(22160,)\n",
            "(1838,)\n",
            "(489,)\n",
            "(5143,)\n",
            "(4400,)\n",
            "(3390,)\n",
            "(10802,)\n",
            "(7208,)\n",
            "(7927,)\n",
            "(24537,)\n",
            "(5891,)\n",
            "(250,)\n",
            "(267,)\n",
            "(2900,)\n",
            "(7101,)\n",
            "(5596,)\n",
            "(9618,)\n",
            "(4500,)\n",
            "(8127,)\n",
            "(1277,)\n",
            "(7260,)\n",
            "(3465,)\n",
            "(174,)\n",
            "(10475,)\n",
            "(13167,)\n",
            "(256,)\n",
            "(4802,)\n",
            "(2736,)\n",
            "(11276,)\n",
            "(2587,)\n",
            "(5927,)\n",
            "(1812,)\n",
            "(4452,)\n",
            "(16301,)\n",
            "(501,)\n",
            "(5469,)\n",
            "(11558,)\n",
            "(2334,)\n",
            "(2315,)\n",
            "(4470,)\n",
            "(2999,)\n",
            "(10728,)\n",
            "(1472,)\n",
            "(11489,)\n",
            "(5834,)\n",
            "(5154,)\n",
            "(9232,)\n",
            "(7327,)\n",
            "(3721,)\n",
            "(158,)\n",
            "(15331,)\n",
            "(14898,)\n",
            "(5239,)\n",
            "(999,)\n",
            "(6197,)\n",
            "(6625,)\n",
            "(431,)\n",
            "(21507,)\n",
            "(4021,)\n",
            "(6899,)\n",
            "(1893,)\n",
            "(1679,)\n",
            "(4791,)\n",
            "(4659,)\n",
            "(687,)\n",
            "(6628,)\n",
            "(2634,)\n",
            "(1707,)\n",
            "(2696,)\n",
            "(2819,)\n",
            "(1436,)\n",
            "(216,)\n",
            "(2154,)\n",
            "(1132,)\n",
            "(15525,)\n",
            "(5727,)\n",
            "(700,)\n",
            "(1095,)\n",
            "(493,)\n",
            "(1506,)\n",
            "(15607,)\n",
            "(16304,)\n",
            "(546,)\n",
            "(1434,)\n",
            "(1635,)\n",
            "(1444,)\n",
            "(1004,)\n",
            "(723,)\n",
            "(766,)\n",
            "(60,)\n",
            "(321,)\n",
            "(3259,)\n",
            "(2921,)\n",
            "(2982,)\n",
            "(124,)\n",
            "(2262,)\n",
            "(83,)\n",
            "(2079,)\n",
            "(445,)\n",
            "(676,)\n",
            "(698,)\n",
            "(1328,)\n",
            "(51,)\n",
            "(796,)\n",
            "(299,)\n",
            "(164,)\n",
            "(88,)\n",
            "(3565,)\n",
            "(9661,)\n",
            "(10656,)\n",
            "(880,)\n",
            "(1941,)\n",
            "(13176,)\n",
            "(365,)\n",
            "(196,)\n",
            "(745,)\n",
            "(748,)\n",
            "(2992,)\n",
            "(2666,)\n",
            "(2064,)\n",
            "(562,)\n",
            "(1916,)\n",
            "(482,)\n",
            "(961,)\n",
            "(3467,)\n",
            "(122,)\n",
            "(1700,)\n",
            "(1392,)\n",
            "(6976,)\n",
            "(12026,)\n",
            "(1167,)\n",
            "(2712,)\n",
            "(1520,)\n",
            "(2216,)\n",
            "(4619,)\n",
            "(11431,)\n",
            "(481,)\n",
            "(2080,)\n",
            "(239,)\n",
            "(1239,)\n",
            "(1061,)\n",
            "(4988,)\n",
            "(569,)\n",
            "(266,)\n",
            "(333,)\n",
            "(271,)\n",
            "(1800,)\n",
            "(581,)\n",
            "(5528,)\n",
            "(150,)\n",
            "(83,)\n",
            "(620,)\n",
            "(80,)\n",
            "(4498,)\n",
            "(4618,)\n",
            "(105,)\n",
            "(56,)\n",
            "(2254,)\n",
            "(613,)\n",
            "(8646,)\n",
            "(1382,)\n",
            "(445,)\n",
            "(236,)\n",
            "(9284,)\n",
            "(8625,)\n",
            "(14448,)\n",
            "(134,)\n",
            "(1779,)\n",
            "(149,)\n",
            "(1232,)\n",
            "(6568,)\n",
            "(455,)\n",
            "(12616,)\n",
            "(759,)\n",
            "(1578,)\n",
            "(257,)\n",
            "(197,)\n",
            "(2249,)\n",
            "(252,)\n",
            "(1664,)\n",
            "(5067,)\n",
            "(521,)\n",
            "(1094,)\n",
            "(54,)\n",
            "(849,)\n",
            "(5865,)\n",
            "(239,)\n",
            "(1067,)\n",
            "(3570,)\n",
            "(355,)\n",
            "(9314,)\n",
            "(85,)\n",
            "(350,)\n",
            "(6206,)\n",
            "(2474,)\n",
            "(2343,)\n",
            "(6344,)\n",
            "(524,)\n",
            "(1723,)\n",
            "(121,)\n",
            "(5690,)\n",
            "(144,)\n",
            "(581,)\n",
            "(815,)\n",
            "(6432,)\n",
            "(1637,)\n",
            "(361,)\n",
            "(1665,)\n",
            "(2319,)\n",
            "(2560,)\n",
            "(1957,)\n",
            "(151,)\n",
            "(156,)\n",
            "(2267,)\n",
            "(3217,)\n",
            "(265,)\n",
            "(587,)\n",
            "(651,)\n",
            "(316,)\n",
            "(639,)\n",
            "(1883,)\n",
            "(2560,)\n",
            "(333,)\n",
            "(1666,)\n",
            "(478,)\n",
            "(94,)\n",
            "(1803,)\n",
            "(212,)\n",
            "(1020,)\n",
            "(2238,)\n",
            "(8090,)\n",
            "(6049,)\n",
            "(1521,)\n",
            "(826,)\n",
            "(1075,)\n",
            "(9554,)\n",
            "(139,)\n",
            "(3778,)\n",
            "(677,)\n",
            "(304,)\n",
            "(1144,)\n",
            "(1732,)\n",
            "(231,)\n",
            "(5220,)\n",
            "(4711,)\n",
            "(59,)\n",
            "(92,)\n",
            "(957,)\n",
            "(1046,)\n",
            "(4447,)\n",
            "(2220,)\n",
            "(550,)\n",
            "(1256,)\n",
            "(1411,)\n",
            "(1237,)\n",
            "(587,)\n",
            "(527,)\n",
            "(3501,)\n",
            "(104,)\n",
            "(3544,)\n",
            "(6111,)\n",
            "(2242,)\n",
            "(1211,)\n",
            "(108,)\n",
            "(7322,)\n",
            "(194,)\n",
            "(869,)\n",
            "(764,)\n",
            "(1270,)\n",
            "(1409,)\n",
            "(1252,)\n",
            "(665,)\n",
            "(179,)\n",
            "(1312,)\n",
            "(11552,)\n",
            "(4196,)\n",
            "(310,)\n",
            "(610,)\n",
            "(6123,)\n",
            "(2714,)\n",
            "(5875,)\n",
            "(578,)\n",
            "(1025,)\n",
            "(202,)\n",
            "(726,)\n",
            "(2263,)\n",
            "(6098,)\n",
            "(104,)\n",
            "(72,)\n",
            "(177,)\n",
            "(3587,)\n",
            "(2041,)\n",
            "(6786,)\n",
            "(1137,)\n",
            "(373,)\n",
            "(3066,)\n",
            "(819,)\n",
            "(175,)\n",
            "(84,)\n",
            "(4566,)\n",
            "(84,)\n",
            "(1457,)\n",
            "(6888,)\n",
            "(552,)\n",
            "(311,)\n",
            "(114,)\n",
            "(386,)\n",
            "(180,)\n",
            "(2549,)\n",
            "(110,)\n",
            "(315,)\n",
            "(293,)\n",
            "(170,)\n",
            "(2043,)\n",
            "(59,)\n",
            "(4924,)\n",
            "(85,)\n",
            "(1525,)\n",
            "(118,)\n",
            "(686,)\n",
            "(2371,)\n",
            "(2163,)\n",
            "(2483,)\n",
            "(461,)\n",
            "(544,)\n",
            "(1848,)\n",
            "(2585,)\n",
            "(8132,)\n",
            "(3480,)\n",
            "(4108,)\n",
            "(642,)\n",
            "(1610,)\n",
            "(688,)\n",
            "(156,)\n",
            "(394,)\n",
            "(141,)\n",
            "(4159,)\n",
            "(18480,)\n",
            "(13954,)\n",
            "(4463,)\n",
            "(119,)\n",
            "(1741,)\n",
            "(112,)\n",
            "(1398,)\n",
            "(1945,)\n",
            "(11406,)\n",
            "(6797,)\n",
            "(7895,)\n",
            "(5538,)\n",
            "(480,)\n",
            "(1547,)\n",
            "(2085,)\n",
            "(1472,)\n",
            "(2821,)\n",
            "(2464,)\n",
            "(69,)\n",
            "(15100,)\n",
            "(2185,)\n",
            "(196,)\n",
            "(1852,)\n",
            "(356,)\n",
            "(3622,)\n",
            "(565,)\n",
            "(9370,)\n",
            "(1891,)\n",
            "(185,)\n",
            "(2331,)\n",
            "(551,)\n",
            "(1508,)\n",
            "(118,)\n",
            "(1547,)\n",
            "(1274,)\n",
            "(3721,)\n",
            "(2021,)\n",
            "(670,)\n",
            "(340,)\n",
            "(77,)\n",
            "(161,)\n",
            "(978,)\n",
            "(1390,)\n",
            "(1863,)\n",
            "(5306,)\n",
            "(6498,)\n",
            "(12222,)\n",
            "(10310,)\n",
            "(9734,)\n",
            "(318,)\n",
            "(326,)\n",
            "(100,)\n",
            "(182,)\n",
            "(3573,)\n",
            "(983,)\n",
            "(137,)\n",
            "(13143,)\n",
            "(2316,)\n",
            "(10340,)\n",
            "(740,)\n",
            "(2243,)\n",
            "(261,)\n",
            "(1460,)\n",
            "(250,)\n",
            "(734,)\n",
            "(72,)\n",
            "(2112,)\n",
            "(3085,)\n",
            "(2278,)\n",
            "(368,)\n",
            "(5264,)\n",
            "(110,)\n",
            "(1167,)\n",
            "(55,)\n",
            "(7326,)\n",
            "(5185,)\n",
            "(4245,)\n",
            "(2566,)\n",
            "(375,)\n",
            "(121,)\n",
            "(548,)\n",
            "(733,)\n",
            "(56,)\n",
            "(223,)\n",
            "(4921,)\n",
            "(1383,)\n",
            "(4840,)\n",
            "(497,)\n",
            "(97,)\n",
            "(83,)\n",
            "(225,)\n",
            "(8407,)\n",
            "(3526,)\n",
            "(497,)\n",
            "(747,)\n",
            "(345,)\n",
            "(3351,)\n",
            "(8820,)\n",
            "(119,)\n",
            "(329,)\n",
            "(129,)\n",
            "(8808,)\n",
            "(271,)\n",
            "(234,)\n",
            "(6435,)\n",
            "(64,)\n",
            "(159,)\n",
            "(2207,)\n",
            "(5506,)\n",
            "(9078,)\n",
            "(1611,)\n",
            "(8884,)\n",
            "(1726,)\n",
            "(95,)\n",
            "(64,)\n",
            "(3551,)\n",
            "(513,)\n",
            "(1308,)\n",
            "(82,)\n",
            "(836,)\n",
            "(99,)\n",
            "(3402,)\n",
            "(407,)\n",
            "(832,)\n",
            "(141,)\n",
            "(174,)\n",
            "(1053,)\n",
            "(4528,)\n",
            "(2183,)\n",
            "(2080,)\n",
            "(1256,)\n",
            "(1116,)\n",
            "(168,)\n",
            "(1127,)\n",
            "(431,)\n",
            "(1005,)\n",
            "(281,)\n",
            "(172,)\n",
            "(908,)\n",
            "(61,)\n",
            "(4263,)\n",
            "(2437,)\n",
            "(1487,)\n",
            "(329,)\n",
            "(163,)\n",
            "(100,)\n",
            "(958,)\n",
            "(2233,)\n",
            "(459,)\n",
            "(944,)\n",
            "(455,)\n",
            "(212,)\n",
            "(944,)\n",
            "(6041,)\n",
            "(1395,)\n",
            "(582,)\n",
            "(104,)\n",
            "(1101,)\n",
            "(177,)\n",
            "(847,)\n",
            "(186,)\n",
            "(1685,)\n",
            "(1433,)\n",
            "(1661,)\n",
            "(674,)\n",
            "(1153,)\n",
            "(3575,)\n",
            "(1257,)\n",
            "(98,)\n",
            "(9734,)\n",
            "(8927,)\n",
            "(6358,)\n",
            "(1571,)\n",
            "(223,)\n",
            "(3089,)\n",
            "(679,)\n",
            "(127,)\n",
            "(686,)\n",
            "(594,)\n",
            "(52,)\n",
            "(751,)\n",
            "(908,)\n",
            "(227,)\n",
            "(432,)\n",
            "(290,)\n",
            "(499,)\n",
            "(12508,)\n",
            "(2655,)\n",
            "(4461,)\n",
            "(515,)\n",
            "(709,)\n",
            "(2658,)\n",
            "(2420,)\n",
            "(3361,)\n",
            "(1048,)\n",
            "(785,)\n",
            "(351,)\n",
            "(243,)\n",
            "(1427,)\n",
            "(2395,)\n",
            "(940,)\n",
            "(1360,)\n",
            "(3559,)\n",
            "(5496,)\n",
            "(62,)\n",
            "(1656,)\n",
            "(785,)\n",
            "(386,)\n",
            "(1304,)\n",
            "(535,)\n",
            "(2921,)\n",
            "(4687,)\n",
            "(82,)\n",
            "(167,)\n",
            "(752,)\n",
            "(1015,)\n",
            "(326,)\n",
            "(230,)\n",
            "(99,)\n",
            "(6050,)\n",
            "(440,)\n",
            "(116,)\n",
            "(196,)\n",
            "(447,)\n",
            "(280,)\n",
            "(705,)\n",
            "(1484,)\n",
            "(1080,)\n",
            "(69,)\n",
            "(13533,)\n",
            "(251,)\n",
            "(3294,)\n",
            "(750,)\n",
            "(524,)\n",
            "(3080,)\n",
            "(62,)\n",
            "(402,)\n",
            "(370,)\n",
            "(1165,)\n",
            "(1561,)\n",
            "(9364,)\n",
            "(2008,)\n",
            "(9674,)\n",
            "(923,)\n",
            "(1087,)\n",
            "(1031,)\n",
            "(333,)\n",
            "(2101,)\n",
            "(2670,)\n",
            "(9008,)\n",
            "(629,)\n",
            "(395,)\n",
            "(121,)\n",
            "(76,)\n",
            "(3114,)\n",
            "(2118,)\n",
            "(133,)\n",
            "(1801,)\n",
            "(235,)\n",
            "(222,)\n",
            "(962,)\n",
            "(963,)\n",
            "(1635,)\n",
            "(247,)\n",
            "(660,)\n",
            "(708,)\n",
            "(107,)\n",
            "(911,)\n",
            "(137,)\n",
            "(220,)\n",
            "(529,)\n",
            "(8563,)\n",
            "(2916,)\n",
            "(3723,)\n",
            "(6521,)\n",
            "(5481,)\n",
            "(635,)\n",
            "(99,)\n",
            "(386,)\n",
            "(741,)\n",
            "(82,)\n",
            "(207,)\n",
            "(762,)\n",
            "(217,)\n",
            "(2340,)\n",
            "(758,)\n",
            "(279,)\n",
            "(1202,)\n",
            "(96,)\n",
            "(2189,)\n",
            "(3741,)\n",
            "(1483,)\n",
            "(5517,)\n",
            "(4135,)\n",
            "(3667,)\n",
            "(458,)\n",
            "(890,)\n",
            "(1526,)\n",
            "(83,)\n",
            "(1880,)\n",
            "(3386,)\n",
            "(1576,)\n",
            "(880,)\n",
            "(297,)\n",
            "(960,)\n",
            "(691,)\n",
            "(887,)\n",
            "(2190,)\n",
            "(4396,)\n",
            "(13314,)\n",
            "(1264,)\n",
            "(1473,)\n",
            "(69,)\n",
            "(1441,)\n",
            "(1530,)\n",
            "(99,)\n",
            "(102,)\n",
            "(2019,)\n",
            "(135,)\n",
            "(2257,)\n",
            "(2540,)\n",
            "(217,)\n",
            "(794,)\n",
            "(489,)\n",
            "(386,)\n",
            "(4578,)\n",
            "(6631,)\n",
            "(10925,)\n",
            "(3285,)\n",
            "(181,)\n",
            "(1579,)\n",
            "(373,)\n",
            "(1757,)\n",
            "(1354,)\n",
            "(311,)\n",
            "(52,)\n",
            "(4445,)\n",
            "(2755,)\n",
            "(2828,)\n",
            "(102,)\n",
            "(307,)\n",
            "(1924,)\n",
            "(820,)\n",
            "(2219,)\n",
            "(1434,)\n",
            "(135,)\n",
            "(1225,)\n",
            "(574,)\n",
            "(212,)\n",
            "(623,)\n",
            "(52,)\n",
            "(77,)\n",
            "(771,)\n",
            "(208,)\n",
            "(241,)\n",
            "(4223,)\n",
            "(348,)\n",
            "(184,)\n",
            "(399,)\n",
            "(1180,)\n",
            "(280,)\n",
            "(284,)\n",
            "(13756,)\n",
            "(3946,)\n",
            "(8044,)\n",
            "(4469,)\n",
            "(1733,)\n",
            "(521,)\n",
            "(2481,)\n",
            "(464,)\n",
            "(872,)\n",
            "(99,)\n",
            "(2283,)\n",
            "(321,)\n",
            "(166,)\n",
            "(289,)\n",
            "(419,)\n",
            "(8678,)\n",
            "(2321,)\n",
            "(1252,)\n",
            "(4045,)\n",
            "(448,)\n",
            "(1058,)\n",
            "(1364,)\n",
            "(405,)\n",
            "(645,)\n",
            "(190,)\n",
            "(318,)\n",
            "(474,)\n",
            "(134,)\n",
            "(2552,)\n",
            "(3632,)\n",
            "(1748,)\n",
            "(2196,)\n",
            "(578,)\n",
            "(400,)\n",
            "(110,)\n",
            "(323,)\n",
            "(351,)\n",
            "(1962,)\n",
            "(1183,)\n",
            "(1607,)\n",
            "(64,)\n",
            "(78,)\n",
            "(954,)\n",
            "(51,)\n",
            "(1044,)\n",
            "(175,)\n",
            "(4516,)\n",
            "(90,)\n",
            "(87,)\n",
            "(2544,)\n",
            "(956,)\n",
            "(946,)\n",
            "(946,)\n",
            "(704,)\n",
            "(94,)\n",
            "(156,)\n",
            "(750,)\n",
            "(745,)\n",
            "(896,)\n",
            "(2041,)\n",
            "(1884,)\n",
            "(1203,)\n",
            "(959,)\n",
            "(2786,)\n",
            "(261,)\n",
            "(1417,)\n",
            "(159,)\n",
            "(703,)\n",
            "(1387,)\n",
            "(9894,)\n",
            "(8016,)\n",
            "(122,)\n",
            "(71,)\n",
            "(69,)\n",
            "(854,)\n",
            "(1133,)\n",
            "(2119,)\n",
            "(3420,)\n",
            "(53,)\n",
            "(704,)\n",
            "(10627,)\n",
            "(73,)\n",
            "(2300,)\n",
            "(533,)\n",
            "(1418,)\n",
            "(516,)\n",
            "(118,)\n",
            "(140,)\n",
            "(140,)\n",
            "(334,)\n",
            "(998,)\n",
            "(2321,)\n",
            "(2716,)\n",
            "(120,)\n",
            "(155,)\n",
            "(388,)\n",
            "(100,)\n",
            "(502,)\n",
            "(104,)\n",
            "(614,)\n",
            "(1671,)\n",
            "(1162,)\n",
            "(879,)\n",
            "(219,)\n",
            "(230,)\n",
            "(92,)\n",
            "(4969,)\n",
            "(264,)\n",
            "(123,)\n",
            "(83,)\n",
            "(3339,)\n",
            "(3340,)\n",
            "(4856,)\n",
            "(2570,)\n",
            "(130,)\n",
            "(1861,)\n",
            "(2083,)\n",
            "(87,)\n",
            "(76,)\n",
            "(1425,)\n",
            "(821,)\n",
            "(60,)\n",
            "(2518,)\n",
            "(1363,)\n",
            "(1567,)\n",
            "(315,)\n",
            "(140,)\n",
            "(80,)\n",
            "(172,)\n",
            "(255,)\n",
            "(348,)\n",
            "(520,)\n",
            "(780,)\n",
            "(283,)\n",
            "(71,)\n",
            "(611,)\n",
            "(483,)\n",
            "(52,)\n",
            "(845,)\n",
            "(96,)\n",
            "(351,)\n",
            "(216,)\n",
            "(737,)\n",
            "(776,)\n",
            "(515,)\n",
            "(4738,)\n",
            "(1010,)\n",
            "(1770,)\n",
            "(62,)\n",
            "(70,)\n",
            "(67,)\n",
            "(53,)\n",
            "(709,)\n",
            "(257,)\n",
            "(324,)\n",
            "(106,)\n",
            "(70,)\n",
            "(544,)\n",
            "(344,)\n",
            "(708,)\n",
            "(175,)\n",
            "(326,)\n",
            "(507,)\n",
            "(1217,)\n",
            "(1034,)\n",
            "(354,)\n",
            "(1160,)\n",
            "(283,)\n",
            "(2163,)\n",
            "(3118,)\n",
            "(536,)\n",
            "(2784,)\n",
            "(1148,)\n",
            "(388,)\n",
            "(754,)\n",
            "(103,)\n",
            "(88,)\n",
            "(64,)\n",
            "(163,)\n",
            "(54,)\n",
            "(54,)\n",
            "(280,)\n",
            "(821,)\n",
            "(156,)\n",
            "(124,)\n",
            "(918,)\n",
            "(158,)\n",
            "(68,)\n",
            "(330,)\n",
            "(1671,)\n",
            "(320,)\n",
            "(254,)\n",
            "(74,)\n",
            "(71,)\n",
            "(1409,)\n",
            "(857,)\n",
            "(1226,)\n",
            "(3030,)\n",
            "(2458,)\n",
            "(7415,)\n",
            "(3113,)\n",
            "(2059,)\n",
            "(1130,)\n",
            "(956,)\n",
            "(634,)\n",
            "(790,)\n",
            "(1250,)\n",
            "(73,)\n",
            "(279,)\n",
            "(132,)\n",
            "(474,)\n",
            "(126,)\n",
            "(5984,)\n",
            "(2272,)\n",
            "(2245,)\n",
            "(1693,)\n",
            "(163,)\n",
            "(324,)\n",
            "(775,)\n",
            "(557,)\n",
            "(1824,)\n",
            "(1049,)\n",
            "(191,)\n",
            "(913,)\n",
            "(170,)\n",
            "(1704,)\n",
            "(1656,)\n",
            "(55,)\n",
            "(1103,)\n",
            "(200,)\n",
            "(1225,)\n",
            "(95,)\n",
            "(135,)\n",
            "(51,)\n",
            "(520,)\n",
            "(1600,)\n",
            "(256,)\n",
            "(1824,)\n",
            "(1536,)\n",
            "(1412,)\n",
            "(3369,)\n",
            "(81,)\n",
            "(1181,)\n",
            "(489,)\n",
            "(796,)\n",
            "(133,)\n",
            "(173,)\n",
            "(1093,)\n",
            "(59,)\n",
            "(308,)\n",
            "(792,)\n",
            "(4891,)\n",
            "(534,)\n",
            "(1671,)\n",
            "(110,)\n",
            "(3032,)\n",
            "(1328,)\n",
            "(66,)\n",
            "(980,)\n",
            "(395,)\n",
            "(953,)\n",
            "(110,)\n",
            "(11076,)\n",
            "(84,)\n",
            "(1470,)\n",
            "(369,)\n",
            "(1612,)\n",
            "(626,)\n",
            "(1641,)\n",
            "(4836,)\n",
            "(3520,)\n",
            "(424,)\n",
            "(446,)\n",
            "(87,)\n",
            "(429,)\n",
            "(1357,)\n",
            "(52,)\n",
            "(1556,)\n",
            "(443,)\n",
            "(770,)\n",
            "(108,)\n",
            "(200,)\n",
            "(2269,)\n",
            "(1625,)\n",
            "(1950,)\n",
            "(4084,)\n",
            "(5499,)\n",
            "(405,)\n",
            "(61,)\n",
            "(133,)\n",
            "(328,)\n",
            "(316,)\n",
            "(397,)\n",
            "(232,)\n",
            "(673,)\n",
            "(416,)\n",
            "(5152,)\n",
            "(1349,)\n",
            "(2991,)\n",
            "(146,)\n",
            "(60,)\n",
            "(249,)\n",
            "(1990,)\n",
            "(278,)\n",
            "(141,)\n",
            "(889,)\n",
            "(72,)\n",
            "(1204,)\n",
            "(63,)\n",
            "(522,)\n",
            "(87,)\n",
            "(1538,)\n",
            "(178,)\n",
            "(1732,)\n",
            "(398,)\n",
            "(343,)\n",
            "(149,)\n",
            "(52,)\n",
            "(54,)\n",
            "(169,)\n",
            "(114,)\n",
            "(444,)\n",
            "(2403,)\n",
            "(705,)\n",
            "(1523,)\n",
            "(453,)\n",
            "(674,)\n",
            "(961,)\n",
            "(1557,)\n",
            "(62,)\n",
            "(464,)\n",
            "(2328,)\n",
            "(282,)\n",
            "(293,)\n",
            "(4961,)\n",
            "(870,)\n",
            "(4197,)\n",
            "(2691,)\n",
            "(6736,)\n",
            "(263,)\n",
            "(493,)\n",
            "(249,)\n",
            "(298,)\n",
            "(79,)\n",
            "(675,)\n",
            "(116,)\n",
            "(169,)\n",
            "(504,)\n",
            "(729,)\n",
            "(68,)\n",
            "(333,)\n",
            "(163,)\n",
            "(124,)\n",
            "(300,)\n",
            "(232,)\n",
            "(182,)\n",
            "(311,)\n",
            "(648,)\n",
            "(2088,)\n",
            "(2799,)\n",
            "(3891,)\n",
            "(6929,)\n",
            "(570,)\n",
            "(462,)\n",
            "(318,)\n",
            "(820,)\n",
            "(158,)\n",
            "(957,)\n",
            "(121,)\n",
            "(206,)\n",
            "(52,)\n",
            "(193,)\n",
            "(543,)\n",
            "(161,)\n",
            "(258,)\n",
            "(347,)\n",
            "(842,)\n",
            "(1592,)\n",
            "(1625,)\n",
            "(267,)\n",
            "(661,)\n",
            "(93,)\n",
            "(317,)\n",
            "(277,)\n",
            "(157,)\n",
            "(857,)\n",
            "(2497,)\n",
            "(232,)\n",
            "(8873,)\n",
            "(2854,)\n",
            "(4307,)\n",
            "(2833,)\n",
            "(3120,)\n",
            "(1318,)\n",
            "(1502,)\n",
            "(101,)\n",
            "(58,)\n",
            "(260,)\n",
            "(496,)\n",
            "(1697,)\n",
            "(964,)\n",
            "(4267,)\n",
            "(382,)\n",
            "(140,)\n",
            "(307,)\n",
            "(136,)\n",
            "(1555,)\n",
            "(224,)\n",
            "(64,)\n",
            "(77,)\n",
            "(66,)\n",
            "(179,)\n",
            "(59,)\n",
            "(1068,)\n",
            "(668,)\n",
            "(308,)\n",
            "(803,)\n",
            "(753,)\n",
            "(2791,)\n",
            "(1405,)\n",
            "(1938,)\n",
            "(2062,)\n",
            "(2086,)\n",
            "(1010,)\n",
            "(3397,)\n",
            "(529,)\n",
            "(1165,)\n",
            "(220,)\n",
            "(1075,)\n",
            "(221,)\n",
            "(652,)\n",
            "(888,)\n",
            "(7307,)\n",
            "(728,)\n",
            "(406,)\n",
            "(418,)\n",
            "(548,)\n",
            "(175,)\n",
            "(51,)\n",
            "(216,)\n",
            "(459,)\n",
            "(250,)\n",
            "(79,)\n",
            "(1951,)\n",
            "(97,)\n",
            "(150,)\n",
            "(151,)\n",
            "(3582,)\n",
            "(8030,)\n",
            "(11373,)\n",
            "(351,)\n",
            "(1012,)\n",
            "(604,)\n",
            "(1859,)\n",
            "(1246,)\n",
            "(74,)\n",
            "(532,)\n",
            "(115,)\n",
            "(533,)\n",
            "(1360,)\n",
            "(276,)\n",
            "(334,)\n",
            "(163,)\n",
            "(102,)\n",
            "(7180,)\n",
            "(2481,)\n",
            "(3235,)\n",
            "(1292,)\n",
            "(71,)\n",
            "(832,)\n",
            "(101,)\n",
            "(218,)\n",
            "(109,)\n",
            "(719,)\n",
            "(399,)\n",
            "(886,)\n",
            "(3609,)\n",
            "(1696,)\n",
            "(4006,)\n",
            "(406,)\n",
            "(317,)\n",
            "(211,)\n",
            "(639,)\n",
            "(135,)\n",
            "(54,)\n",
            "(823,)\n",
            "(93,)\n",
            "(516,)\n",
            "(94,)\n",
            "(1862,)\n",
            "(52,)\n",
            "(2006,)\n",
            "(1533,)\n",
            "(2660,)\n",
            "(138,)\n",
            "(582,)\n",
            "(961,)\n",
            "(93,)\n",
            "(690,)\n",
            "(96,)\n",
            "(9334,)\n",
            "(4317,)\n",
            "(882,)\n",
            "(308,)\n",
            "(97,)\n",
            "(116,)\n",
            "(73,)\n",
            "(78,)\n",
            "(106,)\n",
            "(577,)\n",
            "(514,)\n",
            "(152,)\n",
            "(140,)\n",
            "(304,)\n",
            "(300,)\n",
            "(195,)\n",
            "(391,)\n",
            "(1973,)\n",
            "(4935,)\n",
            "(1370,)\n",
            "(805,)\n",
            "(120,)\n",
            "(93,)\n",
            "(433,)\n",
            "(827,)\n",
            "(265,)\n",
            "(132,)\n",
            "(178,)\n",
            "(216,)\n",
            "(1434,)\n",
            "(863,)\n",
            "(428,)\n",
            "(152,)\n",
            "(658,)\n",
            "(89,)\n",
            "(53,)\n",
            "(222,)\n",
            "(1677,)\n",
            "(4803,)\n",
            "(382,)\n",
            "(380,)\n",
            "(455,)\n",
            "(536,)\n",
            "(131,)\n",
            "(1766,)\n",
            "(358,)\n",
            "(73,)\n",
            "(57,)\n",
            "(474,)\n",
            "(278,)\n",
            "(2379,)\n",
            "(360,)\n",
            "(62,)\n",
            "(678,)\n",
            "(242,)\n",
            "(266,)\n",
            "(129,)\n",
            "(92,)\n",
            "(70,)\n",
            "(228,)\n",
            "(170,)\n",
            "(428,)\n",
            "(310,)\n",
            "(414,)\n",
            "(158,)\n",
            "(68,)\n",
            "(331,)\n",
            "(749,)\n",
            "(261,)\n",
            "(578,)\n",
            "(1350,)\n",
            "(97,)\n",
            "(752,)\n",
            "(92,)\n",
            "(175,)\n",
            "(1716,)\n",
            "(148,)\n",
            "(78,)\n",
            "(776,)\n",
            "(1970,)\n",
            "(88,)\n",
            "(2768,)\n",
            "(148,)\n",
            "(2986,)\n",
            "(407,)\n",
            "(941,)\n",
            "(366,)\n",
            "(556,)\n",
            "(281,)\n",
            "(377,)\n",
            "(160,)\n",
            "(1810,)\n",
            "(120,)\n",
            "(555,)\n",
            "(668,)\n",
            "(147,)\n",
            "(178,)\n",
            "(493,)\n",
            "(238,)\n",
            "(812,)\n",
            "(235,)\n",
            "(125,)\n",
            "(112,)\n",
            "(507,)\n",
            "(558,)\n",
            "(1864,)\n",
            "(1893,)\n",
            "(335,)\n",
            "(285,)\n",
            "(412,)\n",
            "(584,)\n",
            "(163,)\n",
            "(430,)\n",
            "(845,)\n",
            "(2195,)\n",
            "(2205,)\n",
            "(4053,)\n",
            "(1022,)\n",
            "(1228,)\n",
            "(193,)\n",
            "(55,)\n",
            "(1399,)\n",
            "(913,)\n",
            "(179,)\n",
            "(570,)\n",
            "(991,)\n",
            "(604,)\n",
            "(1985,)\n",
            "(709,)\n",
            "(738,)\n",
            "(1512,)\n",
            "(58,)\n",
            "(372,)\n",
            "(368,)\n",
            "(432,)\n",
            "(64,)\n",
            "(107,)\n",
            "(185,)\n",
            "(219,)\n",
            "(176,)\n",
            "(224,)\n",
            "(236,)\n",
            "(230,)\n",
            "(446,)\n",
            "(143,)\n",
            "(51,)\n",
            "(74,)\n",
            "(222,)\n",
            "(216,)\n",
            "(239,)\n",
            "(205,)\n",
            "(1454,)\n",
            "(55,)\n",
            "(670,)\n",
            "(1774,)\n",
            "(2517,)\n",
            "(100,)\n",
            "(774,)\n",
            "(102,)\n",
            "(228,)\n",
            "(66,)\n",
            "(1261,)\n",
            "(60,)\n",
            "(508,)\n",
            "(978,)\n",
            "(52,)\n",
            "(417,)\n",
            "(228,)\n",
            "(75,)\n",
            "(95,)\n",
            "(304,)\n",
            "(102,)\n",
            "(2898,)\n",
            "(1248,)\n",
            "(1189,)\n",
            "(240,)\n",
            "(2454,)\n",
            "(63,)\n",
            "(277,)\n",
            "(362,)\n",
            "(59,)\n",
            "(468,)\n",
            "(352,)\n",
            "(76,)\n",
            "(224,)\n",
            "(147,)\n",
            "(178,)\n",
            "(109,)\n",
            "(404,)\n",
            "(74,)\n",
            "(116,)\n",
            "(327,)\n",
            "(1530,)\n",
            "(431,)\n",
            "(146,)\n",
            "(154,)\n",
            "(217,)\n",
            "(2394,)\n",
            "(275,)\n",
            "(656,)\n",
            "(77,)\n",
            "(778,)\n",
            "(2269,)\n",
            "(472,)\n",
            "(606,)\n",
            "(216,)\n",
            "(119,)\n",
            "(367,)\n",
            "(413,)\n",
            "(96,)\n",
            "(97,)\n",
            "(556,)\n",
            "(122,)\n",
            "(109,)\n",
            "(70,)\n",
            "(3679,)\n",
            "(2872,)\n",
            "(2643,)\n",
            "(496,)\n",
            "(134,)\n",
            "(318,)\n",
            "(166,)\n",
            "(164,)\n",
            "(529,)\n",
            "(526,)\n",
            "(177,)\n",
            "(55,)\n",
            "(570,)\n",
            "(258,)\n",
            "(1234,)\n",
            "(706,)\n",
            "(229,)\n",
            "(1073,)\n",
            "(173,)\n",
            "(1257,)\n",
            "(504,)\n",
            "(1821,)\n",
            "(224,)\n",
            "(3859,)\n",
            "(148,)\n",
            "(158,)\n",
            "(237,)\n",
            "(285,)\n",
            "(207,)\n",
            "(538,)\n",
            "(112,)\n",
            "(181,)\n",
            "(87,)\n",
            "(388,)\n",
            "(775,)\n",
            "(97,)\n",
            "(3314,)\n",
            "(2844,)\n",
            "(90,)\n",
            "(105,)\n",
            "(878,)\n",
            "(72,)\n",
            "(84,)\n",
            "(1002,)\n",
            "(266,)\n",
            "(897,)\n",
            "(125,)\n",
            "(962,)\n",
            "(198,)\n",
            "(104,)\n",
            "(600,)\n",
            "(287,)\n",
            "(296,)\n",
            "(813,)\n",
            "(4516,)\n",
            "(204,)\n",
            "(94,)\n",
            "(168,)\n",
            "(210,)\n",
            "(419,)\n",
            "(426,)\n",
            "(354,)\n",
            "(100,)\n",
            "(114,)\n",
            "(272,)\n",
            "(359,)\n",
            "(85,)\n",
            "(3942,)\n",
            "(103,)\n",
            "(354,)\n",
            "(2335,)\n",
            "(463,)\n",
            "(842,)\n",
            "(290,)\n",
            "(248,)\n",
            "(64,)\n",
            "(115,)\n",
            "(97,)\n",
            "(69,)\n",
            "(65,)\n",
            "(836,)\n",
            "(62,)\n",
            "(111,)\n",
            "(227,)\n",
            "(52,)\n",
            "(60,)\n",
            "(1258,)\n",
            "(85,)\n",
            "(69,)\n",
            "(123,)\n",
            "(150,)\n",
            "(819,)\n",
            "(2168,)\n",
            "(174,)\n",
            "(459,)\n",
            "(413,)\n",
            "(283,)\n",
            "(59,)\n",
            "(136,)\n",
            "(67,)\n",
            "(120,)\n",
            "(244,)\n",
            "(88,)\n",
            "(814,)\n",
            "(56,)\n",
            "(146,)\n",
            "(613,)\n",
            "(270,)\n",
            "(85,)\n",
            "(279,)\n",
            "(98,)\n",
            "(328,)\n",
            "(199,)\n",
            "(145,)\n",
            "(182,)\n",
            "(397,)\n",
            "(54,)\n",
            "(1940,)\n",
            "(2206,)\n",
            "(1538,)\n",
            "(1710,)\n",
            "(987,)\n",
            "(421,)\n",
            "(153,)\n",
            "(80,)\n",
            "(742,)\n",
            "(582,)\n",
            "(568,)\n",
            "(60,)\n",
            "(63,)\n",
            "(960,)\n",
            "(146,)\n",
            "(65,)\n",
            "(671,)\n",
            "(372,)\n",
            "(299,)\n",
            "(80,)\n",
            "(306,)\n",
            "(486,)\n",
            "(537,)\n",
            "(87,)\n",
            "(122,)\n",
            "(67,)\n",
            "(213,)\n",
            "(513,)\n",
            "(397,)\n",
            "(159,)\n",
            "(140,)\n",
            "(108,)\n",
            "(79,)\n",
            "(242,)\n",
            "(530,)\n",
            "(88,)\n",
            "(273,)\n",
            "(188,)\n",
            "(498,)\n",
            "(789,)\n",
            "(317,)\n",
            "(608,)\n",
            "(3466,)\n",
            "(2670,)\n",
            "(307,)\n",
            "(934,)\n",
            "(1124,)\n",
            "(693,)\n",
            "(1117,)\n",
            "(124,)\n",
            "(772,)\n",
            "(127,)\n",
            "(112,)\n",
            "(79,)\n",
            "(78,)\n",
            "(53,)\n",
            "(275,)\n",
            "(84,)\n",
            "(450,)\n",
            "(871,)\n",
            "(1464,)\n",
            "(1312,)\n",
            "(345,)\n",
            "(1315,)\n",
            "(278,)\n",
            "(413,)\n",
            "(255,)\n",
            "(762,)\n",
            "(323,)\n",
            "(949,)\n",
            "(107,)\n",
            "(334,)\n",
            "(257,)\n",
            "(312,)\n",
            "(128,)\n",
            "(174,)\n",
            "(1110,)\n",
            "(157,)\n",
            "(1566,)\n",
            "(238,)\n",
            "(63,)\n",
            "(60,)\n",
            "(826,)\n",
            "(966,)\n",
            "(1588,)\n",
            "(3529,)\n",
            "(4954,)\n",
            "(1585,)\n",
            "(278,)\n",
            "(73,)\n",
            "(538,)\n",
            "(72,)\n",
            "(51,)\n",
            "(217,)\n",
            "(122,)\n",
            "(76,)\n",
            "(71,)\n",
            "(334,)\n",
            "(400,)\n",
            "(211,)\n",
            "(100,)\n",
            "(80,)\n",
            "(146,)\n",
            "(364,)\n",
            "(228,)\n",
            "(72,)\n",
            "(314,)\n",
            "(431,)\n",
            "(677,)\n",
            "(299,)\n",
            "(954,)\n",
            "(1848,)\n",
            "(1960,)\n",
            "(446,)\n",
            "(162,)\n",
            "(86,)\n",
            "(181,)\n",
            "(625,)\n",
            "(430,)\n",
            "(67,)\n",
            "(288,)\n",
            "(506,)\n",
            "(275,)\n",
            "(95,)\n",
            "(81,)\n",
            "(303,)\n",
            "(51,)\n",
            "(134,)\n",
            "(1518,)\n",
            "(572,)\n",
            "(787,)\n",
            "(126,)\n",
            "(216,)\n",
            "(295,)\n",
            "(266,)\n",
            "(87,)\n",
            "(277,)\n",
            "(77,)\n",
            "(123,)\n",
            "(54,)\n",
            "(132,)\n",
            "(166,)\n",
            "(55,)\n",
            "(571,)\n",
            "(475,)\n",
            "(525,)\n",
            "(402,)\n",
            "(447,)\n",
            "(741,)\n",
            "(333,)\n",
            "(1365,)\n",
            "(1376,)\n",
            "(90,)\n",
            "(511,)\n",
            "(350,)\n",
            "(673,)\n",
            "(508,)\n",
            "(746,)\n",
            "(316,)\n",
            "(88,)\n",
            "(286,)\n",
            "(229,)\n",
            "(711,)\n",
            "(126,)\n",
            "(2123,)\n",
            "(1057,)\n",
            "(2213,)\n",
            "(94,)\n",
            "(1181,)\n",
            "(399,)\n",
            "(884,)\n",
            "(101,)\n",
            "(145,)\n",
            "(125,)\n",
            "(98,)\n",
            "(309,)\n",
            "(274,)\n",
            "(75,)\n",
            "(54,)\n",
            "(257,)\n",
            "(71,)\n",
            "(150,)\n",
            "(181,)\n",
            "(745,)\n",
            "(485,)\n",
            "(104,)\n",
            "(100,)\n",
            "(214,)\n",
            "(652,)\n",
            "(5171,)\n",
            "(347,)\n",
            "(60,)\n",
            "(341,)\n",
            "(321,)\n",
            "(88,)\n",
            "(148,)\n",
            "(104,)\n",
            "(1130,)\n",
            "(464,)\n",
            "(134,)\n",
            "(117,)\n",
            "(374,)\n",
            "(345,)\n",
            "(361,)\n",
            "(1652,)\n",
            "(1294,)\n",
            "(1177,)\n",
            "(288,)\n",
            "(592,)\n",
            "(80,)\n",
            "(217,)\n",
            "(522,)\n",
            "(99,)\n",
            "(399,)\n",
            "(100,)\n",
            "(61,)\n",
            "(207,)\n",
            "(83,)\n",
            "(115,)\n",
            "(277,)\n",
            "(667,)\n",
            "(537,)\n",
            "(146,)\n",
            "(86,)\n",
            "(238,)\n",
            "(103,)\n",
            "(68,)\n",
            "(265,)\n",
            "(672,)\n",
            "(1515,)\n",
            "(2183,)\n",
            "(136,)\n",
            "(153,)\n",
            "(400,)\n",
            "(89,)\n",
            "(212,)\n",
            "(674,)\n",
            "(123,)\n",
            "(153,)\n",
            "(354,)\n",
            "(284,)\n",
            "(669,)\n",
            "(154,)\n",
            "(74,)\n",
            "(523,)\n",
            "(88,)\n",
            "(108,)\n",
            "(936,)\n",
            "(1681,)\n",
            "(1597,)\n",
            "(1120,)\n",
            "(680,)\n",
            "(483,)\n",
            "(163,)\n",
            "(238,)\n",
            "(59,)\n",
            "(148,)\n",
            "(294,)\n",
            "(502,)\n",
            "(389,)\n",
            "(197,)\n",
            "(317,)\n",
            "(309,)\n",
            "(108,)\n",
            "(183,)\n",
            "(251,)\n",
            "(238,)\n",
            "(129,)\n",
            "(111,)\n",
            "(376,)\n",
            "(660,)\n",
            "(671,)\n",
            "(261,)\n",
            "(935,)\n",
            "(403,)\n",
            "(264,)\n",
            "(132,)\n",
            "(60,)\n",
            "(195,)\n",
            "(1429,)\n",
            "(1573,)\n",
            "(623,)\n",
            "(125,)\n",
            "(228,)\n",
            "(1157,)\n",
            "(500,)\n",
            "(112,)\n",
            "(72,)\n",
            "(436,)\n",
            "(441,)\n",
            "(185,)\n",
            "(1609,)\n",
            "(5084,)\n",
            "(399,)\n",
            "(53,)\n",
            "(61,)\n",
            "(318,)\n",
            "(58,)\n",
            "(528,)\n",
            "(66,)\n",
            "(346,)\n",
            "(118,)\n",
            "(414,)\n",
            "(141,)\n",
            "(79,)\n",
            "(58,)\n",
            "(775,)\n",
            "(384,)\n",
            "(76,)\n",
            "(426,)\n",
            "(121,)\n",
            "(81,)\n",
            "(83,)\n",
            "(414,)\n",
            "(93,)\n",
            "(203,)\n",
            "(297,)\n",
            "(362,)\n",
            "(416,)\n",
            "(334,)\n",
            "(126,)\n",
            "(355,)\n",
            "(622,)\n",
            "(1793,)\n",
            "(1260,)\n",
            "(1696,)\n",
            "(130,)\n",
            "(65,)\n",
            "(86,)\n",
            "(80,)\n",
            "(52,)\n",
            "(351,)\n",
            "(193,)\n",
            "(183,)\n",
            "(217,)\n",
            "(70,)\n",
            "(67,)\n",
            "(595,)\n",
            "(62,)\n",
            "(531,)\n",
            "(74,)\n",
            "(203,)\n",
            "(494,)\n",
            "(82,)\n",
            "(61,)\n",
            "(66,)\n",
            "(79,)\n",
            "(265,)\n",
            "(68,)\n",
            "(1399,)\n",
            "(6972,)\n",
            "(2480,)\n",
            "(68,)\n",
            "(271,)\n",
            "(214,)\n",
            "(597,)\n",
            "(461,)\n",
            "(219,)\n",
            "(56,)\n",
            "(116,)\n",
            "(77,)\n",
            "(990,)\n",
            "(118,)\n",
            "(98,)\n",
            "(84,)\n",
            "(710,)\n",
            "(116,)\n",
            "(459,)\n",
            "(51,)\n",
            "(288,)\n",
            "(130,)\n",
            "(78,)\n",
            "(107,)\n",
            "(109,)\n",
            "(199,)\n",
            "(52,)\n",
            "(279,)\n",
            "(58,)\n",
            "(64,)\n",
            "(302,)\n",
            "(184,)\n",
            "(69,)\n",
            "(810,)\n",
            "(878,)\n",
            "(611,)\n",
            "(72,)\n",
            "(228,)\n",
            "(629,)\n",
            "(572,)\n",
            "(80,)\n",
            "(845,)\n",
            "(288,)\n",
            "(164,)\n",
            "(191,)\n",
            "(88,)\n",
            "(75,)\n",
            "(110,)\n",
            "(78,)\n",
            "(84,)\n",
            "(53,)\n",
            "(63,)\n",
            "(106,)\n",
            "(162,)\n",
            "(794,)\n",
            "(1552,)\n",
            "(3714,)\n",
            "(475,)\n",
            "(253,)\n",
            "(8366,)\n",
            "(128,)\n",
            "(321,)\n",
            "(449,)\n",
            "(537,)\n",
            "(76,)\n",
            "(53,)\n",
            "(153,)\n",
            "(100,)\n",
            "(74,)\n",
            "(83,)\n",
            "(118,)\n",
            "(108,)\n",
            "(159,)\n",
            "(149,)\n",
            "(111,)\n",
            "(116,)\n",
            "(104,)\n",
            "(58,)\n",
            "(1798,)\n",
            "(2550,)\n",
            "(719,)\n",
            "(2016,)\n",
            "(814,)\n",
            "(61,)\n",
            "(69,)\n",
            "(65,)\n",
            "(406,)\n",
            "(123,)\n",
            "(189,)\n",
            "(136,)\n",
            "(52,)\n",
            "(80,)\n",
            "(51,)\n",
            "(508,)\n",
            "(92,)\n",
            "(62,)\n",
            "(1128,)\n",
            "(69,)\n",
            "(177,)\n",
            "(63,)\n",
            "(1593,)\n",
            "(239,)\n",
            "(163,)\n",
            "(548,)\n",
            "(91,)\n",
            "(60,)\n",
            "(70,)\n",
            "(66,)\n",
            "(96,)\n",
            "(435,)\n",
            "(113,)\n",
            "(411,)\n",
            "(462,)\n",
            "(70,)\n",
            "(262,)\n",
            "(166,)\n",
            "(55,)\n",
            "(86,)\n",
            "(92,)\n",
            "(110,)\n",
            "(164,)\n",
            "(1095,)\n",
            "(1386,)\n",
            "(1299,)\n",
            "(152,)\n",
            "(696,)\n",
            "(244,)\n",
            "(68,)\n",
            "(84,)\n",
            "(59,)\n",
            "(540,)\n",
            "(148,)\n",
            "(131,)\n",
            "(78,)\n",
            "(255,)\n",
            "(246,)\n",
            "(79,)\n",
            "(372,)\n",
            "(64,)\n",
            "(76,)\n",
            "(149,)\n",
            "(116,)\n",
            "(207,)\n",
            "(278,)\n",
            "(56,)\n",
            "(530,)\n",
            "(56,)\n",
            "(77,)\n",
            "(513,)\n",
            "(529,)\n",
            "(3295,)\n",
            "(355,)\n",
            "(87,)\n",
            "(134,)\n",
            "(224,)\n",
            "(197,)\n",
            "(62,)\n",
            "(628,)\n",
            "(166,)\n",
            "(212,)\n",
            "(63,)\n",
            "(272,)\n",
            "(100,)\n",
            "(63,)\n",
            "(60,)\n",
            "(93,)\n",
            "(218,)\n",
            "(276,)\n",
            "(79,)\n",
            "(152,)\n",
            "(229,)\n",
            "(1260,)\n",
            "(1900,)\n",
            "(385,)\n",
            "(303,)\n",
            "(108,)\n",
            "(244,)\n",
            "(86,)\n",
            "(96,)\n",
            "(180,)\n",
            "(266,)\n",
            "(74,)\n",
            "(328,)\n",
            "(86,)\n",
            "(308,)\n",
            "(64,)\n",
            "(285,)\n",
            "(945,)\n",
            "(162,)\n",
            "(392,)\n",
            "(1989,)\n",
            "(673,)\n",
            "(927,)\n",
            "(474,)\n",
            "(167,)\n",
            "(95,)\n",
            "(165,)\n",
            "(162,)\n",
            "(115,)\n",
            "(55,)\n",
            "(134,)\n",
            "(91,)\n",
            "(308,)\n",
            "(173,)\n",
            "(74,)\n",
            "(179,)\n",
            "(169,)\n",
            "(220,)\n",
            "(230,)\n",
            "(430,)\n",
            "(116,)\n",
            "(193,)\n",
            "(703,)\n",
            "(107,)\n",
            "(128,)\n",
            "(894,)\n",
            "(628,)\n",
            "(2525,)\n",
            "(131,)\n",
            "(97,)\n",
            "(161,)\n",
            "(52,)\n",
            "(155,)\n",
            "(114,)\n",
            "(120,)\n",
            "(114,)\n",
            "(65,)\n",
            "(77,)\n",
            "(309,)\n",
            "(265,)\n",
            "(53,)\n",
            "(53,)\n",
            "(387,)\n",
            "(152,)\n",
            "(63,)\n",
            "(124,)\n",
            "(188,)\n",
            "(280,)\n",
            "(52,)\n",
            "(87,)\n",
            "(316,)\n",
            "(301,)\n",
            "(136,)\n",
            "(55,)\n",
            "(235,)\n",
            "(534,)\n",
            "(416,)\n",
            "(450,)\n",
            "(1372,)\n",
            "(1811,)\n",
            "(69,)\n",
            "(78,)\n",
            "(51,)\n",
            "(102,)\n",
            "(211,)\n",
            "(105,)\n",
            "(57,)\n",
            "(74,)\n",
            "(56,)\n",
            "(59,)\n",
            "(61,)\n",
            "(214,)\n",
            "(392,)\n",
            "(287,)\n",
            "(129,)\n",
            "(71,)\n",
            "(138,)\n",
            "(161,)\n",
            "(61,)\n",
            "(1323,)\n",
            "(182,)\n",
            "(215,)\n",
            "(3602,)\n",
            "(990,)\n",
            "(251,)\n",
            "(54,)\n",
            "(155,)\n",
            "(86,)\n",
            "(80,)\n",
            "(114,)\n",
            "(239,)\n",
            "(289,)\n",
            "(82,)\n",
            "(261,)\n",
            "(51,)\n",
            "(160,)\n",
            "(212,)\n",
            "(225,)\n",
            "(55,)\n",
            "(77,)\n",
            "(62,)\n",
            "(103,)\n",
            "(63,)\n",
            "(144,)\n",
            "(86,)\n",
            "(100,)\n",
            "(139,)\n",
            "(115,)\n",
            "(665,)\n",
            "(432,)\n",
            "(192,)\n",
            "(66,)\n",
            "(266,)\n",
            "(621,)\n",
            "(293,)\n",
            "(102,)\n",
            "(321,)\n",
            "(72,)\n",
            "(139,)\n",
            "(143,)\n",
            "(554,)\n",
            "(58,)\n",
            "(99,)\n",
            "(250,)\n",
            "(391,)\n",
            "(388,)\n",
            "(63,)\n",
            "(251,)\n",
            "(313,)\n",
            "(1896,)\n",
            "(5355,)\n",
            "(88,)\n",
            "(67,)\n",
            "(62,)\n",
            "(188,)\n",
            "(451,)\n",
            "(320,)\n",
            "(83,)\n",
            "(69,)\n",
            "(323,)\n",
            "(91,)\n",
            "(250,)\n",
            "(520,)\n",
            "(246,)\n",
            "(104,)\n",
            "(112,)\n",
            "(216,)\n",
            "(61,)\n",
            "(78,)\n",
            "(272,)\n",
            "(82,)\n",
            "(291,)\n",
            "(324,)\n",
            "(97,)\n",
            "(54,)\n",
            "(59,)\n",
            "(57,)\n",
            "(98,)\n",
            "(113,)\n",
            "(221,)\n",
            "(297,)\n",
            "(637,)\n",
            "(458,)\n",
            "(491,)\n",
            "(204,)\n",
            "(112,)\n",
            "(264,)\n",
            "(84,)\n",
            "(75,)\n",
            "(101,)\n",
            "(70,)\n",
            "(63,)\n",
            "(109,)\n",
            "(116,)\n",
            "(579,)\n",
            "(69,)\n",
            "(89,)\n",
            "(557,)\n",
            "(56,)\n",
            "(76,)\n",
            "(692,)\n",
            "(77,)\n",
            "(58,)\n",
            "(176,)\n",
            "(57,)\n",
            "(1496,)\n",
            "(52,)\n",
            "(194,)\n",
            "(609,)\n",
            "(170,)\n",
            "(118,)\n",
            "(507,)\n",
            "(265,)\n",
            "(262,)\n",
            "(74,)\n",
            "(831,)\n",
            "(121,)\n",
            "(157,)\n",
            "(119,)\n",
            "(412,)\n",
            "(61,)\n",
            "(179,)\n",
            "(69,)\n",
            "(71,)\n",
            "(233,)\n",
            "(187,)\n",
            "(60,)\n",
            "(93,)\n",
            "(767,)\n",
            "(456,)\n",
            "(76,)\n",
            "(228,)\n",
            "(1410,)\n",
            "(131,)\n",
            "(694,)\n",
            "(666,)\n",
            "(675,)\n",
            "(4002,)\n",
            "(272,)\n",
            "(2735,)\n",
            "(116,)\n",
            "(154,)\n",
            "(151,)\n",
            "(73,)\n",
            "(79,)\n",
            "(309,)\n",
            "(81,)\n",
            "(179,)\n",
            "(51,)\n",
            "(80,)\n",
            "(105,)\n",
            "(86,)\n",
            "(358,)\n",
            "(51,)\n",
            "(64,)\n",
            "(229,)\n",
            "(74,)\n",
            "(408,)\n",
            "(1815,)\n",
            "(95,)\n",
            "(65,)\n",
            "(178,)\n",
            "(68,)\n",
            "(99,)\n",
            "(118,)\n",
            "(61,)\n",
            "(189,)\n",
            "(100,)\n",
            "(75,)\n",
            "(246,)\n",
            "(231,)\n",
            "(3013,)\n",
            "(78,)\n",
            "(306,)\n",
            "(108,)\n",
            "(1276,)\n",
            "(267,)\n",
            "(85,)\n",
            "(508,)\n",
            "(255,)\n",
            "(86,)\n",
            "(102,)\n",
            "(447,)\n",
            "(350,)\n",
            "(610,)\n",
            "(83,)\n",
            "(104,)\n",
            "(560,)\n",
            "(112,)\n",
            "(65,)\n",
            "(325,)\n",
            "(85,)\n",
            "(79,)\n",
            "(129,)\n",
            "(258,)\n",
            "(102,)\n",
            "(214,)\n",
            "(185,)\n",
            "(151,)\n",
            "(212,)\n",
            "(264,)\n",
            "(53,)\n",
            "(150,)\n",
            "(154,)\n",
            "(152,)\n",
            "(357,)\n",
            "(168,)\n",
            "(57,)\n",
            "(120,)\n",
            "(275,)\n",
            "(230,)\n",
            "(86,)\n",
            "(730,)\n",
            "(112,)\n",
            "(60,)\n",
            "(227,)\n",
            "(60,)\n",
            "(66,)\n",
            "(77,)\n",
            "(59,)\n",
            "(95,)\n",
            "(415,)\n",
            "(250,)\n",
            "(112,)\n",
            "(54,)\n",
            "(124,)\n",
            "(215,)\n",
            "(125,)\n",
            "(64,)\n",
            "(990,)\n",
            "(105,)\n",
            "(132,)\n",
            "(64,)\n",
            "(102,)\n",
            "(185,)\n",
            "(75,)\n",
            "(98,)\n",
            "(160,)\n",
            "(154,)\n",
            "(108,)\n",
            "(92,)\n",
            "(51,)\n",
            "(202,)\n",
            "(88,)\n",
            "(81,)\n",
            "(61,)\n",
            "(53,)\n",
            "(131,)\n",
            "(131,)\n",
            "(216,)\n",
            "(107,)\n",
            "(114,)\n",
            "(620,)\n",
            "(1696,)\n",
            "(535,)\n",
            "(922,)\n",
            "(462,)\n",
            "(401,)\n",
            "(176,)\n",
            "(65,)\n",
            "(54,)\n",
            "(283,)\n",
            "(324,)\n",
            "(133,)\n",
            "(252,)\n",
            "(64,)\n",
            "(768,)\n",
            "(59,)\n",
            "(282,)\n",
            "(70,)\n",
            "(60,)\n",
            "(67,)\n",
            "(84,)\n",
            "(88,)\n",
            "(62,)\n",
            "(89,)\n",
            "(53,)\n",
            "(199,)\n",
            "(144,)\n",
            "(56,)\n",
            "(84,)\n",
            "(65,)\n",
            "(101,)\n",
            "(66,)\n",
            "(62,)\n",
            "(62,)\n",
            "(270,)\n",
            "(132,)\n",
            "(162,)\n",
            "(81,)\n",
            "(72,)\n",
            "(183,)\n",
            "(71,)\n",
            "(353,)\n",
            "(60,)\n",
            "(55,)\n",
            "(125,)\n",
            "(893,)\n",
            "(197,)\n",
            "(387,)\n",
            "(86,)\n",
            "(1043,)\n",
            "(57,)\n",
            "(61,)\n",
            "(53,)\n",
            "(64,)\n",
            "(236,)\n",
            "(102,)\n",
            "(346,)\n",
            "(221,)\n",
            "(284,)\n",
            "(77,)\n",
            "(82,)\n",
            "(91,)\n",
            "(235,)\n",
            "(108,)\n",
            "(759,)\n",
            "(195,)\n",
            "(71,)\n",
            "(277,)\n",
            "(288,)\n",
            "(69,)\n",
            "(180,)\n",
            "(192,)\n",
            "(51,)\n",
            "(503,)\n",
            "(126,)\n",
            "(536,)\n",
            "(60,)\n",
            "(83,)\n",
            "(129,)\n",
            "(56,)\n",
            "(118,)\n",
            "(65,)\n",
            "(67,)\n",
            "(112,)\n",
            "(177,)\n",
            "(76,)\n",
            "(85,)\n",
            "(64,)\n",
            "(56,)\n",
            "(139,)\n",
            "(316,)\n",
            "(1162,)\n",
            "(147,)\n",
            "(626,)\n",
            "(129,)\n",
            "(79,)\n",
            "(90,)\n",
            "(123,)\n",
            "(54,)\n",
            "(89,)\n",
            "(98,)\n",
            "(407,)\n",
            "(57,)\n",
            "(290,)\n",
            "(219,)\n",
            "(62,)\n",
            "(119,)\n",
            "(186,)\n",
            "(87,)\n",
            "(121,)\n",
            "(67,)\n",
            "(72,)\n",
            "(96,)\n",
            "(196,)\n",
            "(105,)\n",
            "(521,)\n",
            "(632,)\n",
            "(1599,)\n",
            "(150,)\n",
            "(58,)\n",
            "(299,)\n",
            "(231,)\n",
            "(77,)\n",
            "(134,)\n",
            "(190,)\n",
            "(91,)\n",
            "(120,)\n",
            "(83,)\n",
            "(62,)\n",
            "(60,)\n",
            "(110,)\n",
            "(51,)\n",
            "(248,)\n",
            "(124,)\n",
            "(87,)\n",
            "(64,)\n",
            "(157,)\n",
            "(152,)\n",
            "(60,)\n",
            "(69,)\n",
            "(73,)\n",
            "(202,)\n",
            "(219,)\n",
            "(87,)\n",
            "(94,)\n",
            "(53,)\n",
            "(67,)\n",
            "(62,)\n",
            "(92,)\n",
            "(203,)\n",
            "(461,)\n",
            "(1109,)\n",
            "(217,)\n",
            "(121,)\n",
            "(102,)\n",
            "(60,)\n",
            "(164,)\n",
            "(97,)\n",
            "(77,)\n",
            "(108,)\n",
            "(83,)\n",
            "(324,)\n",
            "(226,)\n",
            "(137,)\n",
            "(85,)\n",
            "(68,)\n",
            "(214,)\n",
            "(96,)\n",
            "(121,)\n",
            "(55,)\n",
            "(168,)\n",
            "(61,)\n",
            "(116,)\n",
            "(52,)\n",
            "(54,)\n",
            "(102,)\n",
            "(290,)\n",
            "(63,)\n",
            "(1213,)\n",
            "(111,)\n",
            "(103,)\n",
            "(670,)\n",
            "(102,)\n",
            "(72,)\n",
            "(120,)\n",
            "(53,)\n",
            "(54,)\n",
            "(185,)\n",
            "(95,)\n",
            "(84,)\n",
            "(154,)\n",
            "(55,)\n",
            "(151,)\n",
            "(56,)\n",
            "(305,)\n",
            "(186,)\n",
            "(55,)\n",
            "(524,)\n",
            "(62,)\n",
            "(52,)\n",
            "(75,)\n",
            "(126,)\n",
            "(207,)\n",
            "(166,)\n",
            "(53,)\n",
            "(201,)\n",
            "(447,)\n",
            "(237,)\n",
            "(69,)\n",
            "(54,)\n",
            "(611,)\n",
            "(102,)\n",
            "(126,)\n",
            "(242,)\n",
            "(60,)\n",
            "(131,)\n",
            "(76,)\n",
            "(63,)\n",
            "(191,)\n",
            "(210,)\n",
            "(66,)\n",
            "(143,)\n",
            "(95,)\n",
            "(83,)\n",
            "(65,)\n",
            "(270,)\n",
            "(110,)\n",
            "(148,)\n",
            "(60,)\n",
            "(136,)\n",
            "(237,)\n",
            "(532,)\n",
            "(102,)\n",
            "(54,)\n",
            "(69,)\n",
            "(274,)\n",
            "(53,)\n",
            "(144,)\n",
            "(52,)\n",
            "(217,)\n",
            "(594,)\n",
            "(116,)\n",
            "(90,)\n",
            "(271,)\n",
            "(59,)\n",
            "(112,)\n",
            "(457,)\n",
            "(92,)\n",
            "(55,)\n",
            "(212,)\n",
            "(242,)\n",
            "(183,)\n",
            "(105,)\n",
            "(131,)\n",
            "(52,)\n",
            "(56,)\n",
            "(77,)\n",
            "(88,)\n",
            "(51,)\n",
            "(138,)\n",
            "(235,)\n",
            "(58,)\n",
            "(151,)\n",
            "(64,)\n",
            "(1017,)\n",
            "(91,)\n",
            "(63,)\n",
            "(52,)\n",
            "(652,)\n",
            "(51,)\n",
            "(67,)\n",
            "(73,)\n",
            "(69,)\n",
            "(89,)\n",
            "(61,)\n",
            "(127,)\n",
            "(639,)\n",
            "(859,)\n",
            "(721,)\n",
            "(77,)\n",
            "(191,)\n",
            "(83,)\n",
            "(302,)\n",
            "(94,)\n",
            "(63,)\n",
            "(110,)\n",
            "(86,)\n",
            "(282,)\n",
            "(81,)\n",
            "(118,)\n",
            "(255,)\n",
            "(78,)\n",
            "(236,)\n",
            "(53,)\n",
            "(512,)\n",
            "(155,)\n",
            "(136,)\n",
            "(461,)\n",
            "(54,)\n",
            "(126,)\n",
            "(113,)\n",
            "(51,)\n",
            "(82,)\n",
            "(83,)\n",
            "(113,)\n",
            "(110,)\n",
            "(63,)\n",
            "(108,)\n",
            "(798,)\n",
            "(69,)\n",
            "(163,)\n",
            "(55,)\n",
            "(81,)\n",
            "(62,)\n",
            "(66,)\n",
            "(114,)\n",
            "(66,)\n",
            "(56,)\n",
            "(348,)\n",
            "(160,)\n",
            "(229,)\n",
            "(132,)\n",
            "(191,)\n",
            "(66,)\n",
            "(197,)\n",
            "(422,)\n",
            "(79,)\n",
            "(118,)\n",
            "(59,)\n",
            "(78,)\n",
            "(100,)\n",
            "(53,)\n",
            "(59,)\n",
            "(166,)\n",
            "(52,)\n",
            "(163,)\n",
            "(91,)\n",
            "(1036,)\n",
            "(81,)\n",
            "(93,)\n",
            "(111,)\n",
            "(87,)\n",
            "(108,)\n",
            "(52,)\n",
            "(63,)\n",
            "(158,)\n",
            "(173,)\n",
            "(130,)\n",
            "(237,)\n",
            "(57,)\n",
            "(102,)\n",
            "(69,)\n",
            "(63,)\n",
            "(78,)\n",
            "(63,)\n",
            "(51,)\n",
            "(56,)\n",
            "(82,)\n",
            "(55,)\n",
            "(60,)\n",
            "(56,)\n",
            "(57,)\n",
            "(59,)\n",
            "(51,)\n",
            "(58,)\n",
            "(61,)\n",
            "(188,)\n",
            "(70,)\n",
            "(57,)\n",
            "(68,)\n",
            "(109,)\n",
            "(110,)\n",
            "(141,)\n",
            "(51,)\n",
            "(92,)\n",
            "(236,)\n",
            "(552,)\n",
            "(80,)\n",
            "(348,)\n",
            "(64,)\n",
            "(60,)\n",
            "(107,)\n",
            "(96,)\n",
            "(176,)\n",
            "(113,)\n",
            "(67,)\n",
            "(168,)\n",
            "(336,)\n",
            "(107,)\n",
            "(52,)\n",
            "(144,)\n",
            "(566,)\n",
            "(431,)\n",
            "(653,)\n",
            "(131,)\n",
            "(163,)\n",
            "(51,)\n",
            "(118,)\n",
            "(62,)\n",
            "(92,)\n",
            "(83,)\n",
            "(58,)\n",
            "(51,)\n",
            "(134,)\n",
            "(73,)\n",
            "(380,)\n",
            "(111,)\n",
            "(89,)\n",
            "(102,)\n",
            "(57,)\n",
            "(182,)\n",
            "(119,)\n",
            "(62,)\n",
            "(51,)\n",
            "(256,)\n",
            "(104,)\n",
            "(79,)\n",
            "(324,)\n",
            "(80,)\n",
            "(317,)\n",
            "(180,)\n",
            "(130,)\n",
            "(918,)\n",
            "(53,)\n",
            "(55,)\n",
            "(57,)\n",
            "(129,)\n",
            "(221,)\n",
            "(180,)\n",
            "(99,)\n",
            "(53,)\n",
            "(57,)\n",
            "(60,)\n",
            "(175,)\n",
            "(121,)\n",
            "(74,)\n",
            "(92,)\n",
            "(58,)\n",
            "(152,)\n",
            "(180,)\n",
            "(114,)\n",
            "(212,)\n",
            "(60,)\n",
            "(74,)\n",
            "(664,)\n",
            "(61,)\n",
            "(104,)\n",
            "(53,)\n",
            "(108,)\n",
            "(53,)\n",
            "(87,)\n",
            "(55,)\n",
            "(71,)\n",
            "(82,)\n",
            "(73,)\n",
            "(301,)\n",
            "(162,)\n",
            "(231,)\n",
            "(109,)\n",
            "(55,)\n",
            "(114,)\n",
            "(134,)\n",
            "(106,)\n",
            "(56,)\n",
            "(87,)\n",
            "(52,)\n",
            "(104,)\n",
            "(109,)\n",
            "(53,)\n",
            "(181,)\n",
            "(61,)\n",
            "(53,)\n",
            "(79,)\n",
            "(239,)\n",
            "(280,)\n",
            "(52,)\n",
            "(136,)\n",
            "(64,)\n",
            "(76,)\n",
            "(397,)\n",
            "(102,)\n",
            "(76,)\n",
            "(1030,)\n",
            "(85,)\n",
            "(100,)\n",
            "(52,)\n",
            "(71,)\n",
            "(61,)\n",
            "(149,)\n",
            "(509,)\n",
            "(51,)\n",
            "(63,)\n",
            "(61,)\n",
            "(58,)\n",
            "(167,)\n",
            "(51,)\n",
            "(149,)\n",
            "(241,)\n",
            "(55,)\n",
            "(89,)\n",
            "(77,)\n",
            "(79,)\n",
            "(96,)\n",
            "(301,)\n",
            "(57,)\n",
            "(174,)\n",
            "(103,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = \"list all movie names?\"\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Connect to the database and create a cursor\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all the table names in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Get all the columns in the matching tables\n",
        "columns = []\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    cols = [description[0] for description in cursor.description]\n",
        "    columns.extend(cols)\n",
        "\n",
        "# Define a function to convert the question into an enriched question using the semantic representation of the question and the table names and columns in the database\n",
        "def enrich_question(question, output, tables, columns):\n",
        "    # Get the semantic representation of the question\n",
        "    question_embedding = output[0][0][0]\n",
        "\n",
        "    # Find the table name that is most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_table = \"\"\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, table_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_table = table[0]\n",
        "\n",
        "    # Find the columns that are most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_columns = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, column_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_columns = [column]\n",
        "        elif similarity == max_similarity:\n",
        "            max_similarity_columns.append(column)\n",
        "\n",
        "    # Remove any columns that do not exist in the selected table\n",
        "    cursor.execute(\"SELECT * FROM \" + max_similarity_table)\n",
        "    valid_columns = [description[0] for description in cursor.description]\n",
        "    max_similarity_columns = [column for column in max_similarity_columns if column in valid_columns]\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in max_similarity_columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "# Use the enriched question to search the database for matching rows\n",
        "enriched_question = enrich_question(question, output, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "# Execute the enriched question\n",
        "cursor.execute(enriched_question)\n",
        "results = cursor.fetchall()\n",
        "\n",
        "# Print the results\n",
        "print(\"Results:\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "IGFq-6vuO3-A",
        "outputId": "a38a5bcf-4643-4d80-ec98-821fa79256be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enriched question:\n",
            "SELEC FROM M_Location\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-adb9dc1e3d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Execute the enriched question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menriched_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: near \"SELEC\": syntax error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Get Input English Question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Preprocess the English question\n",
        "input_ids = torch.tensor(tokenizer.encode(\n",
        "    question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "question_representation = model(input_ids)[0]  # Last layer hidden-state\n",
        "\n",
        "# Iterate over tables in the database\n",
        "for table in tables:\n",
        "    # Execute a SELECT statement on the table\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    rows = cursor.fetchall()\n",
        "    # Iterate over rows in the table\n",
        "    for row in rows:\n",
        "        # Iterate over values in the row\n",
        "        for i, value in enumerate(row):\n",
        "            # Preprocess the value\n",
        "            value_input_ids = torch.tensor(\n",
        "                tokenizer.encode(str(value))).unsqueeze(0)\n",
        "            # Generate a semantic representation of the value\n",
        "            value_representation = model(value_input_ids)[0]\n",
        "            # Calculate cosine similarity between the question representation and the value representation\n",
        "            similarity = cosine_similarity(\n",
        "                question_representation.detach().numpy(), value_representation.detach().numpy())[0][0]\n",
        "            # If the similarity is above a certain threshold, print the table name and column name as a match\n",
        "            if similarity > 0.8:\n",
        "                print(\"Match found in table \" + table[0] +\n",
        "                      \" in column \" + cursor.description[i][0])\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "iZG6gtINRkdA",
        "outputId": "1cdee04b-b01d-4143-d212-03815daf3f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: List all the comedy movies?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ec3b2de3a1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mvalue_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Calculate cosine similarity between the question representation and the value representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             similarity = cosine_similarity(\n\u001b[0m\u001b[1;32m     44\u001b[0m                 question_representation.detach().numpy(), value_representation.detach().numpy())[0][0]\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# If the similarity is above a certain threshold, print the table name and column name as a match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         X = check_array(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 ) from e\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    795\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the input question\n",
        "input_question = \"Which tables in the database contain columns related to movie ratings?\"\n",
        "tokens = nltk.word_tokenize(input_question)\n",
        "tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Generate word embeddings for the input question\n",
        "input_embeddings = model(torch.tensor([tokenizer.encode(input_question, add_special_tokens=True)]))[0]\n",
        "\n",
        "# Loop through the table names and column names in the database dictionary\n",
        "for table_name, column_names in database_dictionary.items():\n",
        "  # Preprocess the table name\n",
        "  table_tokens = nltk.word_tokenize(table_name)\n",
        "  table_tokens = [token.lower() for token in table_tokens if token.isalpha()]\n",
        "  table_tokens = [token for token in table_tokens if token not in stopwords.words('english')]\n",
        "  table_tokens = [stemmer.stem(token) for token in table_tokens]\n",
        "\n",
        "  # Generate word embeddings for the table name\n",
        "  table_embeddings = model(torch.tensor([tokenizer.encode(table_name, add_special_tokens=True)]))[0]\n",
        "\n",
        "  # Calculate the semantic similarity between the input question and the table name\n",
        "  similarity = cosine_similarity(input_embeddings, table_embeddings)\n",
        "\n",
        "  # If the similarity is high, print the table name and its column names\n",
        "   # If the similarity is high, print the table name and its column names\n",
        "  if similarity > 0.5:\n",
        "    print(f\"Table name: {table_name}\")\n",
        "    print(\"Column names:\")\n",
        "    for column_name in column_names:\n",
        "      # Preprocess the column name\n",
        "      column_tokens = nltk.word_tokenize(column_name)\n",
        "      column_tokens = [token.lower() for token in column_tokens if token.isalpha()]\n",
        "      column_tokens = [token for token in column_tokens if token not in stopwords.words('english')]\n",
        "      column_tokens = [stemmer.stem(token) for token in column_tokens]\n",
        "\n",
        "      # Generate word embeddings for the column name\n",
        "      column_embeddings = model(torch.tensor([tokenizer.encode(column_name, add_special_tokens=True)]))[0]\n",
        "\n",
        "      # Calculate the semantic similarity between the input question and the column name\n",
        "      column_similarity = cosine_similarity(input_embeddings, column_embeddings)\n",
        "\n",
        "      # If the similarity is high, print the column name\n",
        "      if column_similarity > 0.5:\n",
        "        print(f\"- {column_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "jzJwombZRlIw",
        "outputId": "1555f430-aa40-4fb6-e35e-cc00f69c81b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-97888dacefe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Preprocess the input question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Which tables in the database contain columns related to movie ratings?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0iCB0ZQUrCO",
        "outputId": "3e22c887-66b5-43d7-8b41-0e3435753c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert\n",
            "  Downloading bert-2.2.0.tar.gz (3.5 kB)\n",
            "Collecting erlastic\n",
            "  Downloading erlastic-2.0.0.tar.gz (6.8 kB)\n",
            "Building wheels for collected packages: bert, erlastic\n",
            "  Building wheel for bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert: filename=bert-2.2.0-py3-none-any.whl size=3764 sha256=fb9bcc493726fe03c5efc90fbc6a7945aa230fa84609e6008d50710ce8e2761c\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/11/40/6439aef2635f7f0137a79c4defb4c4e65dd051ec0198429e3b\n",
            "  Building wheel for erlastic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for erlastic: filename=erlastic-2.0.0-py3-none-any.whl size=6795 sha256=7de545b7a6281d5c4a6a1c6573ff096644d0bf5b9de3faf5328d4f5ff3395359\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/c9/a6/41a81618e939b746a3151700565d191bca832b6c345ea9b87a\n",
            "Successfully built bert erlastic\n",
            "Installing collected packages: erlastic, bert\n",
            "Successfully installed bert-2.2.0 erlastic-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install elasticsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "LHYDcuLcUyMi",
        "outputId": "9c1334ee-032c-4eeb-ff6b-e7c14b8a8a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.5.3-py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting elastic-transport<9,>=8\n",
            "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
            "Collecting urllib3<2,>=1.26.2\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 15.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, elastic-transport, elasticsearch\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n",
            "Successfully installed elastic-transport-8.4.0 elasticsearch-8.5.3 urllib3-1.26.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import bert\n",
        "import elasticsearch\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect(\"Db-IMDB.db\")\n",
        "\n",
        "# Get all table names in the database\n",
        "table_names = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "\n",
        "# Get all column names in each table\n",
        "column_names = {}\n",
        "for table_name in table_names:\n",
        "    columns = conn.execute(\"SELECT * FROM \" + table_name + \" LIMIT 1;\")\n",
        "    column_names[table_name] = [column[0] for column in columns.description]\n",
        "\n",
        "# Use BERT or other NLP technique to process the English question\n",
        "processed_question = bert.process_question(english_question)\n",
        "\n",
        "# Flatten the list of column names into a single list\n",
        "flat_column_names = [item for sublist in column_names.values() for item in sublist]\n",
        "\n",
        "# Use Elasticsearch or other semantic search technique to search the processed question in the table and column names\n",
        "results = elasticsearch.search(processed_question, list(table_names) + flat_column_names)\n",
        "\n",
        "# Print the matching tables and columns\n",
        "for result in results:\n",
        "    if result in table_names:\n",
        "        print(\"Matching table: \" + result)\n",
        "    else:\n",
        "        for table, columns in column_names.items():\n",
        "            if result in columns:\n",
        "                print(\"Matching column in table \" + table + \": \" + result)\n",
        "\n",
        "\n",
        "\n",
        "# close connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "NNwCKbvDUcAY",
        "outputId": "f2d24d5d-e8c7-47c1-c87b-97e2ed25b4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-03f7e46db4fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" LIMIT 1;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mcolumn_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"tuple\") to str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('DB-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the English question\n",
        "input_ids = tokenizer.encode(\n",
        "    'What are the Movie titles?',\n",
        "    return_tensors='pt',\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "# Pass the encoded question through the BERT model\n",
        "output = model(input_ids)[0]\n",
        "\n",
        "# Perform semantic search on the database\n",
        "table_names = []\n",
        "column_names = []\n",
        "for row in cursor.execute('SELECT * FROM sqlite_master WHERE type=\"table\"'):\n",
        "    table_name = row[1]\n",
        "    table_vector = model(tokenizer.encode(table_name, return_tensors='pt', add_special_tokens=True))[0]\n",
        "    similarity = output.dot(table_vector).mean().item()\n",
        "    if similarity > 0.5:\n",
        "        table_names.append(table_name)\n",
        "        for column in cursor.execute(f'PRAGMA table_info({table_name})'):\n",
        "            column_name = column[1]\n",
        "            column_vector = model(tokenizer.encode(column_name, return_tensors='pt', add_special_tokens=True))[0]\n",
        "            similarity = output.dot(column_vector).mean().item()\n",
        "            if similarity > 0.5:\n",
        "                column_names.append(column_name)\n",
        "\n",
        "print(f'Table names: {table_names}')\n",
        "print(f'Column names: {column_names}')\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_2G1axpUkx4",
        "outputId": "a6107858-e0e3-43b9-9043-76925db55d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table names: []\n",
            "Column names: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect(\"Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get a list of all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Convert the English question to a BERT-encoded input\n",
        "question = \"What are the names of movies?\"\n",
        "input_ids = torch.tensor([tokenizer.encode(question, add_special_tokens=True)])\n",
        "\n",
        "# Use the BERT model to generate embeddings for the input\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    embeddings = outputs[0]\n",
        "\n",
        "# Loop through each table in the database\n",
        "for table in tables:\n",
        "    table_name = table[0]\n",
        "    \n",
        "    # Get a list of all columns in the table\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    columns = cursor.fetchall()\n",
        "    \n",
        "    # Loop through each column in the table\n",
        "    for column in columns:\n",
        "        column_name = column[1]\n",
        "        \n",
        "        # Use the BERT model to generate embeddings for the column name\n",
        "        column_input_ids = torch.tensor([tokenizer.encode(column_name, add_special_tokens=True)])\n",
        "        with torch.no_grad():\n",
        "            column_outputs = model(column_input_ids)\n",
        "            column_embeddings = column_outputs[0]\n",
        "        \n",
        "        # Calculate the cosine similarity between the embeddings of the question and the column name\n",
        "        similarity = torch.nn.functional.cosine_similarity(embeddings[0].view(1, -1), column_embeddings[0].view(1, -1), dim=1)\n",
        "        \n",
        "        # If the similarity is above a certain threshold, print the table name and column name\n",
        "        if similarity > 0.5:\n",
        "            print(f\"Table: {table_name}, Column: {column_name}\")\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "rxnKqKo2aW2B",
        "outputId": "34f4d22a-46be-41eb-8320-5552cd5f842a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-52c8bb535010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Calculate the cosine similarity between the embeddings of the question and the column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# If the similarity is above a certain threshold, print the table name and column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6912) must match the size of tensor b (2304) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect(\"Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get a list of all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Convert the English question to a BERT-encoded input\n",
        "question = \"What are the names of movies?\"\n",
        "input_ids = torch.tensor([tokenizer.encode(question, add_special_tokens=True)])\n",
        "\n",
        "# Use the BERT model to generate embeddings for the input\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    question_embeddings = outputs[0]\n",
        "\n",
        "# Loop through each table in the database\n",
        "for table in tables:\n",
        "    table_name = table[0]\n",
        "    \n",
        "    # Get a list of all columns in the table\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    columns = cursor.fetchall()\n",
        "    \n",
        "    # Loop through each column in the table\n",
        "    for column in columns:\n",
        "        column_name = column[1]\n",
        "        \n",
        "        # Use the BERT model to generate embeddings for the column name\n",
        "        column_input_ids = torch.tensor([tokenizer.encode(column_name, add_special_tokens=True)])\n",
        "        with torch.no_grad():\n",
        "            column_outputs = model(column_input_ids)\n",
        "            column_embeddings = column_outputs[0]\n",
        "        \n",
        "        # Calculate the cosine similarity between the embeddings of the question and the column name\n",
        "        similarity = torch.nn.functional.cosine_similarity(question_embeddings[0].view(1, -1), column_embeddings[0].view(1, -1))\n",
        "\n",
        "        # If the similarity is above a certain threshold, print the table name and column name\n",
        "        if similarity > 0.5:\n",
        "            print(f\"Table: {table_name}, Column: {column_name}\")\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "NTqYuKUvaXgY",
        "outputId": "f74d3731-1a46-4dcf-dce9-a02b9ca00e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-92c1b70ecc06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Calculate the cosine similarity between the embeddings of the question and the column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# If the similarity is above a certain threshold, print the table name and column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6912) must match the size of tensor b (2304) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('DB-IMDB.db')\n",
        "\n",
        "# Get a cursor object\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Execute a SELECT statement to get the table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "\n",
        "# Get the table names\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "# Print the table names\n",
        "print('Tables:', tables)\n",
        "\n",
        "# Loop through the table names\n",
        "for table in tables:\n",
        "    # Get the column names for the table\n",
        "    cursor.execute(f\"SELECT * FROM {table[0]} LIMIT 1;\")\n",
        "    column_names = [description[0] for description in cursor.description]\n",
        "\n",
        "    # Print the column names for the table\n",
        "    print(f'Columns for {table[0]}: {column_names}')\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEBNW_gLb49U",
        "outputId": "fa6bd7e3-6ad7-4dfb-b1ab-8e4ad183da19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# Store the table names and column names in a dictionary, whree the key is the table name and the value is a list of column names\n",
        "table_dict = {}\n",
        "for table in tables:\n",
        "    cursor.execute(\"PRAGMA table_info({})\".format(table[0]))\n",
        "    table_dict[table[0]] = [column[1] for column in cursor.fetchall()]\n",
        "\n",
        "# Print the table names and column names\n",
        "for table in table_dict:\n",
        "    print(table)\n",
        "    print(table_dict[table])\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyqqOHxXdEXl",
        "outputId": "6b16c4ba-1f74-4752-df4c-7083acee5b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Genre\n",
            "['index', 'Name', 'GID']\n",
            "Language\n",
            "['index', 'Name', 'LAID']\n",
            "Country\n",
            "['index', 'Name', 'CID']\n",
            "Location\n",
            "['index', 'Name', 'LID']\n",
            "M_Location\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "M_Country\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "M_Language\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "M_Genre\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Person\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "M_Producer\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Director\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Cast\n",
            "['index', 'MID', 'PID', 'ID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get all tables in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "\n",
        "# Store the table names and column names in a dictionary, whree the key is the table name and the value is a list of column names\n",
        "table_dict = {}\n",
        "for table in tables:\n",
        "    cursor.execute(\"PRAGMA table_info({})\".format(table[0]))\n",
        "    table_dict[table[0]] = [column[1] for column in cursor.fetchall()]\n",
        "\n",
        "# Print the table names and column names\n",
        "for table in table_dict:\n",
        "    print(table)\n",
        "    print(table_dict[table])\n",
        "\n",
        "# Get English Question from user\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Tokenize the question\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_question = tokenizer.tokenize(question)\n",
        "\n",
        "# Semantic search for the question to find the semantically matching table names and column names of the database\n",
        "# The semantic search is done using the BERT model\n",
        "def semantic_search(question, table_dict):\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "    question_embedding = model(tokenizer(question, return_tensors=\"pt\")[\"input_ids\"])[0]\n",
        "    table_dict_embeddings = {}\n",
        "    for table in table_dict:\n",
        "        table_dict_embeddings[table] = model(tokenizer(\" \".join(table_dict[table]), return_tensors=\"pt\")[\"input_ids\"])[0]\n",
        "    table_dict_scores = {}\n",
        "    for table in table_dict_embeddings:\n",
        "        table_dict_scores[table] = model.cosine_similarity(question_embedding, table_dict_embeddings[table])\n",
        "    return table_dict_scores\n",
        "\n",
        "# Get the table name and column name with the highest score\n",
        "def get_best_match(table_dict_scores):\n",
        "    best_match = max(table_dict_scores, key=table_dict_scores.get)\n",
        "    best_match_score = table_dict_scores[best_match]\n",
        "    return best_match, best_match_score\n",
        "\n",
        "# Print the table name and column name with the highest score\n",
        "table_dict_scores = semantic_search(question, table_dict)\n",
        "best_match, best_match_score = get_best_match(table_dict_scores)\n",
        "print(\"Best match: \", best_match)\n",
        "print(\"Best match score: \", best_match_score)\n",
        "\n",
        "\n",
        "# close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "CiBo-fveeHvW",
        "outputId": "20673abb-d5b3-42e3-b90e-6e57991fbb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Genre\n",
            "['index', 'Name', 'GID']\n",
            "Language\n",
            "['index', 'Name', 'LAID']\n",
            "Country\n",
            "['index', 'Name', 'CID']\n",
            "Location\n",
            "['index', 'Name', 'LID']\n",
            "M_Location\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "M_Country\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "M_Language\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "M_Genre\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Person\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "M_Producer\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Director\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Cast\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enter your question: What are titles of Movies?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7bb30911ba4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Print the table name and column name with the highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtable_dict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best match: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-7bb30911ba4c>\u001b[0m in \u001b[0;36msemantic_search\u001b[0;34m(question, table_dict)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtable_dict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_dict_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_dict_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'cosine_similarity'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "def get_tables_and_columns(conn):\n",
        "  # Create a cursor\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "  # Get all tables in the database\n",
        "  cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "  tables = cursor.fetchall()\n",
        "\n",
        "  # Store the table names and column names in a dictionary, where the key is the table name and the value is a list of column names\n",
        "  table_dict = {}\n",
        "  for table in tables:\n",
        "      cursor.execute(\"PRAGMA table_info({})\".format(table[0]))\n",
        "      table_dict[table[0]] = [column[1] for column in cursor.fetchall()]\n",
        "  return table_dict\n",
        "\n",
        "def semantic_search(question, table_dict):\n",
        "    # Tokenize the question\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_question = tokenizer.tokenize(question)\n",
        "\n",
        "    # Use BERT model to find the semantically matching table names and column names of the database\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "    question_embedding = model(tokenizer(question, return_tensors=\"pt\")[\"input_ids\"])[0]\n",
        "    table_dict_embeddings = {}\n",
        "    for table in table_dict:\n",
        "        table_dict_embeddings[table] = model(tokenizer(\" \".join(table_dict[table]), return_tensors=\"pt\")[\"input_ids\"])[0]\n",
        "    table_dict_scores = {}\n",
        "    for table in table_dict_embeddings:\n",
        "        table_dict_scores[table] = model.cosine_similarity(question_embedding, table_dict_embeddings[table])\n",
        "    return table_dict_scores\n",
        "\n",
        "def get_best_match(table_dict_scores):\n",
        "    # Get the table name and column name with the highest score\n",
        "    best_match = max(table_dict_scores, key=table_dict_scores.get)\n",
        "    best_match_score = table_dict_scores[best_match]\n",
        "    return best_match, best_match_score\n",
        "\n",
        "def main():\n",
        "  # Load the database and create a connection\n",
        "  conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "\n",
        "  # Get the table names and column names\n",
        "  table_dict = get_tables_and_columns(conn)\n",
        "\n",
        "  # Print the table names and column names\n",
        "  for table in table_dict:\n",
        "      print(table)\n",
        "      print(table_dict[table])\n",
        "\n",
        "  # Get English Question from user\n",
        "  question = input(\"Enter your question: \")\n",
        "\n",
        "  # Get the table name and column name with the highest score\n",
        "  table_dict_scores = semantic_search(question, table_dict)\n",
        "  best_match, best_match_score = get_best_match(table_dict_scores)\n",
        "  print(\"Best match: \", best_match)\n",
        "  print(\"Best match score: \", best_match_score)\n",
        "\n",
        "  # Close the connection\n",
        "  conn.close()\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "Y15tvHI7fZQ1",
        "outputId": "5cfd1185-dc50-4baa-9d52-7668291c4bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Genre\n",
            "['index', 'Name', 'GID']\n",
            "Language\n",
            "['index', 'Name', 'LAID']\n",
            "Country\n",
            "['index', 'Name', 'CID']\n",
            "Location\n",
            "['index', 'Name', 'LID']\n",
            "M_Location\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "M_Country\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "M_Language\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "M_Genre\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Person\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "M_Producer\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Director\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Cast\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enter your question: What are the tiles of movies?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-bb382f62a9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-bb382f62a9f5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;31m# Get the table name and column name with the highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0mtable_dict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0mbest_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best match: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-bb382f62a9f5>\u001b[0m in \u001b[0;36msemantic_search\u001b[0;34m(question, table_dict)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtable_dict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_dict_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_dict_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'cosine_similarity'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "def get_tables_and_columns(conn):\n",
        "  # Create a cursor\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "  # Get all tables in the database\n",
        "  cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "  tables = cursor.fetchall()\n",
        "\n",
        "  # Store the table names and column names in a dictionary, where the key is the table name and the value is a list of column names\n",
        "  table_dict = {}\n",
        "  for table in tables:\n",
        "      cursor.execute(\"PRAGMA table_info({})\".format(table[0]))\n",
        "      table_dict[table[0]] = [column[1] for column in cursor.fetchall()]\n",
        "  return table_dict\n",
        "\n",
        "def semantic_search(question, table_dict):\n",
        "    # Tokenize the question\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenized_question = tokenizer.tokenize(question)\n",
        "\n",
        "    # Use BERT model to find the semantically matching table names and column names of the database\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "    question_embedding = model(tokenizer(question, return_tensors=\"pt\")[\"input_ids\"])[0]\n",
        "    table_dict_embeddings = {}\n",
        "    for table in table_dict:\n",
        "        table_dict_embeddings[table] = model(tokenizer(\" \".join(table_dict[table]), return_tensors=\"pt\")[\"input_ids\"])[0]\n",
        "    table_dict_scores = {}\n",
        "    for table in table_dict_embeddings:\n",
        "        table_dict_scores[table] = torch.nn.functional.cosine_similarity(question_embedding, table_dict_embeddings[table])\n",
        "    return table_dict_scores\n",
        "\n",
        "def get_best_match(table_dict_scores):\n",
        "    # Get the table name and column name with the highest score\n",
        "    best_match = max(table_dict_scores, key=table_dict_scores.get)\n",
        "    best_match_score = table_dict_scores[best_match]\n",
        "    return best_match, best_match_score\n",
        "\n",
        "def main():\n",
        "  # Load the database and create a connection\n",
        "  conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "\n",
        "  # Get the table names and column names\n",
        "  table_dict = get_tables_and_columns(conn)\n",
        "\n",
        "  # Print the table names and column names\n",
        "  for table in table_dict:\n",
        "      print(table)\n",
        "      print(table_dict[table])\n",
        "\n",
        "  # Get English Question from user\n",
        "  question = input(\"Enter your question: \")\n",
        "\n",
        "  # Get the table name and column name with the highest score\n",
        "  table_dict_scores = semantic_search(question, table_dict)\n",
        "  best_match, best_match_score = get_best_match(table_dict_scores)\n",
        "  print(\"Best match: \", best_match)\n",
        "  print(\"Best match score: \", best_match_score)\n",
        "\n",
        "  # Close the connection\n",
        "  conn.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "dIydYHLlgExG",
        "outputId": "a0a1b956-cb84-482a-f94c-239e076cd3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Genre\n",
            "['index', 'Name', 'GID']\n",
            "Language\n",
            "['index', 'Name', 'LAID']\n",
            "Country\n",
            "['index', 'Name', 'CID']\n",
            "Location\n",
            "['index', 'Name', 'LID']\n",
            "M_Location\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "M_Country\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "M_Language\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "M_Genre\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Person\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "M_Producer\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Director\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "M_Cast\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enter your question: what are the titles of movies?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d07073006ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-d07073006ef3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;31m# Get the table name and column name with the highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mtable_dict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0mbest_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best match: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-d07073006ef3>\u001b[0m in \u001b[0;36msemantic_search\u001b[0;34m(question, table_dict)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtable_dict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_dict_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_dict_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtable_dict_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (11) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = input(\"Enter English Question: \")\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "# Use the semantic representation to search the database for matching table names\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Matching tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "# function to convert question into enriched question using semantic representation of question and table names and columns in the database \n",
        "def enrich_question(question, output, tables, columns):\n",
        "    # Get the semantic representation of the question\n",
        "    question_embedding = output[0][0][0]\n",
        "\n",
        "    # Get the semantic representation of the table names\n",
        "    table_embeddings = []\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        table_embeddings.append(table_embedding)\n",
        "\n",
        "    # Get the semantic representation of the columns\n",
        "    column_embeddings = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        column_embeddings.append(column_embedding)\n",
        "\n",
        "    # Find the table name that is most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_table = \"\"\n",
        "    for table in tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table[0]).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, table_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_table = table[0]\n",
        "\n",
        "    # Find the columns that are most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_columns = []\n",
        "    for column in columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, column_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_columns = [column]\n",
        "        elif similarity == max_similarity:\n",
        "            max_similarity_columns.append(column)\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in max_similarity_columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "# Use enriched question to search the database for matching rows\n",
        "enriched_question = enrich_question(question, output, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlRJrqp-hUfa",
        "outputId": "e9098f31-3d60-4410-9282-83b39795caef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English Question: What are the titles of movies?\n",
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enriched question:\n",
            "SELECT PID FROM M_Location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the database and create a connection\n",
        "conn = sqlite3.connect(\"/content/Db-IMDB.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess the English question\n",
        "question = input(\"Enter English Question: \")\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)  # Batch size 1\n",
        "\n",
        "# Generate a semantic representation of the question\n",
        "output = model(input_ids)\n",
        "\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Tables:\")\n",
        "for table in tables:\n",
        "    print(table[0])\n",
        "\n",
        "# Use the semantic representation to print all columns in the matching tables\n",
        "for table in tables:\n",
        "    cursor.execute(\"SELECT * FROM \" + table[0])\n",
        "    columns = [description[0] for description in cursor.description]\n",
        "    print(\"Columns in table \" + table[0] + \":\")\n",
        "    print(columns)\n",
        "\n",
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    # Remove punctuation and make the text lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    stopwords = [\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\", \"to\", \"was\", \"were\", \"will\", \"with\"]\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    text = \" \".join(filtered_words)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# function to convert question into enriched question using semantic representation of question and table names and columns in the database \n",
        "def enrich_question(question, output, tables, columns):\n",
        "    # Preprocess the question\n",
        "    question = preprocess(question)\n",
        "\n",
        "    # Get the semantic representation of the preprocessed question\n",
        "    question_embedding = output[0][0][tokenizer.encode(question).index(102)]\n",
        "\n",
        "    # Preprocess the table names\n",
        "    processed_tables = []\n",
        "    for table in tables:\n",
        "        processed_tables.append(preprocess(table[0]))\n",
        "\n",
        "    # Get the semantic representation of the preprocessed table names\n",
        "    table_embeddings = []\n",
        "    for table in processed_tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table).index(102)]\n",
        "        table_embeddings.append(table_embedding)\n",
        "\n",
        "    # Preprocess the columns\n",
        "    processed_columns = []\n",
        "    for column in columns:\n",
        "        processed_columns.append(preprocess(column))\n",
        "\n",
        "    # Get the semantic representation of the preprocessed columns\n",
        "    column_embeddings = []\n",
        "    for column in processed_columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        column_embeddings.append(column_embedding)\n",
        "\n",
        "    # Find the table name that is most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_table = \"\"\n",
        "    for table in processed_tables:\n",
        "        table_embedding = output[0][0][tokenizer.encode(table).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, table_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_table = table\n",
        "\n",
        "    # Find the columns that are most similar to the semantic representation of the question\n",
        "    max_similarity = 0\n",
        "    max_similarity_columns = []\n",
        "    for column in processed_columns:\n",
        "        column_embedding = output[0][0][tokenizer.encode(column).index(102)]\n",
        "        similarity = torch.cosine_similarity(question_embedding, column_embedding, dim=0)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            max_similarity_columns = [column]\n",
        "        elif similarity == max_similarity:\n",
        "            max_similarity_columns.append(column)\n",
        "\n",
        "    # Create the enriched question\n",
        "    enriched_question = \"SELECT \"\n",
        "    for column in max_similarity_columns:\n",
        "        enriched_question += column + \", \"\n",
        "    enriched_question = enriched_question[:-2] + \" FROM \" + max_similarity_table\n",
        "\n",
        "    return enriched_question\n",
        "\n",
        "\n",
        "# Use enriched question to search the database for matching rows\n",
        "enriched_question = enrich_question(question, output, tables, columns)\n",
        "print(\"Enriched question:\")\n",
        "print(enriched_question)\n",
        "\n",
        "# Close the connection to the database\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POuP_dU8jfqa",
        "outputId": "33fb5186-3ae0-42ef-8a49-ab950a0dcf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English Question: What are the titles of movies?\n",
            "Matching tables:\n",
            "Movie\n",
            "Genre\n",
            "Language\n",
            "Country\n",
            "Location\n",
            "M_Location\n",
            "M_Country\n",
            "M_Language\n",
            "M_Genre\n",
            "Person\n",
            "M_Producer\n",
            "M_Director\n",
            "M_Cast\n",
            "Columns in table Movie:\n",
            "['index', 'MID', 'title', 'year', 'rating', 'num_votes']\n",
            "Columns in table Genre:\n",
            "['index', 'Name', 'GID']\n",
            "Columns in table Language:\n",
            "['index', 'Name', 'LAID']\n",
            "Columns in table Country:\n",
            "['index', 'Name', 'CID']\n",
            "Columns in table Location:\n",
            "['index', 'Name', 'LID']\n",
            "Columns in table M_Location:\n",
            "['index', 'MID', 'LID', 'ID']\n",
            "Columns in table M_Country:\n",
            "['index', 'MID', 'CID', 'ID']\n",
            "Columns in table M_Language:\n",
            "['index', 'MID', 'LAID', 'ID']\n",
            "Columns in table M_Genre:\n",
            "['index', 'MID', 'GID', 'ID']\n",
            "Columns in table Person:\n",
            "['index', 'PID', 'Name', 'Gender']\n",
            "Columns in table M_Producer:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Director:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Columns in table M_Cast:\n",
            "['index', 'MID', 'PID', 'ID']\n",
            "Enriched question:\n",
            "SELECT pid FROM m_location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Set up the BERT model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def find_semantically_similar_word(words, query):\n",
        "  # Tokenize the words and the query\n",
        "  tokens = [tokenizer.tokenize(word) for word in words]\n",
        "  query_tokens = tokenizer.tokenize(query)\n",
        "\n",
        "  # Convert the tokens to BERT's input format\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(tokens[i]) for i in range(len(tokens))]\n",
        "\n",
        "  query_input_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
        "\n",
        "  # Use BERT to encode the words and the query\n",
        "  word_encodings = [model(torch.tensor(input_ids).unsqueeze(0))[0][0] for input_ids in input_ids]\n",
        "  query_encoding = model(torch.tensor(query_input_ids).unsqueeze(0))[0][0]\n",
        "\n",
        "  # Find the word with the highest cosine similarity to the query\n",
        "  most_similar_word = None\n",
        "  highest_similarity = -1\n",
        "  for i, encoding in enumerate(word_encodings):\n",
        "    similarity = torch.nn.functional.cosine_similarity(encoding, query_encoding, dim=0)\n",
        "    if similarity > highest_similarity:\n",
        "      most_similar_word = words[i]\n",
        "      highest_similarity = similarity\n",
        "  return most_similar_word\n",
        "\n",
        "# Example usage\n",
        "words = ['cat', 'dog', 'mouse', 'elephant']\n",
        "query = 'pet'\n",
        "print(find_semantically_similar_word(words, query))  # Outputs 'cat'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "yYVk06MFj9-N",
        "outputId": "4849b6d2-45c9-4078-9efc-cbfcf5013e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8c0bca78e455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mouse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elephant'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_semantically_similar_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Outputs 'cat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-8c0bca78e455>\u001b[0m in \u001b[0;36mfind_semantically_similar_word\u001b[0;34m(words, query)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mhighest_similarity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mmost_similar_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mhighest_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the Sentence Transformer model\n",
        "model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\n",
        "\n",
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get the names of all tables and columns in the database\n",
        "table_names = cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
        "table_names = [name[0] for name in table_names]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    column_names_in_table = cursor.execute(f\"PRAGMA table_info({table_name});\").fetchall()\n",
        "    column_names_in_table = [name[1] for name in column_names_in_table]\n",
        "    column_names.extend(column_names_in_table)\n",
        "\n",
        "# Encode the table and column names using the Sentence Transformer model\n",
        "encoded_table_names = model.encode(table_names)\n",
        "encoded_column_names = model.encode(column_names)\n",
        "\n",
        "# Create a Faiss index with the encoded table and column names\n",
        "import faiss\n",
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "encoded_names = np.concatenate([encoded_table_names, encoded_column_names])\n",
        "index.add_with_ids(encoded_names, np.arange(len(table_names) + len(column_names)))\n",
        "\n",
        "\n",
        "# Define the search function\n",
        "def search(query, top_k, index, model):\n",
        "    query_vector = model.encode([query])\n",
        "    top_k = index.search(query_vector, top_k)\n",
        "    top_k_ids = top_k[1].tolist()[0]\n",
        "    top_k_ids = list(np.unique(top_k_ids))\n",
        "    return top_k_ids\n",
        "\n",
        "# Get the English question\n",
        "english_question = input(\"Enter English question: \")\n",
        "\n",
        "# Perform semantic search on the database and print the names of the matching tables and columns\n",
        "matching_ids = search(english_question, top_k=len(table_names) + len(column_names), index=index, model=model)\n",
        "for idx in matching_ids:\n",
        "    if idx < len(table_names):\n",
        "        print(f\"Matching table: {table_names[idx]}\")\n",
        "    else:\n",
        "        print(f\"Matching column: {column_names[idx - len(table_names)]}\")\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ6yjGCkloSX",
        "outputId": "e0616683-4983-43bd-da35-e793e630b27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English question: What are the titles of movies?\n",
            "Matching table: Movie\n",
            "Matching table: Genre\n",
            "Matching table: Language\n",
            "Matching table: Country\n",
            "Matching table: Location\n",
            "Matching table: M_Location\n",
            "Matching table: M_Country\n",
            "Matching table: M_Language\n",
            "Matching table: M_Genre\n",
            "Matching table: Person\n",
            "Matching table: M_Producer\n",
            "Matching table: M_Director\n",
            "Matching table: M_Cast\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: title\n",
            "Matching column: year\n",
            "Matching column: rating\n",
            "Matching column: num_votes\n",
            "Matching column: index\n",
            "Matching column: Name\n",
            "Matching column: GID\n",
            "Matching column: index\n",
            "Matching column: Name\n",
            "Matching column: LAID\n",
            "Matching column: index\n",
            "Matching column: Name\n",
            "Matching column: CID\n",
            "Matching column: index\n",
            "Matching column: Name\n",
            "Matching column: LID\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: LID\n",
            "Matching column: ID\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: CID\n",
            "Matching column: ID\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: LAID\n",
            "Matching column: ID\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: GID\n",
            "Matching column: ID\n",
            "Matching column: index\n",
            "Matching column: PID\n",
            "Matching column: Name\n",
            "Matching column: Gender\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: PID\n",
            "Matching column: ID\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: PID\n",
            "Matching column: ID\n",
            "Matching column: index\n",
            "Matching column: MID\n",
            "Matching column: PID\n",
            "Matching column: ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsCjqssZpN_q",
        "outputId": "4739cead-c5d8-4631-ef82-7d9985f43e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 83 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faiss-gpu"
      ],
      "metadata": {
        "id": "ttoghjBepRoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es6jNa6vv-Ny",
        "outputId": "34abfea1-de09-434e-d577-9a81310f6a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.25.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.11.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query word\n",
        "query_word = 'what are the names of movies'\n",
        "\n",
        "# List of words\n",
        "words = ['title', 'movie', 'genre', 'comedy']\n",
        "\n",
        "# Tokenize query word and words\n",
        "query_word_encoded = tokenizer([query_word], padding=True, truncation=True, return_tensors='pt')\n",
        "words_encoded = tokenizer(words, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query word and words\n",
        "with torch.no_grad():\n",
        "    query_word_output = model(**query_word_encoded)\n",
        "    words_output = model(**words_encoded)\n",
        "\n",
        "# Perform pooling for query word and words\n",
        "query_word_embedding = mean_pooling(query_word_output, query_word_encoded['attention_mask'])\n",
        "words_embeddings = mean_pooling(words_output, words_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query word and words\n",
        "query_word_embedding = F.normalize(query_word_embedding, p=2, dim=1)\n",
        "words_embeddings = F.normalize(words_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar word by computing the cosine similarity between the query word embedding and the word embeddings\n",
        "cosine_similarities = torch.nn.functional.cosine_similarity(query_word_embedding, words_embeddings, dim=1)\n",
        "most_similar_word_index = cosine_similarities.argmax().item()\n",
        "most_similar_word = words[most_similar_word_index]\n",
        "\n",
        "print(f\"Most similar word to '{query_word}': {most_similar_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onq5OjjtwICc",
        "outputId": "e3011bd3-3812-448e-b0c8-4520f3a45109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'what are the names of movies': movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query word\n",
        "query_word = 'what are the comedy of movies'\n",
        "\n",
        "# List of words\n",
        "words = ['movie', 'MID', 'title', 'year', 'rating', 'num_votes', 'genre']\n",
        "\n",
        "# Tokenize query word and words\n",
        "query_word_encoded = tokenizer([query_word], padding=True, truncation=True, return_tensors='pt')\n",
        "words_encoded = tokenizer(words, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query word and words\n",
        "with torch.no_grad():\n",
        "    query_word_output = model(**query_word_encoded)\n",
        "    words_output = model(**words_encoded)\n",
        "\n",
        "# Perform pooling for query word and words\n",
        "query_word_embedding = mean_pooling(query_word_output, query_word_encoded['attention_mask'])\n",
        "words_embeddings = mean_pooling(words_output, words_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query word and words\n",
        "query_word_embedding = F.normalize(query_word_embedding, p=2, dim=1)\n",
        "words_embeddings = F.normalize(words_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar word by computing the cosine similarity between the query word embedding and the word embeddings\n",
        "cosine_similarities = torch.nn.functional.cosine_similarity(query_word_embedding, words_embeddings, dim=1)\n",
        "most_similar_word_index = cosine_similarities.argmax().item()\n",
        "most_similar_word = words[most_similar_word_index]\n",
        "\n",
        "print(f\"Most similar word to '{query_word}': {most_similar_word}\")\n",
        "\n",
        "\n",
        "# Print the most similar words in descending order\n",
        "print(\"Most similar words in descending order:\")\n",
        "for i in cosine_similarities.argsort(descending=True):\n",
        "    print(words[i], cosine_similarities[i].item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKTmN4kJxQvc",
        "outputId": "63cc5fd4-a623-45cf-b61c-5a4297b4568d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar word to 'what are the comedy of movies': movie\n",
            "Most similar words in descending order:\n",
            "movie 0.43206480145454407\n",
            "genre 0.3209477961063385\n",
            "rating 0.13822606205940247\n",
            "title 0.05144535377621651\n",
            "MID 0.019145943224430084\n",
            "year 0.013283293694257736\n",
            "num_votes -0.014807472936809063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query sentence\n",
        "query_sentence = 'What are the names of movies'\n",
        "\n",
        "# List of words\n",
        "words = ['title', 'movie', 'person', 'country']\n",
        "\n",
        "# Tokenize query sentence and words\n",
        "query_sentence_encoded = tokenizer([query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "words_encoded = tokenizer(words, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence and words\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    words_output = model(**words_encoded)\n",
        "\n",
        "# Perform pooling for query sentence and words\n",
        "query_sentence_embedding = mean_pooling(query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "words_embeddings = mean_pooling(words_output, words_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence and words\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "words_embeddings = F.normalize(words_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar words by computing the cosine similarity between the query sentence embedding and the word embeddings\n",
        "cosine_similarities = torch.nn.functional.cosine_similarity(query_sentence_embedding, words_embeddings, dim=1)\n",
        "most_similar_words_indices = cosine_similarities.argsort(descending=True)\n",
        "most_similar_words = [words[i] for i in most_similar_words_indices]\n",
        "\n",
        "print(f\"Most similar words to '{query_sentence}': {most_similar_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFtPP2YPzDkb",
        "outputId": "15d3a0e0-21de-4c97-f662-cb255e1ddda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'What are the names of movies': ['movie', 'title', 'country', 'person']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query sentence\n",
        "query_sentence = 'What are the names of movies?'\n",
        "\n",
        "# Connect to database and fetch table names\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "\n",
        "# Tokenize query sentence and table names\n",
        "query_sentence_encoded = tokenizer([query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence and table names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence and table names\n",
        "query_sentence_embedding = mean_pooling(query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence and table names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names by computing the cosine similarity between the query sentence embedding and the table names embeddings\n",
        "cosine_similarities = torch.nn.functional.cosine_similarity(query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities.argsort(descending=True)\n",
        "most_similar_table_names = [table_names[i] for i in most_similar_table_names_indices]\n",
        "\n",
        "print(f\"Most similar table names to '{query_sentence}': {most_similar_table_names}\")\n",
        "\n",
        "# Close database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-GgDYIzD16T",
        "outputId": "6e048809-0630-4ad0-9e60-1aa15617dda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar table names to 'What are the names of movies?': ['Movie', 'Genre', 'M_Genre', 'M_Cast', 'M_Director', 'Language', 'Country', 'M_Country', 'M_Producer', 'Location', 'Person', 'M_Language', 'M_Location']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query sentence\n",
        "query_sentence = 'what are the names of movies?'\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer([query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "column_names_encoded = tokenizer(column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "    column_names_output = model(**column_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(table_names_output, table_names_encoded['attention_mask'])\n",
        "column_names_embeddings = mean_pooling(column_names_output, column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "column_names_embeddings = F.normalize(column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(descending=True)\n",
        "most_similar_table_names = [table_names[i] for i in most_similar_table_names_indices]\n",
        "\n",
        "cosine_similarities_columns = torch.nn.functional.cosine_similarity(query_sentence_embedding, column_names_embeddings, dim=1)\n",
        "most_similar_column_names_indices = cosine_similarities_columns.argsort(descending=True)\n",
        "most_similar_column_names = [column_names[i] for i in most_similar_column_names_indices]\n",
        "\n",
        "print(f\"Most similar table names to '{query_sentence}': {most_similar_table_names}\")\n",
        "print(f\"Most similar column names to '{query_sentence}': {most_similar_column_names}\")\n",
        "\n",
        "# Close database connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqwni5tsFGZh",
        "outputId": "c690fff2-94e1-45e2-f8e0-a6cfb4d4bfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar table names to 'what are the names of movies?': ['Movie', 'Genre', 'M_Genre', 'M_Cast', 'M_Director', 'Language', 'Country', 'M_Country', 'M_Producer', 'Location', 'Person', 'M_Language', 'M_Location']\n",
            "Most similar column names to 'what are the names of movies?': ['Name', 'Name', 'Name', 'Name', 'Name', 'title', 'rating', 'LAID', 'LAID', 'Gender', 'GID', 'GID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'CID', 'CID', 'PID', 'PID', 'PID', 'PID', 'year', 'LID', 'LID', 'num_votes', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'ID', 'ID', 'ID', 'ID', 'ID', 'ID', 'ID']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query sentence\n",
        "query_sentence = 'what are the names of films?'\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer([query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "column_names_encoded = tokenizer(column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "    column_names_output = model(**column_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(table_names_output, table_names_encoded['attention_mask'])\n",
        "column_names_embeddings = mean_pooling(column_names_output, column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "column_names_embeddings = F.normalize(column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(descending=True)\n",
        "most_similar_table_names = [table_names[i] for i in most_similar_table_names_indices]\n",
        "\n",
        "cosine_similarities_columns = torch.nn.functional.cosine_similarity(query_sentence_embedding, column_names_embeddings, dim=1)\n",
        "most_similar_column_names_indices = cosine_similarities_columns.argsort(descending=True)\n",
        "most_similar_column_names = [column_names[i] for i in most_similar_column_names_indices]\n",
        "\n",
        "print(f\"Most similar table names to '{query_sentence}': {most_similar_table_names}\")\n",
        "print(f\"Most similar column names to '{query_sentence}': {most_similar_column_names}\")\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "\n",
        "########\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [column_info[1] for column_info in cursor.fetchall()]\n",
        "highest_matching_table_column_names_encoded = tokenizer(highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(**highest_matching_table_column_names_encoded)\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "highest_matching_table_column_names_embeddings = F.normalize(highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[most_similar_highest_matching_table_column_name_index]\n",
        "print(\"----------------------------------\")\n",
        "print(f\"Most similar column name in the highest matching table ({highest_matching_table_name}): {most_similar_highest_matching_table_column_name}\")\n",
        "\n",
        "    \n",
        "\n",
        "# Close database connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydspvawwG8VS",
        "outputId": "d05e1134-9905-4017-c76e-fe05970b8e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar table names to 'what are the names of films?': ['Movie', 'Genre', 'M_Genre', 'M_Director', 'M_Cast', 'Language', 'M_Producer', 'Country', 'M_Country', 'Location', 'Person', 'M_Language', 'M_Location']\n",
            "Most similar column names to 'what are the names of films?': ['Name', 'Name', 'Name', 'Name', 'Name', 'title', 'rating', 'LAID', 'LAID', 'year', 'CID', 'CID', 'PID', 'PID', 'PID', 'PID', 'GID', 'GID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LID', 'LID', 'Gender', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'index', 'ID', 'ID', 'ID', 'ID', 'ID', 'ID', 'ID', 'num_votes']\n",
            "Table name: Movie, cosine similarity score: 0.5168206691741943\n",
            "Table name: Genre, cosine similarity score: 0.4417805075645447\n",
            "Table name: M_Genre, cosine similarity score: 0.3323620557785034\n",
            "Table name: M_Director, cosine similarity score: 0.2808853089809418\n",
            "Table name: M_Cast, cosine similarity score: 0.2548985183238983\n",
            "Table name: Language, cosine similarity score: 0.19627761840820312\n",
            "Table name: M_Producer, cosine similarity score: 0.18257884681224823\n",
            "Table name: Country, cosine similarity score: 0.17386282980442047\n",
            "Table name: M_Country, cosine similarity score: 0.14677780866622925\n",
            "Table name: Location, cosine similarity score: 0.13867762684822083\n",
            "Table name: Person, cosine similarity score: 0.09092726558446884\n",
            "Table name: M_Language, cosine similarity score: 0.0904441773891449\n",
            "Table name: M_Location, cosine similarity score: 0.07097361236810684\n",
            "----------------------------------\n",
            "Most similar column name in the highest matching table (Movie): title\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar_table_name = table_names[most_similar_table_names_indices[0]]\n",
        "most_similar_column_name_index = cosine_similarities_columns.argmax()\n",
        "most_similar_column_name = column_names[most_similar_column_name_index]\n",
        "most_similar_column_name_index = cosine_similarities_columns.argmax()\n",
        "most_similar_column_name = column_names[most_similar_column_name_index]\n",
        "print(f\"Most matching table name: {most_similar_table_name}\")\n",
        "print(f\"Highest matching column name: {most_similar_column_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xceQESl7Jrno",
        "outputId": "a926a9f7-7968-4e93-d48a-99fa2c4ef68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most matching table name: Movie\n",
            "Highest matching column name: index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [column_info[1] for column_info in cursor.fetchall()]\n",
        "highest_matching_table_column_names_encoded = tokenizer(highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(**highest_matching_table_column_names_encoded)\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "highest_matching_table_column_names_embeddings = F.normalize(highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[most_similar_highest_matching_table_column_name_index]\n",
        "print(f\"Most similar column name in the highest matching table ({highest_matching_table_name}): {most_similar_highest_matching_table_column_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "zgFGRYbtZ10g",
        "outputId": "8508e1b3-c7aa-48d7-8a5b-3abd5d43b8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ProgrammingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-896c5be3903f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_similarity_table_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarities_tables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhighest_matching_table_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_similarity_table_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PRAGMA table_info({highest_matching_table_name});\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhighest_matching_table_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhighest_matching_table_column_names_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhighest_matching_table_column_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProgrammingError\u001b[0m: Cannot operate on a closed database."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query sentence\n",
        "query_sentence = 'what are the names comedy films?'\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names by computing the cosine similarity between the query sentence embedding and the table names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name by using the index obtained above\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Find the column names of the highest matching table by querying the database\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [\n",
        "    column_info[1] for column_info in cursor.fetchall()]\n",
        "\n",
        "# Tokenize the column names of the highest matching table\n",
        "highest_matching_table_column_names_encoded = tokenizer(\n",
        "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute the token embeddings for the column names of the highest matching table\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(\n",
        "        **highest_matching_table_column_names_encoded)\n",
        "\n",
        "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(\n",
        "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize the embeddings for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = F.normalize(\n",
        "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "\n",
        "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
        "    most_similar_highest_matching_table_column_name_index]\n",
        "\n",
        "# Print the most similar column name in the highest matching table\n",
        "print(\"----------------------------------\")\n",
        "print(\n",
        "    f\"Most similar column name in the highest matching table ({highest_matching_table_name}): {most_similar_highest_matching_table_column_name}\")\n",
        "\n",
        "\n",
        "# Close database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb2y8A6MmA7I",
        "outputId": "48f8c64a-d38b-4f66-c9ae-de02f76876d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table name: Movie, cosine similarity score: 0.4493207037448883\n",
            "Table name: Genre, cosine similarity score: 0.37098297476768494\n",
            "Table name: M_Genre, cosine similarity score: 0.24984751641750336\n",
            "Table name: M_Cast, cosine similarity score: 0.22762948274612427\n",
            "Table name: M_Director, cosine similarity score: 0.2252969741821289\n",
            "Table name: Country, cosine similarity score: 0.15246644616127014\n",
            "Table name: M_Producer, cosine similarity score: 0.13296377658843994\n",
            "Table name: Language, cosine similarity score: 0.13268189132213593\n",
            "Table name: M_Country, cosine similarity score: 0.1308581829071045\n",
            "Table name: Person, cosine similarity score: 0.0890883207321167\n",
            "Table name: Location, cosine similarity score: 0.08226652443408966\n",
            "Table name: M_Language, cosine similarity score: 0.0446857213973999\n",
            "Table name: M_Location, cosine similarity score: 0.016101090237498283\n",
            "----------------------------------\n",
            "Most similar column name in the highest matching table (Movie): title\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import json\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Query the sqlite_master table to get the names and types of all objects in the database\n",
        "cursor.execute(\"SELECT name, type FROM sqlite_master WHERE type IN ('table', 'view')\")\n",
        "objects = cursor.fetchall()\n",
        "\n",
        "# Initialize the schema dictionary\n",
        "schema = {}\n",
        "\n",
        "# Iterate over the objects and add an entry to the schema dictionary for each object\n",
        "for obj in objects:\n",
        "    obj_name, obj_type = obj\n",
        "    schema[obj_name] = {}\n",
        "    if obj_type == 'table':\n",
        "        # Query the PRAGMA table_info function to get the column names and types of the table\n",
        "        cursor.execute(f\"PRAGMA table_info({obj_name});\")\n",
        "        columns = cursor.fetchall()\n",
        "        schema[obj_name] = {column[1]: column[2] for column in columns}\n",
        "    elif obj_type == 'view':\n",
        "        # Query the PRAGMA view_info function to get the column names and types of the view\n",
        "        cursor.execute(f\"PRAGMA view_info({obj_name});\")\n",
        "        columns = cursor.fetchall()\n",
        "        schema[obj_name] = {column[1]: column[2] for column in columns}\n",
        "\n",
        "# Convert the schema dictionary to a JSON string\n",
        "schema_json = json.dumps(schema)\n",
        "\n",
        "# Print the JSON string\n",
        "print(schema_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMTdCgVnmBiX",
        "outputId": "03bd3f2f-befd-47e5-97c7-0aafa836350e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"Movie\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"title\": \"TEXT\", \"year\": \"TEXT\", \"rating\": \"REAL\", \"num_votes\": \"INTEGER\"}, \"Genre\": {\"index\": \"INTEGER\", \"Name\": \"TEXT\", \"GID\": \"INTEGER\"}, \"Language\": {\"index\": \"INTEGER\", \"Name\": \"TEXT\", \"LAID\": \"INTEGER\"}, \"Country\": {\"index\": \"INTEGER\", \"Name\": \"TEXT\", \"CID\": \"INTEGER\"}, \"Location\": {\"index\": \"INTEGER\", \"Name\": \"TEXT\", \"LID\": \"INTEGER\"}, \"M_Location\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"LID\": \"REAL\", \"ID\": \"INTEGER\"}, \"M_Country\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"CID\": \"REAL\", \"ID\": \"INTEGER\"}, \"M_Language\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"LAID\": \"INTEGER\", \"ID\": \"INTEGER\"}, \"M_Genre\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"GID\": \"INTEGER\", \"ID\": \"INTEGER\"}, \"Person\": {\"index\": \"INTEGER\", \"PID\": \"TEXT\", \"Name\": \"TEXT\", \"Gender\": \"TEXT\"}, \"M_Producer\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"PID\": \"TEXT\", \"ID\": \"INTEGER\"}, \"M_Director\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"PID\": \"TEXT\", \"ID\": \"INTEGER\"}, \"M_Cast\": {\"index\": \"INTEGER\", \"MID\": \"TEXT\", \"PID\": \"TEXT\", \"ID\": \"INTEGER\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import json\n",
        "\n",
        "# Connect to the database\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Query the sqlite_master table to get the names and types of all objects in the database\n",
        "cursor.execute(\"SELECT name, type FROM sqlite_master WHERE type IN ('table', 'view')\")\n",
        "objects = cursor.fetchall()\n",
        "\n",
        "# Initialize the schema dictionary\n",
        "schema = {}\n",
        "\n",
        "# Iterate over the objects and add an entry to the schema dictionary for each object\n",
        "for obj in objects:\n",
        "    obj_name, obj_type = obj\n",
        "    schema[obj_name] = {}\n",
        "    if obj_type == 'table':\n",
        "        # Query the PRAGMA table_info function to get the column names and types of the table\n",
        "        cursor.execute(f\"PRAGMA table_info({obj_name});\")\n",
        "        columns = cursor.fetchall()\n",
        "        schema[obj_name] = {column[1]: column[2] for column in columns}\n",
        "    elif obj_type == 'view':\n",
        "        # Query the PRAGMA view_info function to get the column names and types of the view\n",
        "        cursor.execute(f\"PRAGMA view_info({obj_name});\")\n",
        "        columns = cursor.fetchall()\n",
        "        schema[obj_name] = {column[1]: column[2] for column in columns}\n",
        "\n",
        "# Convert the schema dictionary to a JSON string\n",
        "schema_json = json.dumps(schema)\n",
        "\n",
        "# Open a file for writing\n",
        "f = open('schema.json', 'w')\n",
        "\n",
        "# Write the JSON string to the file\n",
        "f.write(schema_json)\n",
        "\n",
        "# Close the file\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "WSe_7Nx4o8DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Query sentence\n",
        "query_sentence = 'what are the names comedy films?'\n",
        "\n",
        "# Tokenize query sentence, table names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names\n",
        "query_sentence_embedding = F.adaptive_avg_pool1d(\n",
        "    query_sentence_output[0], 1).squeeze()\n",
        "table_names_embeddings = F.adaptive_avg_pool1d(\n",
        "    table_names_output[0], 1).squeeze()\n",
        "\n",
        "# Normalize embeddings for query sentence, table names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names by computing the cosine similarity between the query sentence embedding and the table names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with their cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "eqYxg44ppgdn",
        "outputId": "1945270f-fbb4-4e8d-ae65-fde4a9f3dfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-a1745d78d06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Normalize embeddings for query sentence, table names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mquery_sentence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_sentence_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mtable_names_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_names_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4632\u001b[0;31m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4634\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             )\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21kJAOfzz0fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"juierror/text-to-sql-with-table-schema\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"juierror/text-to-sql-with-table-schema\")\n",
        "\n",
        "def prepare_input(question: str, table: List[str]):\n",
        "    table_prefix = \"table:\"\n",
        "    question_prefix = \"question:\"\n",
        "    join_table = \",\".join(table)\n",
        "    inputs = f\"{question_prefix} {question} {table_prefix} {join_table}\"\n",
        "    input_ids = tokenizer(inputs, max_length=700, return_tensors=\"pt\").input_ids\n",
        "    return input_ids\n",
        "\n",
        "def inference(question: str, table: List[str]) -> str:\n",
        "    input_data = prepare_input(question=question, table=table)\n",
        "    input_data = input_data.to(model.device)\n",
        "    outputs = model.generate(inputs=input_data, num_beams=10, top_k=10, max_length=700)\n",
        "    result = tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)\n",
        "    return result\n",
        "\n",
        "print(inference(question=\"what are the names of films released in 2018 with rating greater than 6 ?\", table=[\"title\", \"year\", \"rating\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXZPwq-3wRJD",
        "outputId": "b6ebe668-9a6a-4836-ed9e-c11cadf8e5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT title FROM table WHERE rating > 6 AND year = 2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inference(question=\"what are the name of films of rating greater than 6 ?\", table=[\"title\", \"year\", \"rating\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDI_jiKLzzQJ",
        "outputId": "0fffcaa4-c5a9-4e4c-eb1a-e58d57b57b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT title FROM table WHERE rating > 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOBUx7w2f6Dj",
        "outputId": "474b6d03-ec07-4abf-f20e-45b896881ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 6.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
      ],
      "metadata": {
        "id": "JTF8WY5a183n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "56f7ec8f7fe344fea4186d807b4641ad",
            "6979c4b2eede4aaf9d6a7ccd4d1d4d90",
            "e81b733a46a2457c9a2329e7111d5f1a",
            "64f957b5765d406484225b40dbdf1c2e",
            "0829812637fd4780814ea9745c12299d",
            "7c06f7b878424d6886b75b728d9b25ca",
            "ac49c4394d40410b9709e117663dc219",
            "e7334508cbc04489a3ac02d8e1ca3703",
            "902b40365fd244348358136fb0f5b9a9",
            "dd8850c003904f2495f0a4d1f80ad506",
            "9564feba93e74a679b6c73609a2aa6da",
            "db04a71bf01e4825a7fe113b9db21ea8",
            "57881d264ddb423fad029f3bc95e86ab",
            "9143ede7bc1b4eca953a674733a16fe7",
            "4687504556294908a0f721ad1ab4a484",
            "4b4e917211d7467d938405070af092d7",
            "438dc7a31aae46ee88ab3e02044470e8",
            "318246cf645b41beb61c703cee606ad2",
            "e97231d88b8946c8a560b0bff2cd9c3e",
            "e3feaeb873ed4e64b6978dfb5c353f39",
            "baab95d6adc643c38d914287931983a4",
            "1ce0e8e93ccb4bc785ce814938651208",
            "463055764eba466899d4f6da2c8ddc28",
            "bb91cfe6749d47809f7759278579a1b7",
            "366ae2b3245c42c59df1ddf1b7999d9b",
            "9c0c382a8c734d8391aa6874e983349d",
            "21094ff6737d48eea20c59afbbf58165",
            "dd05aeee258a443587b0c69c41079573",
            "044d75eabd4d42f3a89e735d81ecff48",
            "31f87b16439546a599fcea3be96ba05c",
            "e585127a3a774612b7dc970ca990cb72",
            "ff31ce7c93004311aba2f791a0c4e192",
            "f522127a4fef49b684836e1b65cafdd9",
            "b8a769de54a94fc0ac1e4f1593cffdb7",
            "5a467b8a7e564ea1a23938ac9a17441c",
            "209e1a67288d4f36819aefd95c1842bd",
            "e20a69b858284e71b80bcba1b0f7514d",
            "942c51985bdc47d4b98d8d8436601d7c",
            "14fdba29680a4def9deed78cc2a92418",
            "a748ad2fe7ed46e78eba5ed0d3f162ff",
            "f7d9ee6dfeaa4d028a1dc428a01f8263",
            "b6087eca9c8846a2bf33e80723b68753",
            "24e46aa868444344bb266e55b2dbe283",
            "05cecc765f344108beecdb36c3c64d31",
            "9b954b918af64befbb99d659ecdde8dc",
            "785d31be913e4608b3a2aa019bc82403",
            "f16cf505dca14298a01db031c40ca62b",
            "29ab502ac790408a9202bb12b44595fa",
            "d09b3d908b3145bf98ad1cbb07bf6def",
            "e84a81dea0684215b3dffc7ad175ec76",
            "7159447b181e444c8ba3e39bb14cc21f",
            "9271ef926739455f99157ea2a8c95313",
            "460e98a060fa4b13b5e5412dbd3aa6a1",
            "d6ea6c3adb4c4d93b25d664566648399",
            "9d9b4a78b18140deabeb392a961f8722"
          ]
        },
        "outputId": "7add0914-47a2-462a-b953-df45aa4ed926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56f7ec8f7fe344fea4186d807b4641ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db04a71bf01e4825a7fe113b9db21ea8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "463055764eba466899d4f6da2c8ddc28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8a769de54a94fc0ac1e4f1593cffdb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b954b918af64befbb99d659ecdde8dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, I'm writing a new language for you. But first, I'd like to tell you about the language itself\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I'm trying to be as expressive as possible. In order to be expressive, it is necessary to know\"},\n",
              " {'generated_text': \"Hello, I'm a language model, so I don't get much of a license anymore, but I'm probably more familiar with other languages on that\"},\n",
              " {'generated_text': \"Hello, I'm a language model, a functional model... It's not me, it's me!\\n\\nI won't bore you with how\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give language model a set of properties that\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Text to SQL: List all the comedy movies?\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwYqZUMugRsF",
        "outputId": "b2b1941f-a707-409f-c38d-09f63fa60501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Text to SQL: List all the comedy movies? 1.11: (1) The King, and (2) The Adventures of Tintin'},\n",
              " {'generated_text': \"Text to SQL: List all the comedy movies? List them, and try to tell what people say. In our case, we've seen the movie\"},\n",
              " {'generated_text': 'Text to SQL: List all the comedy movies?\\n\\nWhen talking about how to play Comedy as a genre, you probably get all this sort of'},\n",
              " {'generated_text': 'Text to SQL: List all the comedy movies?'},\n",
              " {'generated_text': \"Text to SQL: List all the comedy movies? Click here and follow the instructions!\\n\\nIf you're in the habit of scrolling through the movies\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load the pre-trained language model\n",
        "model = transformers.AutoModel.from_pretrained('gpt2')\n",
        "\n",
        "# Preprocess the input text to extract relevant information\n",
        "input_text = \"Show me the sales data for the product with ID 123\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_tokens = transformers.PreTrainedTokenizer.from_pretrained('gpt2').tokenize(input_text)\n",
        "\n",
        "# Generate a SQL statement\n",
        "output_tokens = model.generate(input_tokens, max_length=100, top_k=100, top_p=0.9, num_return_sequences=1)\n",
        "\n",
        "# Convert the output tokens to a string\n",
        "output_sql = transformers.PreTrainedTokenizer.from_pretrained('gpt2').decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(output_sql)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "9e7k3sgFgTEm",
        "outputId": "f67a93e5-1d14-413b-ea89-e629f47e4ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-22f9cbb4a3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Tokenize the input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Generate a SQL statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1786\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a PreTrainedTokenizer tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3fi3uKinnjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "# Load the GPT-2 model from Hugging Face\n",
        "model = transformers.GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "# Preprocess the input text to extract relevant information\n",
        "input_text = \"Show me the average salary of employees in the sales department\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_tokens = transformers.GPT2Tokenizer.from_pretrained('gpt2').tokenize(input_text)\n",
        "\n",
        "# Convert the tokens to an input tensor\n",
        "input_tensor = torch.tensor(input_tokens).unsqueeze(0)\n",
        "\n",
        "# Generate a SQL statement using the GPT-2 model\n",
        "output = model.generate(input_tensor)\n",
        "output_tokens = output[0].tolist()\n",
        "output_text = transformers.GPT2Tokenizer.from_pretrained('gpt2').decode(output_tokens)\n",
        "\n",
        "# Extract the generated SQL statement from the output text\n",
        "sql_statement = extract_sql_from_output_text(output_text)\n",
        "\n",
        "print(sql_statement)  # Output: SELECT AVG(salary) FROM employees WHERE department = 'sales'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ZHaCLQJGnff9",
        "outputId": "6b27b6af-a213-490d-8cac-09f0c878fc5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dbe6cd5eacaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Convert the tokens to an input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Generate a SQL statement using the GPT-2 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load the GPT-2 model from Hugging Face\n",
        "model = transformers.GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "# Preprocess the input text to extract relevant information\n",
        "input_text = \"Show me the average salary of employees in the sales department\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
        "input_tokens = tokenizer.tokenize(input_text)\n",
        "\n",
        "# Convert the tokens to an input tensor\n",
        "input_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(input_tokens)).unsqueeze(0)\n",
        "\n",
        "# Generate a SQL statement using the GPT-2 model\n",
        "output = model.generate(input_tensor)\n",
        "output_tokens = output[0].tolist()\n",
        "output_text = tokenizer.decode(output_tokens)\n",
        "\n",
        "# Extract the generated SQL statement from the output text\n",
        "sql_statement = extract_sql_from_output_text(output_text)\n",
        "\n",
        "print(sql_statement)  # Output: SELECT AVG(salary) FROM employees WHERE department = 'sales'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "TxXqq6D8n15M",
        "outputId": "ff2188df-fdef-4809-af09-6424c9d42a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bb9287741d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Generate a SQL statement using the GPT-2 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         ```\"\"\"\n\u001b[1;32m   1294\u001b[0m         \u001b[0;31m# 0. Validate the `.generate()` call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerate_compatible_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0mexception_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" Please use one of the following classes instead: {generate_compatible_classes}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load a pretrained language model\n",
        "model = transformers.BertForQuestionAnswering.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Define the English question and database column name\n",
        "question = \"How many customers are there in the database?\"\n",
        "column_name = \"customers\"\n",
        "\n",
        "# Tokenize the question and encode it for input to the model\n",
        "input_ids = transformers.BertTokenizer.from_pretrained('bert-base-cased').encode(question, return_tensors='pt')\n",
        "\n",
        "# Run the model to generate a list of possible answers to the question\n",
        "output = model(input_ids)\n",
        "answers = output[0]\n",
        "\n",
        "# Extract the relevant information from the answers to map to SQL elements\n",
        "count_function = \"count\"\n",
        "table_name = \"customers\"\n",
        "\n",
        "# Construct the SQL query\n",
        "query = f\"SELECT {count_function}(*) FROM {table_name}\"\n",
        "\n",
        "# Print the generated SQL query\n",
        "print(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "a486cc174d394e7189a0ab8e867a4f57",
            "7e74fb494a084da8811097ca4cacc811",
            "e0866ef9f7744e9c9750ea93e512d228",
            "ed8e467c30014db1a237ef9f1af1219b",
            "6fa25c401aa5499080ecd49664227a7e",
            "241de83b953b467984d8a49b57188642",
            "63e8c15180fc4fdbb234db6fd59b9274",
            "8c6c1100598a4ae4a226df7e2f8e5f6e",
            "3f796888ebee45b1b4809d2b491acf7a",
            "b30f71f3da714f1888d030ac7244660d",
            "6f5474aa234d4e349952c5aed4017311",
            "a58de913f0bf427b91da05eaa93a8e3f",
            "9498f3a919804e6e85431be6f399a4ba",
            "c5d2efc5c4a347539a4443bfcabab2f2",
            "1ea69daf7c0a4724b903a48fc4b79383",
            "ef904123bb1d412dbdb198b2260aa4f9",
            "5f0f9927b5764073a4bbd6aa75b133f3",
            "96caa72c25524137a10401d0e3ebdbb7",
            "f53375975fee4f61959fd2493e44eb31",
            "6a2c5477f010458f9380dac8d2eb972e",
            "2d5e2ea828db4318a4e267932f6be5b7",
            "e98057658c3345e8badd94a2a476471f",
            "46608e8ccab9423cb10f5705702bcbc6",
            "59cb9efd153b442780f7a61972df6bf7",
            "63539f7d5e364614a2846cf3dcafb4e5",
            "a1568a18595d411eb484c31fadbaeb95",
            "e149f1f6fe0a4c51a45ed1414893227d",
            "4bb887b2f41c462d823d57d5e9f9a981",
            "959b8a99273a4bf1bac34d678fa93ab4",
            "f04f156192eb440a8ee2f4f46c71d03a",
            "82165193dda2411cac6ed75c121d92ba",
            "d152a8b8f95142aa9054e0d8c156fa53",
            "f016f586469d4cdfb5abf70b8ac00404",
            "966fd06c693e41e1bd685630ccb48154",
            "4c8039f306d543bea974c1d5cfb89214",
            "4f8233fe343e4d5a867722b4cd345f11",
            "ca4fe1cc8b6e404786995e5e7cf3e8ae",
            "14b519f89c3a4f709a1abc5941a475d5",
            "82f6f11638f44db3a1135b2b2f4c8a11",
            "d4f99af487024c5a80a7383d38a7fe21",
            "57191384d5d74fe1a72a586b9ae897a7",
            "346e2bebe6184411ade03d7fcbdc3124",
            "b15f7d5dbb1a475b80309af9861d16b8",
            "9a656b11e5844b6aae698db9430c5e44"
          ]
        },
        "id": "a1AR1WUOoXGk",
        "outputId": "405b1508-2e60-4d3f-c68a-e4894a2f46bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a486cc174d394e7189a0ab8e867a4f57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a58de913f0bf427b91da05eaa93a8e3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46608e8ccab9423cb10f5705702bcbc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "966fd06c693e41e1bd685630ccb48154"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT count(*) FROM customers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = 'What are names of films?'\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')  # Update the path to the database file\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name by using the index obtained above\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Find the column names of the highest matching table by querying the database\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [\n",
        "    column_info[1] for column_info in cursor.fetchall()]\n",
        "\n",
        "# Tokenize the column names of the highest matching table\n",
        "highest_matching_table_column_names_encoded = tokenizer(\n",
        "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute the token embeddings for the column names of the highest matching table\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(\n",
        "        **highest_matching_table_column_names_encoded)\n",
        "\n",
        "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(\n",
        "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize the embeddings for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = F.normalize(\n",
        "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "\n",
        "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
        "    most_similar_highest_matching_table_column_name_index]\n",
        "\n",
        "\n",
        "# Generate an SQL SELECT query based on the most similar table names and column names\n",
        "query = f\"SELECT {most_similar_highest_matching_table_column_name} FROM {most_similar_table_names[0]}\"\n",
        "\n",
        "# Iterate through the list of possible queries and execute each one\n",
        "\n",
        "try:\n",
        "  cursor.execute(query)\n",
        "  results = cursor.fetchall()\n",
        "  print(f'Query: {query}')\n",
        "  print(f'Results: {results}')\n",
        "except Exception as e:\n",
        "  print(f'Error: {e}')\n",
        "\n",
        "# Print the generated SQL query\n",
        "print(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqtHi5H9pjoU",
        "outputId": "adeb129a-83e4-42d6-a10b-315757b05a21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table name: Movie, cosine similarity score: 0.5118898749351501\n",
            "Table name: Genre, cosine similarity score: 0.45985153317451477\n",
            "Table name: M_Genre, cosine similarity score: 0.35045233368873596\n",
            "Table name: M_Director, cosine similarity score: 0.2829570770263672\n",
            "Table name: M_Cast, cosine similarity score: 0.2674572169780731\n",
            "Table name: Language, cosine similarity score: 0.1999293565750122\n",
            "Table name: M_Producer, cosine similarity score: 0.19191791117191315\n",
            "Table name: Country, cosine similarity score: 0.17112775146961212\n",
            "Table name: M_Country, cosine similarity score: 0.15022629499435425\n",
            "Table name: Location, cosine similarity score: 0.13676486909389496\n",
            "Table name: Person, cosine similarity score: 0.09814347326755524\n",
            "Table name: M_Language, cosine similarity score: 0.09574835002422333\n",
            "Table name: M_Location, cosine similarity score: 0.0736055076122284\n",
            "SELECT title FROM Movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Connect to database\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Load model from Hugging Face Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-finetuned-squad')\n",
        "model = AutoModel.from_pretrained('distilbert-base-cased-finetuned-squad')\n",
        "\n",
        "# Define the English question\n",
        "question = 'What are the names of films in the database?'\n",
        "\n",
        "# Tokenize the question and encode it as a tensor\n",
        "input_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0)\n",
        "\n",
        "# Generate a token mask to indicate which tokens are part of the question\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "# Use the model to generate a list of possible SQL queries\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    query_list = outputs[0]\n",
        "\n",
        "# Iterate through the list of possible queries and execute each one\n",
        "for query in query_list:\n",
        "    try:\n",
        "        cursor.execute(query)\n",
        "        results = cursor.fetchall()\n",
        "        print(f'Query: {query}')\n",
        "        print(f'Results: {results}')\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "LexD-eXdrqqJ",
        "outputId": "0a2528a4-626e-4974-e855-58ae0d5dfd92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/distilbert-base-cased-finetuned-squad/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1068\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     )\n\u001b[0;32m-> 1376\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-63ae828e-462134084ea2cb812dbd1361)\n\nRepository Not Found for url: https://huggingface.co/distilbert-base-cased-finetuned-squad/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nInvalid username or password.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2243be9a6880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load model from Hugging Face Hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-cased-finetuned-squad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-cased-finetuned-squad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     ```\"\"\"\n\u001b[1;32m    432\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: distilbert-base-cased-finetuned-squad is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = input(\"Enter English Question: \")\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "# Update the path to the database file\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name by using the index obtained above\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Find the column names of the highest matching table by querying the database\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [\n",
        "    column_info[1] for column_info in cursor.fetchall()]\n",
        "\n",
        "# Tokenize the column names of the highest matching table\n",
        "highest_matching_table_column_names_encoded = tokenizer(\n",
        "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute the token embeddings for the column names of the highest matching table\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(\n",
        "        **highest_matching_table_column_names_encoded)\n",
        "\n",
        "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(\n",
        "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize the embeddings for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = F.normalize(\n",
        "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "\n",
        "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
        "    most_similar_highest_matching_table_column_name_index]\n",
        "\n",
        "\n",
        "# Generate an SQL SELECT query based on the most similar table names and column names\n",
        "query = f\"SELECT {most_similar_highest_matching_table_column_name} FROM {most_similar_table_names[0]}\"\n",
        "\n",
        "# Iterate through the list of possible queries and execute each one\n",
        "\n",
        "# Print the generated SQL query\n",
        "print(query)\n",
        "\n",
        "try:\n",
        "    cursor.execute(query)\n",
        "    results = cursor.fetchall()\n",
        "    print(f'Query: {query}')\n",
        "    print(f'Results: {results}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "\n",
        "\n",
        "# close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "J-PBOiR0rthY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7f06bf-d4a7-49e8-8694-5a1d8c492c18"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English Question: what are year of films?\n",
            "Table name: Movie, cosine similarity score: 0.4627804756164551\n",
            "Table name: Genre, cosine similarity score: 0.3473077416419983\n",
            "Table name: M_Genre, cosine similarity score: 0.2832810580730438\n",
            "Table name: M_Director, cosine similarity score: 0.2467322051525116\n",
            "Table name: Country, cosine similarity score: 0.23800981044769287\n",
            "Table name: Language, cosine similarity score: 0.19361281394958496\n",
            "Table name: M_Producer, cosine similarity score: 0.18729272484779358\n",
            "Table name: M_Country, cosine similarity score: 0.1826772689819336\n",
            "Table name: M_Cast, cosine similarity score: 0.17451390624046326\n",
            "Table name: Location, cosine similarity score: 0.12608277797698975\n",
            "Table name: M_Language, cosine similarity score: 0.08102075010538101\n",
            "Table name: Person, cosine similarity score: 0.072911337018013\n",
            "Table name: M_Location, cosine similarity score: 0.04325410723686218\n",
            "SELECT year FROM Movie\n",
            "Query: SELECT year FROM Movie\n",
            "Results: [('2018',), ('2018',), ('2018',), ('2012',), ('2018',), ('2018',), ('2016',), ('2018',), ('2016',), ('2018',), ('2017',), ('2008',), ('I 2009',), ('2016',), ('1977',), ('2018',), ('2012',), ('2013',), ('2015',), ('2018',), ('2018',), ('2018',), ('2018',), ('2007',), ('2018',), ('2002',), ('2016',), ('2018',), ('1951',), ('2009',), ('2018',), ('2014',), ('2004',), ('2009',), ('2007',), ('2015',), ('2014',), ('2014',), ('2012',), ('1997',), ('1983',), ('1994',), ('2018',), ('2017',), ('2018',), ('2018',), ('2018',), ('2014',), ('2008',), ('2011',), ('2004',), ('1996',), ('2018',), ('2018',), ('2017',), ('2001',), ('2018',), ('2010',), ('2018',), ('2018',), ('2018',), ('2011',), ('2006',), ('2018',), ('2013',), ('2015',), ('1971',), ('2017',), ('2018',), ('2018',), ('2007',), ('I 2018',), ('2018',), ('2018',), ('2018',), ('2018',), ('2008',), ('2014',), ('2013',), ('2013',), ('2017',), ('XVII 2016',), ('2018',), ('2012',), ('1958',), ('2017',), ('2015',), ('2017',), ('2004',), ('2018',), ('I 2017',), ('1984',), ('2018',), ('2001',), ('2015',), ('2017',), ('2018',), ('2017',), ('2016',), ('2017',), ('2018',), ('1987',), ('2018',), ('2018',), ('2015',), ('II 2018',), ('2015',), ('2012',), ('2017',), ('2012',), ('2017',), ('2014',), ('1996',), ('2015',), ('2015',), ('2017',), ('2012',), ('2014',), ('2016',), ('2017',), ('2009',), ('2005',), ('2018',), ('2016',), ('2006',), ('2016',), ('I 2002',), ('2017',), ('2016',), ('I 2018',), ('III 2016',), ('2013',), ('2015',), ('2008',), ('2003',), ('2018',), ('2018',), ('2013',), ('2017',), ('2008',), ('2013',), ('2012',), ('2016',), ('2017',), ('I 2018',), ('1995',), ('2009',), ('I 2018',), ('2006',), ('2017',), ('2003',), ('2016',), ('2018',), ('2017',), ('2016',), ('2015',), ('2001',), ('2017',), ('2012',), ('2015',), ('2016',), ('2011',), ('2007',), ('1998',), ('2007',), ('2016',), ('2017',), ('2014',), ('2013',), ('2017',), ('2018',), ('2004',), ('2011',), ('2015',), ('2008',), ('2017',), ('2011',), ('2017',), ('2018',), ('2011',), ('2008',), ('2016',), ('2012',), ('2017',), ('2013',), ('2013',), ('2016',), ('2014',), ('2004',), ('2018',), ('2017',), ('2012',), ('2018',), ('I 2015',), ('2018',), ('2018',), ('1975',), ('2017',), ('2018',), ('1939',), ('2018',), ('I 2018',), ('2007',), ('2013',), ('2013',), ('2004',), ('2009',), ('2015',), ('2006',), ('2006',), ('2015',), ('2016',), ('2017',), ('I 2018',), ('2015',), ('2017',), ('2002',), ('2016',), ('2018',), ('2003',), ('I 2015',), ('2014',), ('2017',), ('2000',), ('I 2015',), ('2017',), ('2016',), ('2017',), ('2013',), ('2018',), ('2014',), ('1988',), ('2016',), ('2017',), ('2007',), ('2006',), ('2003',), ('2013',), ('2008',), ('2013',), ('2018',), ('2008',), ('I 2016',), ('I 2016',), ('2018',), ('2011',), ('2016',), ('2012',), ('2018',), ('2016',), ('2011',), ('2009',), ('2011',), ('I 2006',), ('2014',), ('I 2010',), ('2015',), ('2014',), ('I 2013',), ('2013',), ('2010',), ('2000',), ('I 2014',), ('2006',), ('2015',), ('2011',), ('2009',), ('I 2018',), ('2016',), ('2012',), ('I 2018',), ('2013',), ('2012',), ('2017',), ('2001',), ('2012',), ('2011',), ('2015',), ('2017',), ('II 2017',), ('2012',), ('2016',), ('2018',), ('2004',), ('2012',), ('2015',), ('2004',), ('2015',), ('2012',), ('2016',), ('I 2014',), ('2002',), ('I 2016',), ('2008',), ('2017',), ('2015',), ('2015',), ('2006',), ('2014',), ('2012',), ('2013',), ('2004',), ('2007',), ('2017',), ('2009',), ('2018',), ('2017',), ('2018',), ('2003',), ('2016',), ('2000',), ('2013',), ('1996',), ('2007',), ('2016',), ('2009',), ('2017',), ('1998',), ('2014',), ('2008',), ('2013',), ('2006',), ('2011',), ('2014',), ('2000',), ('2016',), ('2007',), ('2013',), ('2013',), ('2012',), ('2002',), ('2006',), ('2013',), ('2010',), ('2017',), ('2016',), ('2009',), ('2010',), ('2017',), ('2016',), ('2000',), ('2016',), ('I 2018',), ('2005',), ('2015',), ('1996',), ('2016',), ('2005',), ('2018',), ('1996',), ('2015',), ('2015',), ('2016',), ('I 2016',), ('I 2014',), ('1994',), ('2010',), ('1997',), ('1991',), ('2016',), ('I 2015',), ('2016',), ('2010',), ('2017',), ('2016',), ('2010',), ('1993',), ('2016',), ('2015',), ('2015',), ('2012',), ('I 2016',), ('2005',), ('2012',), ('2017',), ('2017',), ('1981',), ('2017',), ('2002',), ('2016',), ('2008',), ('2010',), ('2014',), ('2014',), ('2011',), ('2018',), ('1989',), ('2018',), ('2017',), ('1976',), ('2017',), ('1998',), ('2017',), ('2017',), ('2017',), ('2007',), ('2017',), ('2005',), ('2015',), ('2015',), ('2009',), ('2005',), ('2010',), ('1959',), ('2015',), ('2014',), ('2017',), ('1970',), ('2008',), ('2017',), ('2007',), ('2018',), ('VI 2015',), ('2011',), ('2010',), ('2010',), ('2011',), ('2016',), ('2012',), ('2011',), ('2010',), ('2014',), ('1979',), ('2015',), ('I 1964',), ('2018',), ('2007',), ('1993',), ('2014',), ('2018',), ('2013',), ('2016',), ('2000',), ('2009',), ('2013',), ('2007',), ('2014',), ('2015',), ('2012',), ('2015',), ('2004',), ('2014',), ('2017',), ('2014',), ('2001',), ('2017',), ('2016',), ('2016',), ('2012',), ('1994',), ('2005',), ('2005',), ('2004',), ('2009',), ('2013',), ('1999',), ('2015',), ('2018',), ('2012',), ('2010',), ('2017',), ('I 2003',), ('2013',), ('2015',), ('2011',), ('2018',), ('2014',), ('2017',), ('I 2014',), ('2017',), ('2015',), ('2008',), ('I 2014',), ('2016',), ('2013',), ('1999',), ('2016',), ('I 2017',), ('2001',), ('2014',), ('2004',), ('2014',), ('2014',), ('III 2017',), ('2018',), ('I 2005',), ('2002',), ('2016',), ('2018',), ('I 2013',), ('2007',), ('2017',), ('2017',), ('I 2008',), ('2018',), ('2018',), ('2003',), ('2018',), ('2010',), ('1999',), ('2011',), ('1995',), ('2016',), ('I 2016',), ('2000',), ('2015',), ('2003',), ('2016',), ('2007',), ('2014',), ('2017',), ('2014',), ('2014',), ('1983',), ('2017',), ('1990',), ('2017',), ('1992',), ('1993',), ('2013',), ('2015',), ('2013',), ('2009',), ('1992',), ('2007',), ('2010',), ('2006',), ('2014',), ('2002',), ('1959',), ('I 2007',), ('2005',), ('2010',), ('2017',), ('2005',), ('2009',), ('2007',), ('I 2013',), ('2005',), ('1996',), ('2013',), ('1997',), ('2004',), ('1995',), ('2012',), ('2014',), ('2013',), ('2015',), ('2016',), ('2012',), ('2013',), ('2006',), ('2004',), ('2013',), ('2013',), ('2011',), ('2010',), ('2011',), ('2008',), ('2015',), ('2012',), ('2011',), ('2005',), ('2013',), ('2001',), ('1975',), ('2014',), ('2016',), ('2010',), ('2005',), ('2007',), ('2012',), ('2002',), ('1990',), ('2006',), ('2010',), ('2012',), ('2016',), ('2006',), ('1994',), ('2012',), ('2013',), ('2010',), ('I 2011',), ('I 2013',), ('2010',), ('2013',), ('2014',), ('2017',), ('2009',), ('2003',), ('2017',), ('2010',), ('2010',), ('1995',), ('1983',), ('2005',), ('2014',), ('2014',), ('2009',), ('2005',), ('2010',), ('1987',), ('2015',), ('2015',), ('2016',), ('2016',), ('2010',), ('2012',), ('2006',), ('2009',), ('1991',), ('1998',), ('1957',), ('2013',), ('2016',), ('2015',), ('1999',), ('2001',), ('2005',), ('2011',), ('2008',), ('1999',), ('1999',), ('I 2009',), ('2001',), ('2012',), ('2016',), ('2007',), ('2008',), ('2018',), ('2017',), ('2012',), ('1996',), ('2011',), ('2008',), ('2007',), ('2013',), ('2002',), ('2005',), ('1994',), ('2014',), ('1995',), ('2005',), ('2016',), ('2012',), ('2015',), ('1994',), ('2013',), ('2017',), ('2008',), ('2015',), ('2002',), ('1980',), ('2014',), ('1989',), ('1966',), ('2015',), ('2006',), ('2013',), ('1967',), ('1973',), ('1970',), ('2013',), ('1998',), ('1994',), ('2003',), ('1999',), ('1994',), ('1992',), ('1988',), ('1993',), ('2001',), ('2012',), ('1995',), ('2007',), ('2014',), ('2007',), ('2011',), ('1995',), ('1991',), ('1970',), ('1990',), ('2010',), ('2006',), ('2008',), ('2016',), ('1998',), ('1994',), ('2002',), ('I 2010',), ('2003',), ('2008',), ('1996',), ('2014',), ('2004',), ('1975',), ('2005',), ('2016',), ('2008',), ('2006',), ('I 1997',), ('1993',), ('2008',), ('2015',), ('2003',), ('1991',), ('1994',), ('2009',), ('1994',), ('1981',), ('1968',), ('1980',), ('2017',), ('1999',), ('1969',), ('2014',), ('2002',), ('2018',), ('1957',), ('2013',), ('1982',), ('1993',), ('I 2016',), ('2004',), ('2009',), ('III 2015',), ('1978',), ('1965',), ('2008',), ('1975',), ('2003',), ('1973',), ('2003',), ('1988',), ('2009',), ('1992',), ('2015',), ('2008',), ('2016',), ('2000',), ('1994',), ('2018',), ('2011',), ('1973',), ('2001',), ('2015',), ('2013',), ('2013',), ('1995',), ('1988',), ('2012',), ('1987',), ('1965',), ('2010',), ('1982',), ('2013',), ('1972',), ('1991',), ('1998',), ('2017',), ('2009',), ('2007',), ('1956',), ('2006',), ('2016',), ('2013',), ('2016',), ('1996',), ('2000',), ('2014',), ('2007',), ('1996',), ('1982',), ('2008',), ('2002',), ('2017',), ('1971',), ('2013',), ('2008',), ('2011',), ('1982',), ('1974',), ('1992',), ('2017',), ('2014',), ('1995',), ('2007',), ('1960',), ('2003',), ('2013',), ('1977',), ('2014',), ('1987',), ('2017',), ('1996',), ('2011',), ('1965',), ('2015',), ('2014',), ('2008',), ('1978',), ('2017',), ('2010',), ('1971',), ('2004',), ('1998',), ('2005',), ('I 2016',), ('2008',), ('1999',), ('1985',), ('1975',), ('2000',), ('1982',), ('2011',), ('1999',), ('2016',), ('2005',), ('I 2003',), ('1981',), ('2011',), ('2000',), ('2014',), ('1974',), ('2016',), ('1996',), ('1981',), ('2005',), ('1991',), ('2006',), ('1993',), ('II 2012',), ('2010',), ('1993',), ('1972',), ('2004',), ('2000',), ('III 2015',), ('1986',), ('2005',), ('1982',), ('1949',), ('1970',), ('2005',), ('2001',), ('2000',), ('2010',), ('2013',), ('1998',), ('1993',), ('2002',), ('2002',), ('1991',), ('1976',), ('1992',), ('2011',), ('2005',), ('2002',), ('2017',), ('1985',), ('2012',), ('1991',), ('1992',), ('1993',), ('2016',), ('2012',), ('2007',), ('1994',), ('2017',), ('2013',), ('1979',), ('1964',), ('2000',), ('2007',), ('1992',), ('III 2007',), ('1995',), ('1970',), ('2003',), ('1994',), ('2011',), ('1994',), ('2001',), ('2016',), ('2013',), ('2009',), ('2003',), ('2016',), ('2017',), ('1974',), ('1998',), ('1982',), ('2010',), ('2013',), ('2011',), ('2014',), ('2008',), ('2001',), ('1997',), ('2013',), ('2005',), ('2007',), ('2012',), ('2005',), ('2010',), ('1982',), ('1995',), ('1989',), ('2007',), ('2010',), ('2012',), ('1998',), ('2012',), ('1977',), ('2003',), ('2002',), ('2010',), ('2004',), ('2005',), ('1993',), ('2004',), ('I 2014',), ('2017',), ('1977',), ('1999',), ('1983',), ('2007',), ('1991',), ('2009',), ('2002',), ('1966',), ('2009',), ('2006',), ('2014',), ('1997',), ('1998',), ('1999',), ('2009',), ('1997',), ('2003',), ('2017',), ('1973',), ('2001',), ('2005',), ('1995',), ('2003',), ('1992',), ('2010',), ('2006',), ('2009',), ('1998',), ('2002',), ('2014',), ('1993',), ('2015',), ('2011',), ('2013',), ('1978',), ('2012',), ('2017',), ('2003',), ('2017',), ('1991',), ('2003',), ('1967',), ('2018',), ('1967',), ('2016',), ('2010',), ('1976',), ('2013',), ('2013',), ('2004',), ('1982',), ('2002',), ('1996',), ('1997',), ('1994',), ('2014',), ('2003',), ('2003',), ('1964',), ('1993',), ('1969',), ('2007',), ('1981',), ('2015',), ('2015',), ('2011',), ('2015',), ('1978',), ('1974',), ('2014',), ('2007',), ('2016',), ('2000',), ('1974',), ('1976',), ('2014',), ('1997',), ('1992',), ('2002',), ('2018',), ('2014',), ('1999',), ('2017',), ('2008',), ('1982',), ('1983',), ('1997',), ('2006',), ('1988',), ('2014',), ('2010',), ('I 2011',), ('1977',), ('2017',), ('2012',), ('2009',), ('1973',), ('1971',), ('1976',), ('1985',), ('2004',), ('1993',), ('2005',), ('1990',), ('1997',), ('2006',), ('2009',), ('1955',), ('2003',), ('2002',), ('1987',), ('2003',), ('1986',), ('2014',), ('1958',), ('2007',), ('2013',), ('1966',), ('1966',), ('2003',), ('2010',), ('2012',), ('2017',), ('1993',), ('1971',), ('1999',), ('2010',), ('1949',), ('1965',), ('1998',), ('1976',), ('2016',), ('2000',), ('2000',), ('2010',), ('2018',), ('2010',), ('1989',), ('1988',), ('2010',), ('2000',), ('2003',), ('2010',), ('1964',), ('1996',), ('2011',), ('2009',), ('2015',), ('2016',), ('2001',), ('2010',), ('1992',), ('2014',), ('2014',), ('2004',), ('1967',), ('II 2010',), ('2004',), ('1972',), ('1956',), ('1972',), ('1987',), ('1961',), ('2009',), ('2018',), ('2002',), ('2012',), ('1997',), ('1997',), ('2013',), ('2014',), ('I 2009',), ('2010',), ('2006',), ('2003',), ('2009',), ('1998',), ('2003',), ('2013',), ('2014',), ('2015',), ('2000',), ('2012',), ('2016',), ('2013',), ('1992',), ('2006',), ('1962',), ('1988',), ('2015',), ('2016',), ('2004',), ('2000',), ('2003',), ('1998',), ('1992',), ('2015',), ('2018',), ('2008',), ('2002',), ('1993',), ('1969',), ('1983',), ('1995',), ('1966',), ('2004',), ('2007',), ('1995',), ('2004',), ('2016',), ('2017',), ('1985',), ('2015',), ('2008',), ('1997',), ('2002',), ('2010',), ('1958',), ('2011',), ('I 2013',), ('1997',), ('2007',), ('2008',), ('2016',), ('2015',), ('2017',), ('2013',), ('1989',), ('2010',), ('2003',), ('1964',), ('1965',), ('1988',), ('2010',), ('2004',), ('2014',), ('2010',), ('2009',), ('1994',), ('2015',), ('1990',), ('2007',), ('1994',), ('2011',), ('1998',), ('2009',), ('2014',), ('1999',), ('1982',), ('1998',), ('2006',), ('2007',), ('2003',), ('2014',), ('2013',), ('II 2013',), ('1993',), ('2005',), ('1975',), ('2016',), ('1987',), ('1991',), ('2000',), ('I 2013',), ('2001',), ('1962',), ('2011',), ('1966',), ('2017',), ('2013',), ('2015',), ('2011',), ('2013',), ('2005',), ('2012',), ('2007',), ('2007',), ('1995',), ('2001',), ('1990',), ('2013',), ('2017',), ('1980',), ('2004',), ('1991',), ('2006',), ('2011',), ('2008',), ('2003',), ('1980',), ('2001',), ('1986',), ('2005',), ('2010',), ('1984',), ('1984',), ('1995',), ('2014',), ('I 2008',), ('1989',), ('1975',), ('1986',), ('2008',), ('2007',), ('2012',), ('2015',), ('1997',), ('1995',), ('2017',), ('2004',), ('I 1986',), ('2006',), ('1992',), ('2002',), ('1985',), ('2002',), ('1999',), ('2006',), ('1988',), ('I 1968',), ('2014',), ('1971',), ('1999',), ('2016',), ('2017',), ('2012',), ('2001',), ('2001',), ('1954',), ('I 2017',), ('1993',), ('2009',), ('2016',), ('2016',), ('1999',), ('2004',), ('2016',), ('1993',), ('2004',), ('2014',), ('1993',), ('2013',), ('2005',), ('2011',), ('1999',), ('2003',), ('2014',), ('2000',), ('1982',), ('2012',), ('2014',), ('2003',), ('1990',), ('2010',), ('1986',), ('1994',), ('2011',), ('2015',), ('2004',), ('1995',), ('2017',), ('2009',), ('2001',), ('2004',), ('2010',), ('2004',), ('1999',), ('1999',), ('1997',), ('1998',), ('2006',), ('2007',), ('2001',), ('1986',), ('I 1980',), ('1998',), ('2014',), ('1986',), ('2008',), ('2017',), ('2000',), ('1985',), ('1967',), ('2000',), ('1984',), ('2006',), ('2017',), ('2012',), ('1994',), ('1987',), ('2003',), ('2009',), ('1998',), ('1989',), ('2005',), ('1998',), ('1993',), ('2002',), ('2016',), ('1993',), ('I 2009',), ('1993',), ('1989',), ('2016',), ('2014',), ('1951',), ('2002',), ('2006',), ('2003',), ('2014',), ('2007',), ('2006',), ('2001',), ('2003',), ('1978',), ('1979',), ('1980',), ('2005',), ('1991',), ('2018',), ('1982',), ('2010',), ('1988',), ('2001',), ('2013',), ('2014',), ('2002',), ('1982',), ('2013',), ('1966',), ('1974',), ('1982',), ('1996',), ('2016',), ('2015',), ('2014',), ('2007',), ('1994',), ('2009',), ('1960',), ('2005',), ('2005',), ('2005',), ('1999',), ('1991',), ('1975',), ('1966',), ('2006',), ('1970',), ('2016',), ('2017',), ('2005',), ('2009',), ('2008',), ('1989',), ('1981',), ('2002',), ('1999',), ('1990',), ('1981',), ('2008',), ('1941',), ('1996',), ('2014',), ('2005',), ('2006',), ('2011',), ('1996',), ('2013',), ('1993',), ('1999',), ('2007',), ('2015',), ('1986',), ('2009',), ('1972',), ('2002',), ('1995',), ('1981',), ('2014',), ('2017',), ('1995',), ('1989',), ('2002',), ('1977',), ('1998',), ('1993',), ('1991',), ('2005',), ('2009',), ('2005',), ('1996',), ('2015',), ('1992',), ('2011',), ('2010',), ('1986',), ('1995',), ('2002',), ('2009',), ('2004',), ('1988',), ('1998',), ('2001',), ('2006',), ('2012',), ('1960',), ('2014',), ('2017',), ('1980',), ('2013',), ('2016',), ('2012',), ('2000',), ('1956',), ('2004',), ('1999',), ('1993',), ('1996',), ('1975',), ('2002',), ('1983',), ('2018',), ('2015',), ('1970',), ('2016',), ('1957',), ('1996',), ('2001',), ('1999',), ('2006',), ('1974',), ('2011',), ('2010',), ('1994',), ('1999',), ('2013',), ('2013',), ('1951',), ('1994',), ('2014',), ('2012',), ('1994',), ('2013',), ('1997',), ('1980',), ('2000',), ('1972',), ('1987',), ('1983',), ('2018',), ('1993',), ('1987',), ('2013',), ('1984',), ('1990',), ('2014',), ('2018',), ('2001',), ('I 2007',), ('1996',), ('1999',), ('2006',), ('2010',), ('2002',), ('1994',), ('II 2008',), ('I 1983',), ('1982',), ('2015',), ('2013',), ('2004',), ('2007',), ('2012',), ('1992',), ('1999',), ('1988',), ('2017',), ('I 2010',), ('1993',), ('2002',), ('2012',), ('1993',), ('2007',), ('2003',), ('2008',), ('I 2015',), ('1962',), ('2008',), ('2017',), ('2014',), ('2009',), ('2013',), ('2017',), ('2011',), ('1986',), ('1983',), ('1990',), ('1988',), ('2007',), ('2003',), ('1987',), ('1968',), ('2015',), ('1990',), ('1984',), ('1985',), ('2014',), ('2014',), ('1976',), ('2013',), ('2008',), ('2005',), ('2014',), ('2014',), ('2017',), ('1983',), ('1990',), ('2001',), ('2006',), ('2015',), ('2017',), ('2003',), ('2001',), ('2005',), ('V 2015',), ('1994',), ('2005',), ('1999',), ('1978',), ('2003',), ('2002',), ('1982',), ('2013',), ('2012',), ('1995',), ('1996',), ('2004',), ('2001',), ('1990',), ('1972',), ('2016',), ('1998',), ('2007',), ('2017',), ('1988',), ('2009',), ('1994',), ('2004',), ('1989',), ('2002',), ('2001',), ('2000',), ('2018',), ('2005',), ('1983',), ('1993',), ('1999',), ('2016',), ('I 2015',), ('1982',), ('2007',), ('2009',), ('2008',), ('2015',), ('2004',), ('2004',), ('2013',), ('2001',), ('I 2002',), ('1972',), ('1992',), ('2003',), ('1990',), ('2014',), ('2009',), ('2014',), ('2002',), ('2011',), ('1992',), ('2002',), ('2006',), ('2004',), ('2003',), ('2004',), ('2013',), ('1966',), ('1951',), ('1992',), ('2005',), ('1987',), ('2004',), ('1988',), ('1983',), ('1982',), ('2013',), ('1980',), ('1983',), ('2006',), ('1995',), ('2011',), ('2005',), ('2004',), ('1983',), ('2007',), ('1994',), ('1990',), ('1963',), ('1983',), ('1974',), ('2007',), ('1988',), ('2013',), ('1974',), ('2015',), ('2017',), ('2001',), ('2005',), ('2004',), ('2002',), ('2010',), ('1978',), ('1994',), ('2011',), ('2008',), ('1962',), ('2006',), ('1993',), ('1996',), ('2005',), ('2003',), ('1990',), ('2009',), ('2005',), ('2010',), ('1989',), ('1994',), ('1976',), ('2015',), ('I 2009',), ('2015',), ('1992',), ('2001',), ('1984',), ('2016',), ('1998',), ('1976',), ('2007',), ('2000',), ('1971',), ('1987',), ('2016',), ('1972',), ('1994',), ('2002',), ('1991',), ('2003',), ('1997',), ('2006',), ('2007',), ('1973',), ('1995',), ('2004',), ('2007',), ('1991',), ('1998',), ('1965',), ('2008',), ('1971',), ('2010',), ('1993',), ('2012',), ('2000',), ('2016',), ('2003',), ('1975',), ('1965',), ('2005',), ('1974',), ('1980',), ('1968',), ('1970',), ('1971',), ('1994',), ('1981',), ('1931',), ('2005',), ('1971',), ('2006',), ('2005',), ('1997',), ('1976',), ('2017',), ('2013',), ('2014',), ('2001',), ('I 2017',), ('1997',), ('1986',), ('1980',), ('2001',), ('1961',), ('1963',), ('2017',), ('1967',), ('2001',), ('2011',), ('1973',), ('2007',), ('2018',), ('2008',), ('1975',), ('2016',), ('2010',), ('1970',), ('2005',), ('2005',), ('2011',), ('2000',), ('1997',), ('1980',), ('1977',), ('1998',), ('1966',), ('1969',), ('1983',), ('2013',), ('1996',), ('1990',), ('2006',), ('1989',), ('2007',), ('2000',), ('1955',), ('2015',), ('1972',), ('1993',), ('2002',), ('1961',), ('1993',), ('2017',), ('2005',), ('2010',), ('2016',), ('2000',), ('1969',), ('2003',), ('1973',), ('1971',), ('2014',), ('2010',), ('1994',), ('1995',), ('1985',), ('2016',), ('2005',), ('1991',), ('2002',), ('2013',), ('2014',), ('1969',), ('1985',), ('1985',), ('1967',), ('2003',), ('2013',), ('1993',), ('1980',), ('2000',), ('1953',), ('2006',), ('1998',), ('1981',), ('1980',), ('2003',), ('1969',), ('1992',), ('2008',), ('2010',), ('2009',), ('1993',), ('1949',), ('2015',), ('2011',), ('2017',), ('2008',), ('1996',), ('2011',), ('2018',), ('1967',), ('1972',), ('1999',), ('2001',), ('1968',), ('1986',), ('1993',), ('2015',), ('1979',), ('2001',), ('2011',), ('2009',), ('1970',), ('1979',), ('2009',), ('2000',), ('1964',), ('1972',), ('2012',), ('2012',), ('2012',), ('1959',), ('1989',), ('2005',), ('1981',), ('1965',), ('2008',), ('2014',), ('2012',), ('2014',), ('2004',), ('2003',), ('1996',), ('2006',), ('1973',), ('1972',), ('1999',), ('1995',), ('I 2001',), ('1991',), ('1995',), ('2005',), ('2015',), ('2008',), ('1983',), ('2004',), ('2014',), ('1996',), ('1974',), ('1985',), ('2015',), ('2005',), ('1996',), ('1988',), ('1990',), ('2011',), ('2002',), ('2003',), ('2005',), ('2016',), ('2013',), ('2003',), ('2013',), ('1980',), ('1974',), ('1979',), ('1995',), ('1978',), ('1999',), ('1986',), ('2005',), ('1971',), ('1995',), ('2004',), ('1983',), ('2005',), ('1969',), ('2003',), ('2005',), ('2008',), ('1981',), ('2007',), ('1988',), ('1995',), ('1997',), ('2003',), ('2010',), ('1989',), ('1970',), ('1954',), ('2002',), ('2007',), ('1958',), ('2004',), ('2002',), ('1976',), ('2001',), ('1991',), ('2013',), ('1988',), ('1998',), ('1992',), ('1999',), ('2007',), ('1987',), ('1963',), ('2003',), ('2004',), ('1970',), ('1999',), ('2002',), ('1976',), ('1986',), ('1999',), ('1993',), ('2000',), ('1997',), ('1970',), ('1998',), ('1989',), ('1990',), ('1989',), ('2013',), ('2013',), ('1996',), ('1985',), ('1999',), ('1993',), ('2002',), ('2018',), ('2015',), ('1955',), ('1977',), ('2004',), ('1991',), ('1986',), ('2009',), ('1958',), ('2004',), ('2013',), ('2014',), ('2010',), ('1971',), ('2011',), ('2009',), ('2008',), ('1965',), ('1976',), ('2008',), ('2014',), ('2005',), ('2005',), ('1989',), ('2010',), ('1998',), ('1988',), ('2009',), ('2016',), ('1993',), ('2013',), ('2006',), ('1977',), ('1996',), ('1990',), ('1960',), ('1981',), ('2005',), ('2001',), ('2004',), ('1977',), ('1979',), ('1988',), ('2012',), ('1978',), ('1996',), ('1999',), ('III 2007',), ('1969',), ('1953',), ('1992',), ('2014',), ('2006',), ('1987',), ('2003',), ('1975',), ('1999',), ('2006',), ('2012',), ('1996',), ('2014',), ('2007',), ('1977',), ('2015',), ('1996',), ('I 2010',), ('1984',), ('2005',), ('1992',), ('2011',), ('I 1989',), ('2015',), ('2005',), ('2013',), ('2005',), ('2005',), ('1964',), ('1993',), ('2004',), ('2017',), ('2013',), ('2003',), ('2004',), ('2011',), ('2010',), ('2005',), ('I 2013',), ('2006',), ('1992',), ('1960',), ('1948',), ('1997',), ('2006',), ('1995',), ('2008',), ('2003',), ('1952',), ('1983',), ('2002',), ('1976',), ('1964',), ('1990',), ('2000',), ('1978',), ('1957',), ('2017',), ('2014',), ('1982',), ('2007',), ('1993',), ('2006',), ('2010',), ('1992',), ('1988',), ('2009',), ('1963',), ('1953',), ('1966',), ('2013',), ('2014',), ('2009',), ('1954',), ('1957',), ('1983',), ('1999',), ('1972',), ('2012',), ('2003',), ('1980',), ('1978',), ('II 1998',), ('1977',), ('2014',), ('1989',), ('2003',), ('1971',), ('1968',), ('1994',), ('1990',), ('2013',), ('1965',), ('1997',), ('1984',), ('1993',), ('2010',), ('2002',), ('2001',), ('1952',), ('2006',), ('1961',), ('1960',), ('2015',), ('1997',), ('1973',), ('1999',), ('1982',), ('2005',), ('II 2013',), ('1978',), ('1990',), ('2007',), ('2009',), ('1955',), ('1989',), ('1999',), ('1996',), ('1968',), ('2004',), ('2002',), ('1995',), ('1969',), ('2006',), ('2008',), ('1970',), ('1968',), ('1984',), ('1979',), ('2004',), ('1996',), ('1975',), ('1972',), ('1974',), ('2004',), ('2005',), ('1968',), ('I 2018',), ('1977',), ('1962',), ('1999',), ('2011',), ('2016',), ('2007',), ('2016',), ('2008',), ('2018',), ('2010',), ('2016',), ('2013',), ('1996',), ('2003',), ('2006',), ('2004',), ('2007',), ('I 2014',), ('1989',), ('2012',), ('1995',), ('1976',), ('1972',), ('1977',), ('1984',), ('1995',), ('2014',), ('2008',), ('2007',), ('1953',), ('2005',), ('2010',), ('1981',), ('1996',), ('1977',), ('1981',), ('1994',), ('2011',), ('2010',), ('1955',), ('2010',), ('2009',), ('2009',), ('1992',), ('2010',), ('1997',), ('2000',), ('2009',), ('2013',), ('2015',), ('1997',), ('2008',), ('2012',), ('1992',), ('2003',), ('1968',), ('2000',), ('2004',), ('2017',), ('1997',), ('1985',), ('2000',), ('2002',), ('1994',), ('2003',), ('1992',), ('2010',), ('2015',), ('1962',), ('1995',), ('1964',), ('1977',), ('2000',), ('2014',), ('1996',), ('2002',), ('2008',), ('2004',), ('2005',), ('2005',), ('1997',), ('2016',), ('1967',), ('1971',), ('1967',), ('1971',), ('2008',), ('1995',), ('1985',), ('2005',), ('2014',), ('2011',), ('1984',), ('2011',), ('2005',), ('2016',), ('1990',), ('2007',), ('2010',), ('1980',), ('2016',), ('1998',), ('2008',), ('2001',), ('1952',), ('2004',), ('1978',), ('1996',), ('1981',), ('1952',), ('1980',), ('1994',), ('2003',), ('2012',), ('2005',), ('1998',), ('2010',), ('2009',), ('1997',), ('2005',), ('2001',), ('2012',), ('2005',), ('1994',), ('2006',), ('1995',), ('2005',), ('2007',), ('2016',), ('2006',), ('2008',), ('1972',), ('1954',), ('2008',), ('1981',), ('2002',), ('2016',), ('1979',), ('1962',), ('2011',), ('1978',), ('1999',), ('2004',), ('2015',), ('1974',), ('1978',), ('1986',), ('1953',), ('2006',), ('2012',), ('1972',), ('2011',), ('2010',), ('2007',), ('1984',), ('1977',), ('1994',), ('1988',), ('1996',), ('2006',), ('1964',), ('1980',), ('1982',), ('1983',), ('2012',), ('1981',), ('2010',), ('2007',), ('2004',), ('1987',), ('1983',), ('1973',), ('2005',), ('2000',), ('2009',), ('1999',), ('2015',), ('2006',), ('2015',), ('2008',), ('2012',), ('1973',), ('1994',), ('1966',), ('1985',), ('2003',), ('1974',), ('1960',), ('2014',), ('2012',), ('1992',), ('1986',), ('1998',), ('1973',), ('2006',), ('1973',), ('1990',), ('2015',), ('1994',), ('1999',), ('1988',), ('1970',), ('2001',), ('1976',), ('1999',), ('2000',), ('2011',), ('1970',), ('2010',), ('2014',), ('2006',), ('1986',), ('2008',), ('1997',), ('1954',), ('1973',), ('1990',), ('1973',), ('2010',), ('2016',), ('2000',), ('2003',), ('1989',), ('1967',), ('1993',), ('1974',), ('2006',), ('2007',), ('2004',), ('I 2010',), ('2007',), ('2000',), ('1984',), ('2012',), ('1987',), ('2011',), ('1996',), ('1991',), ('2006',), ('2009',), ('2001',), ('2011',), ('2005',), ('1989',), ('2009',), ('2013',), ('1947',), ('1975',), ('2012',), ('1994',), ('1989',), ('2015',), ('1953',), ('2008',), ('1999',), ('1959',), ('1968',), ('2007',), ('1993',), ('2010',), ('2010',), ('I 1992',), ('2013',), ('2005',), ('1982',), ('1988',), ('1969',), ('I 2008',), ('1977',), ('1999',), ('1987',), ('I 2009',), ('1977',), ('2006',), ('1951',), ('2012',), ('2010',), ('1997',), ('1971',), ('1998',), ('1972',), ('1989',), ('2005',), ('1967',), ('2008',), ('2006',), ('2012',), ('2006',), ('1981',), ('1978',), ('2001',), ('1973',), ('1979',), ('2003',), ('2006',), ('1984',), ('2005',), ('1982',), ('1992',), ('1987',), ('2008',), ('1983',), ('2012',), ('1989',), ('2015',), ('1991',), ('2006',), ('1991',), ('2009',), ('1973',), ('I 2012',), ('2004',), ('2017',), ('2013',), ('2000',), ('2012',), ('2010',), ('2008',), ('2011',), ('2010',), ('1998',), ('1995',), ('1975',), ('2003',), ('2011',), ('1993',), ('2008',), ('1988',), ('2017',), ('1981',), ('2008',), ('2016',), ('I 2009',), ('2002',), ('2008',), ('I 2017',), ('2003',), ('1970',), ('1992',), ('1986',), ('1966',), ('1978',), ('1994',), ('2007',), ('1981',), ('2010',), ('2004',), ('2008',), ('1975',), ('2003',), ('1997',), ('2006',), ('1985',), ('2008',), ('2014',), ('2016',), ('2001',), ('1994',), ('1983',), ('1980',), ('2012',), ('2002',), ('1936',), ('1966',), ('1976',), ('1963',), ('1968',), ('1987',), ('1972',), ('2010',), ('1983',), ('2006',), ('1991',), ('I 1986',), ('2012',), ('2016',), ('1960',), ('2015',), ('1980',), ('1994',), ('1990',), ('2005',), ('1983',), ('2016',), ('1987',), ('1989',), ('2001',), ('1993',), ('1971',), ('1982',), ('2004',), ('1993',), ('1969',), ('1981',), ('1990',), ('1996',), ('2011',), ('2002',), ('1997',), ('2002',), ('1991',), ('2002',), ('2002',), ('1971',), ('2013',), ('1959',), ('1996',), ('2010',), ('1984',), ('1989',), ('1989',), ('2013',), ('2008',), ('1985',), ('2011',), ('1966',), ('1957',), ('1988',), ('1992',), ('1997',), ('1989',), ('1983',), ('2010',), ('1980',), ('1968',), ('2012',), ('1998',), ('1980',), ('2009',), ('2007',), ('2014',), ('1952',), ('1988',), ('1960',), ('1997',), ('1974',), ('1972',), ('1989',), ('2018',), ('1991',), ('1999',), ('1996',), ('1989',), ('1985',), ('I 1996',), ('1985',), ('1997',), ('1955',), ('2013',), ('1986',), ('2010',), ('2016',), ('2011',), ('1998',), ('2010',), ('2011',), ('1991',), ('2002',), ('2006',), ('2011',), ('2005',), ('2011',), ('1964',), ('1961',), ('1962',), ('2017',), ('I 2016',), ('1977',), ('1999',), ('1979',), ('1984',), ('1966',), ('1982',), ('2004',), ('1973',), ('2001',), ('1996',), ('1998',), ('2016',), ('2012',), ('1946',), ('1986',), ('1956',), ('2012',), ('2016',), ('2011',), ('1979',), ('2007',), ('1990',), ('1972',), ('2001',), ('1999',), ('2011',), ('1958',), ('2009',), ('1967',), ('1981',), ('2016',), ('2010',), ('2000',), ('2010',), ('I 2011',), ('1994',), ('1967',), ('2014',), ('2001',), ('1987',), ('1992',), ('1998',), ('2007',), ('1994',), ('2002',), ('2008',), ('1963',), ('2007',), ('2005',), ('2004',), ('1992',), ('2015',), ('2003',), ('2007',), ('1958',), ('1953',), ('2010',), ('2012',), ('1979',), ('2002',), ('2012',), ('1994',), ('2009',), ('2006',), ('1982',), ('2005',), ('1996',), ('1994',), ('2000',), ('2006',), ('2008',), ('2011',), ('1987',), ('1972',), ('2013',), ('1984',), ('1959',), ('1973',), ('2012',), ('2006',), ('1994',), ('1967',), ('2006',), ('2011',), ('1985',), ('2001',), ('1989',), ('1999',), ('1969',), ('1984',), ('1993',), ('2000',), ('2006',), ('1960',), ('2005',), ('1974',), ('2004',), ('1996',), ('2002',), ('1998',), ('2008',), ('1964',), ('2012',), ('1962',), ('2015',), ('2009',), ('1998',), ('1982',), ('2003',), ('1970',), ('1983',), ('2016',), ('1973',), ('2011',), ('2004',), ('2004',), ('1969',), ('2007',), ('1968',), ('1962',), ('1997',), ('1989',), ('1993',), ('2000',), ('1996',), ('2007',), ('1987',), ('1977',), ('1981',), ('1986',), ('1997',), ('1989',), ('2003',), ('1992',), ('2005',), ('1979',), ('1990',), ('1975',), ('2003',), ('1980',), ('1980',), ('2001',), ('1975',), ('2002',), ('1995',), ('2016',), ('1986',), ('2012',), ('1980',), ('2005',), ('2003',), ('2000',), ('1964',), ('2013',), ('2009',), ('1965',), ('1972',), ('1988',), ('2006',), ('1979',), ('2013',), ('2006',), ('1968',), ('2008',), ('2008',), ('1985',), ('2008',), ('2013',), ('1984',), ('2001',), ('1981',), ('1997',), ('2015',), ('1965',), ('1963',), ('2002',), ('2004',), ('2003',), ('1979',), ('1981',), ('1989',), ('2014',), ('2010',), ('2000',), ('1992',), ('2001',), ('1960',), ('1999',), ('2012',), ('2005',), ('1977',), ('1969',), ('2003',), ('2003',), ('2010',), ('1994',), ('2004',), ('2008',), ('1962',), ('1986',), ('1971',), ('1984',), ('1999',), ('1988',), ('2010',), ('1974',), ('2013',), ('1975',), ('1981',), ('1964',), ('1971',), ('1969',), ('2008',), ('1973',), ('2000',), ('1986',), ('2012',), ('1980',), ('2001',), ('1989',), ('1990',), ('2007',), ('2009',), ('1994',), ('2013',), ('1961',), ('1957',), ('1992',), ('1965',), ('1982',), ('2017',), ('1957',), ('2005',), ('2009',), ('2011',), ('1955',), ('2009',), ('1985',), ('2013',), ('1978',), ('1996',), ('1976',), ('2011',), ('1948',), ('2015',), ('IV 2011',), ('2009',), ('1971',), ('1977',), ('I 2011',), ('2014',), ('1967',), ('2009',), ('2003',), ('1991',), ('1992',), ('II 1983',), ('1998',), ('1995',), ('1997',), ('2009',), ('1993',), ('2008',), ('2006',), ('1957',), ('2005',), ('2007',), ('1975',), ('1995',), ('1974',), ('2014',), ('2005',), ('1972',), ('2006',), ('1990',), ('1972',), ('2005',), ('2008',), ('2002',), ('2004',), ('1960',), ('1974',), ('1996',), ('1998',), ('1951',), ('1998',), ('2004',), ('2005',), ('1974',), ('2009',), ('2008',), ('1997',), ('1957',), ('2013',), ('1998',), ('1995',), ('1964',), ('1967',), ('2016',), ('1999',), ('1992',), ('1978',), ('1968',), ('2010',), ('2011',), ('2000',), ('2004',), ('2004',), ('2009',), ('2011',), ('1992',), ('2006',), ('IV 2010',), ('1995',), ('2011',), ('2003',), ('2010',), ('2002',), ('1984',), ('2003',), ('1983',), ('1995',), ('1992',), ('2010',), ('2005',), ('2012',), ('2008',), ('2007',), ('2014',), ('2002',), ('I 2001',), ('1988',), ('1979',), ('2003',), ('2010',), ('1970',), ('2003',), ('1979',), ('I 2009',), ('1982',), ('1973',), ('1978',), ('II 2011',), ('1991',), ('2015',), ('1994',), ('2016',), ('2013',), ('2000',), ('1970',), ('1993',), ('1987',), ('2004',), ('2011',), ('1985',), ('1998',), ('1987',), ('1987',), ('1981',), ('1997',), ('1997',), ('1982',), ('2005',), ('2003',), ('2003',), ('1973',), ('2006',), ('1981',), ('1986',), ('2004',), ('2005',), ('2014',), ('2015',), ('1995',), ('1991',), ('1984',), ('1971',), ('1997',), ('1977',), ('1995',), ('IV 2017',), ('2015',), ('2011',), ('1994',), ('2018',), ('2007',), ('1974',), ('1995',), ('1976',), ('1993',), ('1998',), ('1988',), ('1970',), ('2003',), ('2011',), ('2014',), ('1963',), ('1983',), ('2008',), ('2009',), ('2005',), ('2007',), ('1972',), ('2004',), ('2005',), ('1990',), ('1979',), ('2014',), ('2013',), ('1977',), ('2001',), ('2001',), ('2000',), ('1943',), ('2007',), ('2011',), ('1984',), ('1958',), ('2004',), ('1984',), ('2002',), ('1984',), ('2000',), ('2010',), ('1978',), ('1989',), ('1967',), ('1989',), ('1956',), ('2001',), ('2004',), ('2003',), ('1973',), ('1991',), ('1966',), ('2013',), ('2002',), ('2006',), ('2012',), ('1983',), ('1996',), ('2009',), ('1979',), ('1976',), ('2012',), ('2005',), ('2009',), ('2008',), ('2001',), ('2009',), ('1936',), ('2013',), ('2003',), ('1999',), ('1988',), ('1980',), ('2012',), ('2000',), ('1986',), ('1955',), ('2015',), ('2009',), ('1965',), ('2017',), ('2007',), ('1993',), ('2002',), ('1990',), ('1960',), ('1970',), ('1984',), ('2004',), ('1974',), ('2007',), ('1969',), ('1972',), ('2011',), ('2001',), ('2010',), ('I 2007',), ('2007',), ('1974',), ('1999',), ('1936',), ('1982',), ('1993',), ('1998',), ('1988',), ('I 1989',), ('2016',), ('2006',), ('1993',), ('2005',), ('2011',), ('1994',), ('1979',), ('1990',), ('2005',), ('2011',), ('2005',), ('1950',), ('2009',), ('2004',), ('1992',), ('2009',), ('2002',), ('2008',), ('1985',), ('I 1969',), ('1994',), ('2003',), ('2012',), ('1957',), ('2004',), ('1996',), ('2000',), ('2003',), ('2003',), ('2016',), ('2009',), ('2015',), ('1992',), ('2015',), ('2006',), ('2002',), ('2011',), ('1988',), ('1985',), ('2012',), ('2002',), ('1997',), ('1957',), ('2010',), ('1999',), ('2010',), ('2006',), ('2007',), ('2013',), ('2012',), ('2017',), ('2015',), ('1974',), ('1954',), ('2015',), ('1955',), ('2018',), ('1992',), ('1982',), ('2007',), ('2005',), ('1993',), ('2011',), ('1992',), ('1973',), ('1992',), ('1991',), ('2008',), ('2004',), ('1968',), ('1948',), ('1984',), ('1953',), ('I 2010',), ('1999',), ('1977',), ('1996',), ('1956',), ('1997',), ('1993',), ('1963',), ('2007',), ('1984',), ('I 2009',), ('1977',), ('2005',), ('2005',), ('1973',), ('1950',), ('1973',), ('1996',), ('1971',), ('1987',), ('1991',), ('1986',), ('2012',), ('2006',), ('1991',), ('2013',), ('2007',), ('1975',), ('1990',), ('1982',), ('1999',), ('2005',), ('1980',), ('2000',), ('1995',), ('2000',), ('2014',), ('2013',), ('2011',), ('1977',), ('1992',), ('1990',), ('1994',), ('I 1989',), ('2001',), ('1967',), ('2010',), ('1980',), ('1979',), ('2008',), ('2003',), ('1994',), ('2005',), ('1992',), ('2013',), ('2007',), ('1982',), ('2008',), ('2016',), ('1968',), ('2002',), ('1975',), ('2017',), ('1992',), ('2015',), ('2009',), ('1995',), ('1983',), ('I 2014',), ('2013',), ('2012',), ('2009',), ('2005',), ('2008',), ('2011',), ('2001',), ('2011',), ('2006',), ('2006',), ('2008',), ('1979',), ('2011',), ('1952',), ('2001',), ('1986',), ('1957',), ('1984',), ('1992',), ('1990',), ('2005',), ('2011',), ('2010',), ('2001',), ('2007',), ('1985',), ('2010',), ('2003',), ('1987',), ('2009',), ('2017',), ('2015',), ('2006',), ('2014',), ('2007',), ('1975',), ('1994',), ('1988',), ('2002',), ('1995',), ('1966',), ('2004',), ('2004',), ('1983',), ('2010',), ('2001',), ('2011',), ('2009',), ('1983',), ('2004',), ('2010',), ('2001',), ('2009',), ('1968',), ('2012',), ('2002',), ('1978',), ('2009',), ('1985',), ('2006',), ('1962',), ('2005',), ('2009',), ('1993',), ('1970',), ('2005',), ('1996',), ('1997',), ('2012',), ('1979',), ('2003',), ('1991',), ('2001',), ('2011',), ('1960',), ('1989',), ('2013',), ('1990',), ('2011',), ('2010',), ('2007',), ('1980',), ('2013',), ('2004',), ('1981',), ('2007',), ('2002',), ('1970',), ('1999',), ('1985',), ('1990',), ('1995',), ('1986',), ('2007',), ('2008',), ('2004',), ('2004',), ('1996',), ('2010',), ('1979',), ('2015',), ('2007',), ('2000',), ('2005',), ('2006',), ('1947',), ('2011',), ('1946',), ('2000',), ('1976',), ('2012',), ('1984',), ('1992',), ('1984',), ('1991',), ('2007',), ('1993',), ('1992',), ('1998',), ('1987',), ('2005',), ('1989',), ('2014',), ('2003',), ('2004',), ('2007',), ('1973',), ('2008',), ('2003',), ('II 2009',), ('1984',), ('1991',), ('1992',), ('1999',), ('1993',), ('1999',), ('2012',), ('2004',), ('2002',), ('2004',), ('1987',), ('2009',), ('2008',), ('2010',), ('1997',), ('1998',), ('2009',), ('2011',), ('2011',), ('2005',), ('1996',), ('2008',), ('2012',), ('2004',), ('1981',), ('2010',), ('2011',), ('1997',), ('1992',), ('1991',), ('I 1992',), ('1978',), ('2002',), ('1996',), ('1981',), ('2006',), ('1989',), ('2011',), ('1988',), ('1997',), ('1985',), ('1999',), ('2009',), ('2003',), ('1958',), ('2013',), ('2000',), ('2000',), ('1990',), ('1968',), ('1995',), ('1985',), ('1997',), ('2003',), ('2001',), ('2003',), ('2009',), ('1963',), ('2004',), ('2003',), ('2004',), ('1982',), ('2014',), ('1989',), ('1995',), ('1995',), ('1985',), ('1997',), ('1991',), ('1989',), ('2008',), ('2011',), ('2012',), ('2010',), ('2011',), ('2014',), ('2008',), ('2009',), ('2009',), ('2005',), ('2006',), ('2008',), ('2007',), ('1972',), ('1987',), ('2006',), ('2005',), ('1992',), ('2009',), ('2002',), ('I 2011',), ('1961',), ('1991',), ('1976',), ('2002',), ('2012',), ('2018',), ('2010',), ('1981',), ('2005',), ('2008',), ('1990',), ('1968',), ('2009',), ('1998',), ('2005',), ('1995',), ('2006',), ('1993',), ('1998',), ('1996',), ('1983',), ('1995',), ('1975',), ('1986',), ('1993',), ('2006',), ('1939',), ('1994',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def find_where_condition(english_question):\n",
        "  # Parse the English question to extract the relevant information\n",
        "  doc = nlp(english_question)\n",
        "  where_condition = ''\n",
        "  for token in doc:\n",
        "    # If the token is a named entity, it may be relevant for the WHERE condition\n",
        "    if token.ent_type_ == 'QUANTITY':\n",
        "      # Extract the entity and the constraint from the question\n",
        "      entity = token.text\n",
        "      constraint = token.dep_\n",
        "      # Construct the WHERE condition using the extracted information\n",
        "      where_condition = f\"{entity} {constraint}\"\n",
        "  return where_condition\n",
        "\n",
        "# Find the WHERE condition for the question 'Which films have a rating greater than 8?'\n",
        "english_question = 'Which films have a rating greater than 8?'\n",
        "where_condition = find_where_condition(english_question)\n",
        "print(where_condition)  # Output: 'rating >'\n",
        "\n",
        "# Modify the SQL query to include the WHERE condition\n",
        "query = f\"SELECT * FROM films WHERE {where_condition}\"\n",
        "print(query)  # Output: 'SELECT * FROM films WHERE rating >'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCR0B9X8uxY4",
        "outputId": "e1bc4601-bbd9-4e55-d069-1d6c6ae686d5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SELECT * FROM films WHERE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model from the spacy library\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = 'What are names of films of year 2018?'\n",
        "\n",
        "# Tokenize and tag the query sentence using the spacy model\n",
        "doc = nlp(query_sentence)\n",
        "\n",
        "# Set a flag to indicate whether a WHERE condition is needed\n",
        "where_needed = False\n",
        "\n",
        "# Iterate over the tokens in the query sentence\n",
        "for token in doc:\n",
        "    # Check if the token is a conjunction or preposition\n",
        "    if token.pos_ in ['CONJ', 'ADP']:\n",
        "        where_needed = True\n",
        "        break\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "# Update the path to the database file\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# If a WHERE condition is needed, retrieve the relevant table and column names\n",
        "if where_needed:\n",
        "    table_names = [table_info[0] for table_info in cursor.execute(\n",
        "        \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "    column_names = []\n",
        "    for table_name in table_names:\n",
        "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "        column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Execute the SQL query to retrieve the data from the highest matching table\n",
        "cursor.execute(\n",
        "    f\"SELECT * FROM {highest_matching_table_name}\")\n",
        "\n",
        "# Fetch the results of the SQL query\n",
        "results = cursor.fetchall()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Results: {results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1OGt-MEx4bn",
        "outputId": "7a6291fb-f3e4-4a2d-c602-9a7d460307a9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table name: Movie, cosine similarity score: 0.43755286931991577\n",
            "Table name: Genre, cosine similarity score: 0.37454211711883545\n",
            "Table name: M_Genre, cosine similarity score: 0.28868740797042847\n",
            "Table name: Country, cosine similarity score: 0.2200353890657425\n",
            "Table name: M_Director, cosine similarity score: 0.21986666321754456\n",
            "Table name: M_Cast, cosine similarity score: 0.20338064432144165\n",
            "Table name: M_Country, cosine similarity score: 0.1879069060087204\n",
            "Table name: M_Producer, cosine similarity score: 0.1701001673936844\n",
            "Table name: Language, cosine similarity score: 0.16172876954078674\n",
            "Table name: Location, cosine similarity score: 0.16073311865329742\n",
            "Table name: Person, cosine similarity score: 0.10034488141536713\n",
            "Table name: M_Language, cosine similarity score: 0.07654733955860138\n",
            "Table name: M_Location, cosine similarity score: 0.06711308658123016\n",
            "Results: [(0, 'tt2388771', 'Mowgli', '2018', 6.6, 21967), (1, 'tt5164214', \"Ocean's Eight\", '2018', 6.2, 110861), (2, 'tt1365519', 'Tomb Raider', '2018', 6.4, 142585), (3, 'tt0848228', 'The Avengers', '2012', 8.1, 1137529), (4, 'tt8239946', 'Tumbbad', '2018', 8.5, 7483), (5, 'tt7027278', 'Kedarnath', '2018', 5.5, 1970), (6, 'tt3498820', 'Captain America: Civil War', '2016', 7.8, 536641), (7, 'tt8108198', 'Andhadhun', '2018', 9.0, 18160), (8, 'tt3741834', 'Lion', '2016', 8.1, 170216), (9, 'tt6747420', 'Rajma Chawal', '2018', 5.7, 681), (10, 'tt1981128', 'Geostorm', '2017', 5.3, 72375), (11, 'tt1010048', 'Slumdog Millionaire', '2008', 8.0, 729879), (12, 'tt1190080', '2012', 'I 2009', 5.8, 320472), (13, 'tt3726012', 'Mastizaade', '2016', 2.4, 2205), (14, 'tt0075860', 'Close Encounters of the Third Kind', '1977', 7.7, 163278), (15, 'tt6923462', 'Manto', '2018', 7.5, 1457), (16, 'tt0454876', 'Life of Pi', '2012', 7.9, 518138), (17, 'tt1606378', 'A Good Day to Die Hard', '2013', 5.3, 184585), (18, 'tt2120120', 'Pixels', '2015', 5.6, 116089), (19, 'tt8108202', 'Stree', '2018', 8.0, 12996), (20, 'tt5970844', 'Thugs of Hindostan', '2018', 3.6, 11188), (21, 'tt7725596', 'Badhaai Ho', '2018', 8.2, 8418), (22, 'tt5461944', 'Hotel Mumbai', '2018', 7.1, 160), (23, 'tt0838221', 'The Darjeeling Limited', '2007', 7.2, 161266), (24, 'tt8426854', 'Jalebi', '2018', 5.9, 475), (25, 'tt0286499', 'Bend It Like Beckham', '2002', 6.7, 97876), (26, 'tt5074352', 'Dangal', '2016', 8.5, 111130), (27, 'tt5474036', 'Manmarziyaan', '2018', 7.0, 2770), (28, 'tt0043456', 'The Day the Earth Stood Still', '1951', 7.8, 71047), (29, 'tt1098327', 'Dragonball Evolution', '2009', 2.6, 64493), (30, 'tt8396128', 'Pataakha', '2018', 7.3, 1606), (31, 'tt2884206', 'I Origins', '2014', 7.4, 101213), (32, 'tt0366551', 'Harold & Kumar Go to White Castle', '2004', 7.1, 169683), (33, 'tt1187043', '3 Idiots', '2009', 8.4, 287848), (34, 'tt0986264', 'Taare Zameen Par', '2007', 8.4, 133783), (35, 'tt3735246', 'Bajirao Mastani', '2015', 7.2, 27161), (36, 'tt2338151', 'PK', '2014', 8.2, 130977), (37, 'tt2980648', 'The Hundred-Foot Journey', '2014', 7.3, 66899), (38, 'tt1954470', 'Gangs of Wasseypur', '2012', 8.2, 66292), (39, 'tt0120102', 'Seven Years in Tibet', '1997', 7.0, 109930), (40, 'tt0086034', 'Octopussy', '1983', 6.6, 84600), (41, 'tt0109424', 'Chung Hing sam lam', '1994', 8.1, 50603), (42, 'tt6452574', 'Sanju', '2018', 8.1, 35436), (43, 'tt5816682', 'Victoria & Abdul', '2017', 6.8, 23051), (44, 'tt7919680', 'Karwaan', '2018', 7.6, 6333), (45, 'tt8852558', 'Mitron', '2018', 7.1, 1165), (46, 'tt7218518', 'Padman', '2018', 8.1, 12749), (47, 'tt3004774', 'Tigers', '2014', 7.4, 746), (48, 'tt1166100', 'Ghajini', '2008', 7.3, 49705), (49, 'tt1412386', 'The Best Exotic Marigold Hotel', '2011', 7.3, 84136), (50, 'tt0327437', 'Around the World in 80 Days', '2004', 5.9, 77402), (51, 'tt0116409', 'The Ghost and the Darkness', '1996', 6.8, 51491), (52, 'tt5935704', 'Padmaavat', '2018', 7.0, 20603), (53, 'tt7098658', 'Raazi', '2018', 7.8, 13771), (54, 'tt6967980', 'Bareilly Ki Barfi', '2017', 7.5, 10592), (55, 'tt0169102', 'Lagaan: Once Upon a Time in India', '2001', 8.1, 90179), (56, 'tt8439854', 'Lust Stories', '2018', 6.6, 6885), (57, 'tt1188996', 'My Name Is Khan', '2010', 8.0, 88262), (58, 'tt3823392', 'Love Sonia', '2018', 7.2, 275), (59, 'tt7720922', 'Batti Gul Meter Chalu', '2018', 6.0, 2370), (60, 'tt7820846', 'Loveyatri', '2018', 2.9, 1345), (61, 'tt1285241', 'Don 2', '2011', 7.1, 45291), (62, 'tt0405508', 'Rang De Basanti', '2006', 8.2, 97421), (63, 'tt7853242', 'Love Per Square Foot', '2018', 7.2, 4847), (64, 'tt2882328', 'Ugly', '2013', 8.1, 15869), (65, 'tt2631186', 'Bahubali: The Beginning', '2015', 8.2, 90115), (66, 'tt0066763', 'Anand', '1971', 8.8, 21616), (67, 'tt4849438', 'Baahubali 2: The Conclusion', '2017', 8.3, 61978), (68, 'tt7147540', 'Sui Dhaaga: Made in India', '2018', 6.8, 3171), (69, 'tt7526836', 'FryDay', '2018', 5.9, 376), (70, 'tt0808357', 'Se, jie', '2007', 7.6, 34025), (71, 'tt6173990', 'Gold', 'I 2018', 7.6, 7678), (72, 'tt6129302', 'Bhavesh Joshi Superhero', '2018', 7.6, 3493), (73, 'tt7581902', 'Sonu Ke Titu Ki Sweety', '2018', 7.2, 13599), (74, 'tt8202612', 'Satyameva Jayate', '2018', 5.8, 4446), (75, 'tt7431594', 'Race 3', '2018', 2.1, 27282), (76, 'tt0809504', 'The Accidental Husband', '2008', 5.6, 21636), (77, 'tt1647668', 'Million Dollar Arm', '2014', 7.0, 40342), (78, 'tt3044882', 'Nasha', '2013', 3.3, 1000), (79, 'tt2350496', 'Dabba', '2013', 7.8, 41224), (80, 'tt6148156', 'Vikram Vedha', '2017', 8.8, 16385), (81, 'tt6206564', 'Trapped', 'XVII 2016', 7.6, 6814), (82, 'tt6826438', 'Parmanu: The Story of Pokhran', '2018', 7.8, 13693), (83, 'tt2082197', 'Barfi!', '2012', 8.1, 66919), (84, 'tt0051383', 'Auntie Mame', '1958', 8.0, 9261), (85, 'tt3224288', 'Beyond the Clouds', '2017', 7.0, 1123), (86, 'tt3148502', 'Tamasha', '2015', 7.2, 18984), (87, 'tt7180544', 'Mukkabaaz', '2017', 8.1, 4273), (88, 'tt0361411', 'Bride & Prejudice', '2004', 6.2, 18827), (89, 'tt7638344', 'Dhadak', '2018', 4.4, 7520), (90, 'tt5690142', 'Mom', 'I 2017', 7.3, 6025), (91, 'tt0087892', 'A Passage to India', '1984', 7.4, 15181), (92, 'tt7881542', 'Happy Phirr Bhag Jayegi', '2018', 4.6, 589), (93, 'tt0248126', 'Kabhi Khushi Kabhie Gham...', '2001', 7.5, 38633), (94, 'tt4635372', 'Masaan', '2015', 8.1, 17190), (95, 'tt4977530', \"Viceroy's House\", '2017', 6.7, 5008), (96, 'tt8011276', 'Laila Majnu', '2018', 7.8, 939), (97, 'tt4699202', 'Gurgaon', '2017', 6.7, 907), (98, 'tt5946128', 'Dear Zindagi', '2016', 7.7, 30352), (99, 'tt6484982', 'Newton', '2017', 7.7, 11177), (100, 'tt6774212', 'Aiyaary', '2018', 5.2, 2971), (101, 'tt0093603', 'Nayakan', '1987', 8.8, 12432), (102, 'tt6514196', 'Baazaar', '2018', 6.9, 1099), (103, 'tt7363076', 'Raid', '2018', 7.4, 9462), (104, 'tt4110568', 'Dil Dhadakne Do', '2015', 6.8, 12186), (105, 'tt7700730', 'October', 'II 2018', 7.6, 8660), (106, 'tt4934950', 'Talvar', '2015', 8.2, 23970), (107, 'tt2181831', 'Shahid', '2012', 8.3, 12127), (108, 'tt3405236', 'Raees', '2017', 6.9, 35263), (109, 'tt1821480', 'Kahaani', '2012', 8.2, 50219), (110, 'tt3418424', 'One Less God', '2017', 5.8, 408), (111, 'tt2574698', 'Gunday', '2014', 2.1, 56040), (112, 'tt0115641', 'Beavis and Butt-Head Do America', '1996', 6.8, 47517), (113, 'tt3863552', 'Bajrangi Bhaijaan', '2015', 8.0, 60969), (114, 'tt3043252', 'Parched', '2015', 7.6, 3875), (115, 'tt4129428', 'Jagga Jasoos', '2017', 6.4, 7530), (116, 'tt1849718', 'Agneepath', '2012', 7.0, 19962), (117, 'tt3001638', 'Mary Kom', '2014', 6.8, 8962), (118, 'tt4434004', 'Udta Punjab', '2016', 7.8, 21805), (119, 'tt7469726', 'Shaadi Mein Zaroor Aana', '2017', 7.8, 8455), (120, 'tt0891592', 'Street Fighter: The Legend of Chun-Li', '2009', 3.7, 21488), (121, 'tt0375611', 'Black', '2005', 8.2, 30225), (122, 'tt7690638', 'Soorma', '2018', 7.5, 2850), (123, 'tt5662932', 'Raman Raghav 2.0', '2016', 7.3, 9769), (124, 'tt0432637', 'Krrish', '2006', 6.4, 15508), (125, 'tt4387040', 'Airlift', '2016', 8.1, 46484), (126, 'tt0238936', 'Devdas', 'I 2002', 7.6, 33456), (127, 'tt6692354', 'Ittefaq', '2017', 7.2, 7561), (128, 'tt4559006', 'Ae Dil Hai Mushkil', '2016', 5.8, 17713), (129, 'tt6972140', 'Blackmail', 'I 2018', 7.0, 5538), (130, 'tt5571734', 'Pink', 'III 2016', 8.2, 30231), (131, 'tt2436516', 'Go Goa Gone', '2013', 6.6, 10180), (132, 'tt4430212', 'Drishyam', '2015', 8.3, 51007), (133, 'tt1185420', 'Dostana', '2008', 6.5, 12486), (134, 'tt0367495', 'Anbe Sivam', '2003', 8.8, 12285), (135, 'tt5842616', 'Veere Di Wedding', '2018', 3.1, 9901), (136, 'tt6071752', 'Dil Juunglee', '2018', 4.5, 177), (137, 'tt1833673', 'Dhoom:3', '2013', 5.4, 39578), (138, 'tt6108090', 'Secret Superstar', '2017', 8.0, 14820), (139, 'tt1280558', 'A Wednesday', '2008', 8.2, 66187), (140, 'tt2215477', 'Goliyon Ki Rasleela Ram-Leela', '2013', 6.4, 16086), (141, 'tt2408040', 'B.A. Pass', '2012', 6.5, 2967), (142, 'tt4832640', 'Sultan', '2016', 7.1, 32109), (143, 'tt5785170', 'Toilet - Ek Prem Katha', '2017', 7.3, 13815), (144, 'tt7981260', 'Garbage', 'I 2018', 4.2, 213), (145, 'tt0112870', 'Dilwale Dulhania Le Jayenge', '1995', 8.2, 56133), (146, 'tt1327035', 'Dev.D', '2009', 8.0, 25358), (147, 'tt7618184', 'Mulk', 'I 2018', 6.1, 10354), (148, 'tt0457655', 'Efter brylluppet', '2006', 7.8, 28878), (149, 'tt5956100', 'Tiger Zinda Hai', '2017', 6.1, 17804), (150, 'tt0347304', 'Kal Ho Naa Ho', '2003', 8.0, 56080), (151, 'tt4900716', 'Kapoor & Sons', '2016', 7.7, 18959), (152, 'tt8136908', 'Halkaa', '2018', 7.3, 151), (153, 'tt5460276', 'Kaabil', '2017', 7.1, 21640), (154, 'tt3771536', 'Shivaay', '2016', 6.3, 8813), (155, 'tt4535650', 'Dilwale', '2015', 5.2, 28043), (156, 'tt0265343', 'Monsoon Wedding', '2001', 7.4, 21795), (157, 'tt5542802', 'Raabta', '2017', 4.0, 2378), (158, 'tt1787988', 'Talaash', '2012', 7.3, 35343), (159, 'tt3678782', 'Badlapur', '2015', 7.5, 18393), (160, 'tt3702652', 'The Other Side of the Door', '2016', 5.3, 15433), (161, 'tt1839596', 'Rockstar', '2011', 7.7, 33455), (162, 'tt1024943', 'Om Shanti Om', '2007', 6.7, 34131), (163, 'tt0172684', 'Kuch Kuch Hota Hai', '1998', 7.7, 41863), (164, 'tt1093370', 'Jab We Met', '2007', 7.9, 40758), (165, 'tt4814290', 'Te3n', '2016', 7.2, 9511), (166, 'tt5997666', 'Jab Harry Met Sejal', '2017', 5.4, 15884), (167, 'tt3390572', 'Haider', '2014', 8.2, 44746), (168, 'tt1321869', 'The Lovers', '2013', 4.6, 2807), (169, 'tt5882970', 'Tubelight', '2017', 4.1, 11886), (170, 'tt7275232', 'Welcome to New York', '2018', 2.3, 316), (171, 'tt0420332', 'Veer-Zaara', '2004', 7.9, 43753), (172, 'tt1934231', 'Delhi Belly', '2011', 7.6, 23791), (173, 'tt3767372', 'Piku', '2015', 7.6, 23651), (174, 'tt0449994', 'Jodhaa Akbar', '2008', 7.6, 25928), (175, 'tt5613834', 'A Gentleman', '2017', 6.2, 3918), (176, 'tt1562871', 'Ra.One', '2011', 4.8, 34920), (177, 'tt6485666', 'Mersal', '2017', 8.1, 21247), (178, 'tt6588966', 'Hichki', '2018', 7.5, 6096), (179, 'tt1562872', 'Zindagi Na Milegi Dobara', '2011', 8.1, 56579), (180, 'tt1182937', 'Rab Ne Bana Di Jodi', '2008', 7.2, 39110), (181, 'tt3679060', 'Dishoom', '2016', 5.1, 5443), (182, 'tt1620933', 'Paan Singh Tomar', '2012', 8.2, 28578), (183, 'tt7399470', 'Qarib Qarib Singlle', '2017', 7.0, 3823), (184, 'tt2213054', 'Kai po che!', '2013', 7.6, 23703), (185, 'tt2178470', 'Yeh Jawaani Hai Deewani', '2013', 7.0, 32668), (186, 'tt5255710', 'Sanam Teri Kasam', '2016', 7.3, 7913), (187, 'tt3541262', 'Myeong-ryang', '2014', 7.2, 8489), (188, 'tt0367110', 'Swades: We, the People', '2004', 8.3, 72397), (189, 'tt6102396', 'Kaalakaandi', '2018', 6.2, 1576), (190, 'tt6170954', 'Naam Shabana', '2017', 6.3, 4221), (191, 'tt2176013', 'Jab Tak Hai Jaan', '2012', 6.8, 47551), (192, 'tt6580564', '102 Not Out', '2018', 7.5, 4179), (193, 'tt1974382', 'The Challenger', 'I 2015', 5.7, 1122), (194, 'tt6843812', 'Baaghi 2', '2018', 5.1, 6809), (195, 'tt7607940', 'Namaste England', '2018', 1.3, 1621), (196, 'tt0073707', 'Sholay', '1975', 8.2, 45175), (197, 'tt6978268', 'Omerta', '2017', 7.0, 572), (198, 'tt7412738', 'Nanu Ki Jaanu', '2018', 6.5, 721), (199, 'tt0031580', 'The Little Princess', '1939', 7.3, 4956), (200, 'tt8055888', 'Mard Ko Dard Nahin Hota', '2018', 7.5, 125), (201, 'tt7329858', 'Pari', 'I 2018', 6.6, 4020), (202, 'tt0871510', 'Chak De! India', '2007', 8.2, 64275), (203, 'tt3322420', 'Queen', '2013', 8.3, 53328), (204, 'tt2112124', 'Chennai Express', '2013', 6.0, 43370), (205, 'tt0400234', 'Black Friday', '2004', 8.6, 15500), (206, 'tt1275863', 'Love Aaj Kal', '2009', 6.8, 13113), (207, 'tt3447364', 'Detective Byomkesh Bakshy!', '2015', 7.6, 13768), (208, 'tt0441048', 'Dhoom:2', '2006', 6.5, 20855), (209, 'tt0433416', 'The Namesake', '2006', 7.5, 17893), (210, 'tt5050788', 'Loev', '2015', 5.9, 1486), (211, 'tt4399594', 'Fitoor', '2016', 5.4, 3719), (212, 'tt5474042', 'Half Girlfriend', '2017', 4.3, 5106), (213, 'tt8458202', 'Pihu', 'I 2018', 6.6, 168), (214, 'tt2361746', 'Haraamkhor', '2015', 6.4, 2737), (215, 'tt6971752', 'Shubh Mangal Saavdhan', '2017', 6.9, 5404), (216, 'tt0280720', 'The Guru', '2002', 5.4, 14292), (217, 'tt5197544', 'Baar Baar Dekho', '2016', 5.3, 5188), (218, 'tt5325684', 'Hate Story IV', '2018', 3.4, 1052), (219, 'tt0374887', 'Munna Bhai M.B.B.S.', '2003', 8.2, 62149), (220, 'tt3848892', 'Baby', 'I 2015', 8.1, 46737), (221, 'tt2557256', 'Bing feng: Chong sheng zhi men', '2014', 4.8, 2890), (222, 'tt7471004', 'The House Next Door', '2017', 6.8, 1472), (223, 'tt0213890', 'Mohabbatein', '2000', 7.1, 24031), (224, 'tt3469244', 'Phantom', 'I 2015', 5.8, 7505), (225, 'tt5745450', 'Chef', '2017', 5.6, 1549), (226, 'tt5518128', 'Love Games', '2016', 4.4, 668), (227, 'tt6514010', 'Brij Mohan Amar Rahe', '2017', 7.1, 2328), (228, 'tt3614516', 'Ankhon Dekhi', '2013', 8.0, 7924), (229, 'tt2150177', 'Bhaiaji Superhit', '2018', 5.7, 202), (230, 'tt3542028', 'Chal Bhaag', '2014', 4.3, 73), (231, 'tt0096028', 'Salaam Bombay!', '1988', 8.0, 7599), (232, 'tt5071886', 'Kabali', '2016', 6.4, 10035), (233, 'tt5764096', 'Hindi Medium', '2017', 7.8, 15535), (234, 'tt0479751', 'Sivaji', '2007', 7.5, 14881), (235, 'tt0466460', 'Khosla Ka Ghosla!', '2006', 8.3, 19118), (236, 'tt0379370', 'Maqbool', '2003', 8.2, 7352), (237, 'tt2377938', 'Special Chabbis', '2013', 8.0, 43542), (238, 'tt0964516', 'Fashion', '2008', 7.0, 10704), (239, 'tt1029231', 'Krrish 3', '2013', 5.3, 18433), (240, 'tt7162758', 'Paltan', '2018', 5.5, 515), (241, 'tt1292703', 'Oye Lucky! Lucky Oye!', '2008', 7.8, 14094), (242, 'tt3495026', 'Fan', 'I 2016', 7.2, 37639), (243, 'tt5358948', 'One Night Stand', 'I 2016', 3.2, 847), (244, 'tt7609114', 'Yamla Pagla Deewana Phir Se...', '2018', 5.3, 684), (245, 'tt1926313', 'Pyaar Ka Punchnama', '2011', 7.7, 17080), (246, 'tt5713232', 'Madaari', '2016', 7.6, 10537), (247, 'tt2283748', 'OMG: Oh My God!', '2012', 8.2, 41975), (248, 'tt8338746', 'Phamous', '2018', 4.5, 99), (249, 'tt5108476', 'Befikre', '2016', 3.9, 6134), (250, 'tt1836987', 'Trishna', '2011', 6.0, 2781), (251, 'tt1324059', 'Wake Up Sid', '2009', 7.6, 23993), (252, 'tt1948150', 'Singham', '2011', 6.8, 12356), (253, 'tt0461936', 'Don', 'I 2006', 7.2, 30993), (254, 'tt2929690', 'Margarita with a Straw', '2014', 7.3, 2793), (255, 'tt1198101', 'Kites', 'I 2010', 6.1, 11149), (256, 'tt3495030', 'Dum Laga Ke Haisha', '2015', 7.5, 13979), (257, 'tt3554418', 'Khoobsurat', '2014', 6.4, 6586), (258, 'tt3177332', 'Horror Story', 'I 2013', 4.4, 1232), (259, 'tt2203308', 'Aashiqui 2', '2013', 7.0, 23969), (260, 'tt1438298', 'Guzaarish', '2010', 7.5, 14871), (261, 'tt0234000', 'Kaho Naa... Pyaar Hai', '2000', 6.9, 11855), (262, 'tt2461132', 'Happy New Year', 'I 2014', 5.2, 30754), (263, 'tt0425326', 'Outsourced', '2006', 7.0, 10670), (264, 'tt3337550', 'Rahasya', '2015', 7.6, 3596), (265, 'tt1629376', '7 Khoon Maaf', '2011', 6.1, 4815), (266, 'tt1261047', 'Gulaal', '2009', 8.1, 12019), (267, 'tt7384848', 'Missing', 'I 2018', 5.8, 1294), (268, 'tt5918074', 'A Death in the Gunj', '2016', 7.5, 2688), (269, 'tt2309987', 'Hate Story', '2012', 5.3, 3086), (270, 'tt7722258', 'Genius', 'I 2018', 4.9, 615), (271, 'tt2797242', 'Bombay Talkies', '2013', 6.7, 4380), (272, 'tt2172071', 'Student of the Year', '2012', 5.5, 12040), (273, 'tt1806913', 'Golmaal Again', '2017', 5.0, 6715), (274, 'tt0292490', 'Dil Chahta Hai', '2001', 8.2, 59380), (275, 'tt1773764', 'Ship of Theseus', '2012', 8.1, 5409), (276, 'tt1734110', 'No One Killed Jessica', '2011', 7.2, 9661), (277, 'tt4430136', 'Pyaar Ka Punchnama 2', '2015', 7.3, 12356), (278, 'tt6277440', 'Badrinath Ki Dulhania', '2017', 6.2, 8131), (279, 'tt4909752', 'Rangoon', 'II 2017', 5.8, 3143), (280, 'tt2226666', 'Jism 2', '2012', 3.0, 3018), (281, 'tt3859980', 'Mohenjo Daro', '2016', 5.8, 9236), (282, 'tt8060624', 'Nawabzaade', '2018', 5.0, 661), (283, 'tt0422091', 'Dhoom', '2004', 6.7, 15972), (284, 'tt2016894', 'Ek Tha Tiger', '2012', 5.5, 25626), (285, 'tt2979920', 'Bombay Velvet', '2015', 5.7, 4917), (286, 'tt0347473', 'Main Hoon Na', '2004', 7.0, 28435), (287, 'tt5121000', 'Aligarh', '2015', 7.8, 4462), (288, 'tt2317337', 'Vicky Donor', '2012', 7.8, 33976), (289, 'tt5165344', 'Rustom', '2016', 7.1, 18773), (290, 'tt2905838', 'Bang Bang', 'I 2014', 5.5, 19424), (291, 'tt0296574', 'Company', '2002', 8.0, 12722), (292, 'tt5465370', 'Akira', 'I 2016', 5.9, 3344), (293, 'tt0473367', 'Jaane Tu... Ya Jaane Na', '2008', 7.5, 22562), (294, 'tt6405126', 'Simran', '2017', 5.4, 2428), (295, 'tt4467262', 'Katti Batti', '2015', 4.9, 2418), (296, 'tt2976182', 'Roy', '2015', 3.3, 6568), (297, 'tt0488414', 'Omkara', '2006', 8.1, 16627), (298, 'tt3709344', 'City Lights', '2014', 7.3, 3794), (299, 'tt2126282', 'Ekk Deewana Tha', '2012', 5.9, 2635), (300, 'tt2356180', 'Bhaag Milkha Bhaag', '2013', 8.2, 52802), (301, 'tt0352314', 'Ek Hasina Thi', '2004', 7.6, 5276), (302, 'tt0499375', 'Guru', '2007', 7.8, 19300), (303, 'tt6272828', 'Mubarakan', '2017', 5.6, 2830), (304, 'tt1185442', 'Kurbaan', '2009', 5.7, 4150), (305, 'tt4232066', 'Once Again', '2018', 7.3, 347), (306, 'tt5456546', 'Judwaa 2', '2017', 3.9, 6783), (307, 'tt7617988', 'Kuchh Bheege Alfaaz', '2018', 7.7, 365), (308, 'tt0254481', 'Koi... Mil Gaya', '2003', 7.1, 18342), (309, 'tt4864932', 'Baaghi', '2016', 5.2, 5486), (310, 'tt0242519', 'Hera Pheri', '2000', 8.2, 46252), (311, 'tt2635622', 'The Attacks of 26/11', '2013', 6.8, 4125), (312, 'tt0116630', 'Indian', '1996', 8.1, 8419), (313, 'tt1077248', 'Johnny Gaddaar', '2007', 7.9, 9847), (314, 'tt5997928', 'Hotel Salvation', '2016', 7.2, 1243), (315, 'tt1274295', 'Kaminey', '2009', 7.4, 14922), (316, 'tt6471054', 'Ajji', '2017', 7.1, 469), (317, 'tt0164538', 'Dil Se..', '1998', 7.7, 23879), (318, 'tt2372678', '2 States', '2014', 6.9, 21230), (319, 'tt1049405', 'The Other End of the Line', '2008', 6.2, 5403), (320, 'tt2855648', 'Madras Cafe', '2013', 7.7, 20300), (321, 'tt0449999', 'Kabhi Alvida Naa Kehna', '2006', 6.1, 16374), (322, 'tt1954598', 'Ladies vs. Ricky Bahl', '2011', 6.0, 7610), (323, 'tt2609218', 'Ragini MMS 2', '2014', 3.9, 2389), (324, 'tt0222012', 'Hey Ram', '2000', 8.0, 9888), (325, 'tt5286444', 'Neerja', '2016', 7.7, 17500), (326, 'tt0995740', 'No Smoking', '2007', 7.2, 5458), (327, 'tt2359810', 'Raanjhanaa', '2013', 7.6, 26011), (328, 'tt2229848', 'Chashme Baddoor', '2013', 5.4, 4612), (329, 'tt2168910', 'Cocktail', '2012', 6.2, 11935), (330, 'tt0272736', 'Mujhse Dosti Karoge!', '2002', 5.1, 4358), (331, 'tt0439662', 'Fanaa', '2006', 7.2, 28078), (332, 'tt1375789', 'Race 2', '2013', 5.3, 13733), (333, 'tt1639426', 'Udaan', '2010', 8.2, 37942), (334, 'tt6299040', 'The Ghazi Attack', '2017', 7.6, 9077), (335, 'tt4169250', 'M.S. Dhoni: The Untold Story', '2016', 7.7, 27042), (336, 'tt1091229', 'Chandni Chowk to China', '2009', 4.0, 6944), (337, 'tt1433810', 'Dhobi Ghat', '2010', 7.1, 11406), (338, 'tt5697870', 'Lucknow Central', '2017', 6.1, 2073), (339, 'tt4940456', 'Mirzya', '2016', 4.7, 1495), (340, 'tt0151150', 'Josh', '2000', 6.2, 8426), (341, 'tt5477608', 'Kahaani 2', '2016', 6.6, 4128), (342, 'tt7142506', 'Sir', 'I 2018', 7.0, 146), (343, 'tt0376127', 'Anniyan', '2005', 8.2, 11244), (344, 'tt4276752', 'Xun Long Jue', '2015', 6.0, 2798), (345, 'tt0117878', 'Tere Mere Sapne', '1996', 6.2, 422), (346, 'tt5638500', '1920 London', '2016', 4.1, 1261), (347, 'tt0463345', 'La mujer de mi hermano', '2005', 6.2, 2902), (348, 'tt6613812', 'Angrezi Mein Kehte Hain', '2018', 6.8, 741), (349, 'tt0114142', 'Playback', '1996', 4.0, 291), (350, 'tt4333674', 'Hunterrr', '2015', 7.0, 4936), (351, 'tt4865436', 'Hate Story 3', '2015', 4.5, 2631), (352, 'tt5156746', 'Force 2', '2016', 6.3, 4028), (353, 'tt0315642', 'Wazir', 'I 2016', 7.1, 15105), (354, 'tt2980794', 'Highway', 'I 2014', 7.6, 22497), (355, 'tt0109117', 'Andaz Apna Apna', '1994', 8.2, 44368), (356, 'tt1610452', 'Band Baaja Baaraat', '2010', 7.2, 13191), (357, 'tt0118983', 'Dil To Pagal Hai', '1997', 7.1, 18497), (358, 'tt0102258', 'Lamhe', '1991', 7.4, 2275), (359, 'tt4807830', 'Lipstick Under My Burkha', '2016', 6.9, 5134), (360, 'tt3802576', 'Brothers', 'I 2015', 6.7, 10433), (361, 'tt5472758', 'Happy Bhag Jayegi', '2016', 6.4, 4376), (362, 'tt1572311', 'Tees Maar Khan', '2010', 2.6, 9858), (363, 'tt5472374', 'Meri Pyaari Bindu', '2017', 5.7, 2811), (364, 'tt4874298', 'Madly', '2016', 6.3, 97), (365, 'tt1714832', 'Do Dooni Chaar', '2010', 7.6, 4341), (366, 'tt0106333', 'Baazigar', '1993', 7.8, 22636), (367, 'tt3410408', 'Rocky Handsome', '2016', 6.8, 6143), (368, 'tt2424988', 'Gabbar is Back', '2015', 7.2, 20584), (369, 'tt3696192', 'Singh Is Bliing', '2015', 5.3, 6577), (370, 'tt2181931', 'English Vinglish', '2012', 7.9, 29331), (371, 'tt5743656', 'Phobia', 'I 2016', 6.9, 2184), (372, 'tt0346457', 'Mangal Pandey: The Rising', '2005', 6.7, 8592), (373, 'tt2072227', 'Shanghai', '2012', 7.2, 8859), (374, 'tt6143422', 'Dark wind', '2017', 9.0, 245), (375, 'tt6173826', 'Guest iin London', '2017', 5.4, 1081), (376, 'tt0215132', 'Rocky', '1981', 6.9, 537), (377, 'tt4906960', 'Baadshaho', '2017', 5.0, 3626), (378, 'tt0294662', 'Kaante', '2002', 6.6, 4091), (379, 'tt1411956', 'Sold', '2016', 6.8, 1298), (380, 'tt1230165', 'Rock On!!', '2008', 7.8, 18668), (381, 'tt1428459', 'We Are Family', '2010', 5.6, 2536), (382, 'tt3175038', 'Ek Villain', '2014', 6.5, 14131), (383, 'tt3483712', 'Raja Natwarlal', '2014', 6.1, 3513), (384, 'tt1694542', 'Tanu Weds Manu', '2011', 6.7, 11397), (385, 'tt8223250', 'Bioscopewala', '2018', 7.8, 554), (386, 'tt0100095', 'Maine Pyar Kiya', '1989', 7.4, 9326), (387, 'tt4552546', '3 Storeys', '2018', 7.2, 257), (388, 'tt5742874', 'Sarkar 3', '2017', 4.8, 1550), (389, 'tt0075143', 'Al-risâlah', '1976', 8.9, 5202), (390, 'tt4552486', 'Dobaara: See Your Evil', '2017', 4.1, 279), (391, 'tt0195231', 'Satya', '1998', 8.2, 11186), (392, 'tt7647198', 'Love and Shukla', '2017', 7.3, 425), (393, 'tt5983262', 'Tikli and Laxmi Bomb', '2017', 7.2, 376), (394, 'tt6265988', 'Bhoomi', '2017', 5.4, 1382), (395, 'tt1020937', 'Awarapan', '2007', 7.2, 5456), (396, 'tt5982852', 'Jolly LLB 2', '2017', 7.3, 17945), (397, 'tt0425055', 'Game 6', '2005', 5.9, 2363), (398, 'tt2614722', 'Umrika', '2015', 6.7, 897), (399, 'tt2140465', 'Tanu Weds Manu Returns', '2015', 7.7, 23056), (400, 'tt1043451', 'Delhi-6', '2009', 6.0, 6208), (401, 'tt0451850', 'Paheli', '2005', 6.7, 11357), (402, 'tt1580704', 'That Girl in Yellow Boots', '2010', 6.7, 3144), (403, 'tt0052954', 'Kaagaz Ke Phool', '1959', 8.1, 1698), (404, 'tt3679000', 'Any Body Can Dance 2', '2015', 5.5, 4382), (405, 'tt3483646', 'Bewakoofiyaan', '2014', 5.5, 3206), (406, 'tt5178120', 'The Song of Scorpions', '2017', 6.5, 105), (407, 'tt0066070', 'Mera Naam Joker', '1970', 8.0, 4107), (408, 'tt1266583', 'Mumbai Meri Jaan', '2008', 7.8, 4986), (409, 'tt6791730', 'Tumhari Sulu', '2017', 7.0, 3514), (410, 'tt0758053', 'Saawariya', '2007', 5.4, 6723), (411, 'tt7695014', 'Saheb Biwi Aur Gangster 3', '2018', 4.4, 500), (412, 'tt4271730', 'Alone', 'VI 2015', 3.8, 1403), (413, 'tt1985981', 'Desi Boyz', '2011', 5.9, 11587), (414, 'tt1291465', 'Raajneeti', '2010', 7.1, 14876), (415, 'tt1244093', 'Hisss', '2010', 3.0, 1374), (416, 'tt1836912', 'Shaitan', '2011', 7.3, 7275), (417, 'tt4979082', 'Jai Gangaajal', '2016', 5.7, 2240), (418, 'tt2258337', 'Eega', '2012', 7.8, 18311), (419, 'tt1954206', 'The Dirty Picture', '2011', 6.6, 7727), (420, 'tt1667838', 'I Hate Luv Storys', '2010', 5.6, 6771), (421, 'tt3717068', 'Court', '2014', 7.7, 3086), (422, 'tt0079221', 'Gol Maal', '1979', 8.6, 14778), (423, 'tt4332114', 'Dolly Ki Doli', '2015', 4.6, 2379), (424, 'tt0058547', 'Sangam', 'I 1964', 7.6, 1295), (425, 'tt8484590', 'Teri Bhabhi Hai Pagle', '2018', 5.5, 63), (426, 'tt0800956', 'Life in a Metro', '2007', 7.4, 9447), (427, 'tt0107311', 'Khal Nayak', '1993', 7.2, 2523), (428, 'tt2049630', 'Unfreedom', '2014', 6.2, 1693), (429, 'tt2677064', 'Vishwaroopam 2', '2018', 6.1, 1721), (430, 'tt2476154', 'Grand Masti', '2013', 4.4, 5235), (431, 'tt5235880', 'A Flying Jatt', '2016', 3.4, 2834), (432, 'tt0248185', 'Mission Kashmir', '2000', 6.8, 5686), (433, 'tt1144804', 'Kambakkht Ishq', '2009', 3.9, 6041), (434, 'tt2804026', 'Aurangzeb', '2013', 6.6, 3582), (435, 'tt0795434', 'Namastey London', '2007', 7.3, 19070), (436, 'tt2309764', 'Singham Returns', '2014', 5.7, 6237), (437, 'tt5207116', 'X: Past Is Present', '2015', 5.6, 455), (438, 'tt1949548', 'Heroine', '2012', 5.1, 3594), (439, 'tt4088588', 'Dhanak', '2015', 8.0, 2812), (440, 'tt0405507', 'Murder', '2004', 5.4, 3243), (441, 'tt3495000', 'Mardaani', '2014', 7.3, 9063), (442, 'tt8581230', 'B.A. Pass 2', '2017', 2.2, 184), (443, 'tt3019620', 'Titli', '2014', 7.6, 3403), (444, 'tt0248617', 'Yaadein...', '2001', 4.5, 2906), (445, 'tt6712014', 'Fukrey Returns', '2017', 6.5, 3740), (446, 'tt5943306', 'Freaky Ali', '2016', 5.0, 2286), (447, 'tt5752374', 'Shorgul', '2016', 5.7, 138), (448, 'tt2187153', 'Thuppakki', '2012', 7.9, 20733), (449, 'tt0109206', 'Bandit Queen', '1994', 7.6, 3863), (450, 'tt0470869', \"Neal 'N' Nikki\", '2005', 3.3, 1326), (451, 'tt0432047', 'Sarkar', '2005', 7.7, 13913), (452, 'tt0323013', 'Lakshya', '2004', 7.9, 17299), (453, 'tt1183946', 'The Stoneman Murders', '2009', 7.4, 2028), (454, 'tt2198235', 'Monsoon Shootout', '2013', 6.4, 670), (455, 'tt0150992', 'Hum Dil De Chuke Sanam', '1999', 7.5, 14881), (456, 'tt3742284', 'NH10', '2015', 7.2, 11015), (457, 'tt8427036', 'Helicopter Eela', '2018', 5.0, 273), (458, 'tt1918886', 'Joker', '2012', 2.6, 4446), (459, 'tt1499201', 'Anjaana Anjaani', '2010', 5.8, 5303), (460, 'tt6467738', 'Irada', '2017', 6.3, 937), (461, 'tt0330082', 'Boom', 'I 2003', 2.3, 1236), (462, 'tt2699840', 'John Day', '2013', 5.7, 692), (463, 'tt3836958', 'Shamitabh', '2015', 6.8, 6336), (464, 'tt1918965', 'Murder 2', '2011', 6.2, 5615), (465, 'tt6275262', 'Vodka Diaries', '2018', 5.7, 790), (466, 'tt3017412', 'Happy Ending', '2014', 5.4, 3000), (467, 'tt5764024', 'OK Jaanu', '2017', 5.1, 3087), (468, 'tt3794302', 'Pizza', 'I 2014', 6.2, 776), (469, 'tt3209826', 'Patel Ki Punjabi Shaadi', '2017', 3.8, 243), (470, 'tt4949324', 'Mohalla Assi', '2015', 7.5, 802), (471, 'tt1101665', 'Shaurya: It Takes Courage to Make Right... Right', '2008', 7.3, 3209), (472, 'tt0403935', 'Action Jackson', 'I 2014', 3.3, 2746), (473, 'tt5290620', 'Kyaa Kool Hain Hum 3', '2016', 1.9, 3798), (474, 'tt2423132', 'Akaash Vani', '2013', 6.3, 1485), (475, 'tt0220832', 'Vaastav: The Reality', '1999', 8.0, 10671), (476, 'tt4559046', 'Housefull 3', '2016', 5.2, 6493), (477, 'tt5775220', 'Noor', 'I 2017', 4.0, 1174), (478, 'tt0249371', 'Asoka', '2001', 6.7, 12189), (479, 'tt2372222', 'Kick', '2014', 5.4, 21132), (480, 'tt0382383', 'Yuva', '2004', 7.4, 7540), (481, 'tt3142232', 'Heropanti', '2014', 5.4, 7090), (482, 'tt3678938', 'Humpty Sharma Ki Dulhania', '2014', 6.0, 9482), (483, 'tt6926486', 'Daddy', 'III 2017', 6.4, 1239), (484, 'tt8175968', 'High Jack', '2018', 5.3, 204), (485, 'tt0240200', 'Water', 'I 2005', 7.8, 13515), (486, 'tt0303785', 'Bollywood/Hollywood', '2002', 6.0, 2383), (487, 'tt4990516', 'Tu Hai Mera Sunday', '2016', 7.8, 816), (488, 'tt8698956', 'Lupt', '2018', 6.9, 167), (489, 'tt2224317', 'Lootera', 'I 2013', 7.3, 12953), (490, 'tt0806088', 'Heyy Babyy', '2007', 6.1, 11321), (491, 'tt7172308', \"Kuldip Patwal: I Didn't Do It!\", '2017', 7.6, 343), (492, 'tt6537508', 'Behen Hogi Teri', '2017', 5.4, 1297), (493, 'tt1017456', 'Race', 'I 2008', 6.6, 11209), (494, 'tt8324474', 'Hope Aur Hum', '2018', 6.5, 214), (495, 'tt5020726', 'Silvat', '2018', 8.0, 69), (496, 'tt0346723', 'Chalte Chalte', '2003', 6.6, 13162), (497, 'tt4979110', 'Daas Dev', '2018', 5.1, 146), (498, 'tt1395054', 'Once Upon a Time in Mumbaai', '2010', 7.4, 14114), (499, 'tt0195002', 'Kaun?', '1999', 7.7, 2838), (500, 'tt1825801', 'Hostel', '2011', 5.1, 499), (501, 'tt0139876', 'Baasha', '1995', 8.3, 7010), (502, 'tt5686868', 'Miss Teacher', '2016', 3.1, 161), (503, 'tt4022278', 'Fever', 'I 2016', 4.9, 531), (504, 'tt0248012', 'Fiza', '2000', 6.2, 3365), (505, 'tt5005684', 'Nil Battey Sannata', '2015', 8.5, 4123), (506, 'tt0356982', 'Paanch', '2003', 7.7, 1904), (507, 'tt5227468', 'Beiimaan Love', '2016', 2.9, 319), (508, 'tt0995031', 'Bhool Bhulaiyaa', '2007', 7.3, 17696), (509, 'tt3173910', 'Hasee Toh Phasee', '2014', 6.8, 12145), (510, 'tt6711660', 'Babumoshai Bandookbaaz', '2017', 7.0, 853), (511, 'tt0498351', 'The Hero of Color City', '2014', 4.1, 415), (512, 'tt3142688', 'Finding Fanny', '2014', 5.8, 5768), (513, 'tt0085743', 'Jaane Bhi Do Yaaro', '1983', 8.5, 11052), (514, 'tt5954088', 'Commando 2', '2017', 5.2, 2192), (515, 'tt0318275', 'Adisaya Piravi', '1990', 6.8, 240), (516, 'tt5502766', 'Phillauri', '2017', 6.1, 2950), (517, 'tt0105271', 'Roja', '1992', 8.2, 9764), (518, 'tt0109555', 'Darr', '1993', 7.8, 19832), (519, 'tt2246724', 'Ghanchakkar', '2013', 5.7, 4282), (520, 'tt3483612', 'Hamari Adhuri Kahani', '2015', 6.7, 4465), (521, 'tt3415692', 'Mahabharat', '2013', 5.4, 296), (522, 'tt1230448', 'Billu', '2009', 6.2, 9018), (523, 'tt0104561', 'Jo Jeeta Wohi Sikandar', '1992', 8.2, 19048), (524, 'tt1120897', 'Dhan Dhana Dhan Goal', '2007', 4.9, 2075), (525, 'tt1558578', 'Knock Out', '2010', 6.0, 1764), (526, 'tt0456144', 'Lage Raho Munna Bhai', '2006', 8.1, 36953), (527, 'tt4190220', 'Zed Plus', '2014', 6.4, 288), (528, 'tt0222024', 'Hum Tumhare Hain Sanam', '2002', 5.5, 5755), (529, 'tt0053126', 'North West Frontier', '1959', 7.2, 1867), (530, 'tt0488798', 'Welcome', 'I 2007', 6.8, 14620), (531, 'tt0457802', 'The Blue Umbrella', '2005', 7.6, 1583), (532, 'tt1708453', 'Aakrosh', '2010', 7.0, 2854), (533, 'tt6117702', 'Munna Michael', '2017', 3.4, 2163), (534, 'tt0451919', 'Socha Na Tha', '2005', 7.5, 6495), (535, 'tt1434447', 'Rocket Singh: Salesman of the Year', '2009', 7.5, 15677), (536, 'tt0920464', 'Manorama Six Feet Under', '2007', 7.7, 5459), (537, 'tt2275802', 'Satyagraha', 'I 2013', 6.0, 4463), (538, 'tt0415908', 'Kaal', '2005', 4.6, 4218), (539, 'tt0116308', 'Fire', '1996', 7.3, 5264), (540, 'tt2301155', 'Shootout at Wadala', '2013', 5.9, 5466), (541, 'tt0119861', 'Pardes', '1997', 7.0, 12292), (542, 'tt0418362', 'Mujhse Shaadi Karogi', '2004', 6.8, 11746), (543, 'tt0114234', 'Rangeela', '1995', 7.5, 7835), (544, 'tt1395025', 'Agent Vinod', '2012', 5.1, 6483), (545, 'tt2762334', 'Main Tera Hero', '2014', 5.3, 6427), (546, 'tt2199711', 'Vishwaroopam', '2013', 8.4, 37578), (547, 'tt4007558', 'Shaandaar', '2015', 3.7, 4074), (548, 'tt4744086', 'Ki & Ka', '2016', 5.8, 4572), (549, 'tt1785333', 'Chittagong', '2012', 7.2, 1228), (550, 'tt2912578', 'Ankur Arora Murder Case', '2013', 6.5, 1270), (551, 'tt0464160', 'Chup Chup Ke', '2006', 6.7, 6418), (552, 'tt0418460', 'Aitraaz', '2004', 6.8, 8684), (553, 'tt2551378', 'Bullett Raja', '2013', 5.0, 3388), (554, 'tt2658126', 'Commando', '2013', 6.1, 3720), (555, 'tt1729637', 'Bodyguard', '2011', 4.6, 19770), (556, 'tt1608777', 'LSD: Love, Sex Aur Dhokha', '2010', 7.1, 4697), (557, 'tt1740710', 'Mere Brother Ki Dulhan', '2011', 5.9, 8137), (558, 'tt1182972', 'Bachna Ae Haseeno', '2008', 6.2, 8969), (559, 'tt4023852', 'Welcome to Karachi', '2015', 4.4, 936), (560, 'tt2186933', 'Luv Shuv Tey Chicken Khurana', '2012', 6.6, 3269), (561, 'tt1918927', 'Luv Ka the End', '2011', 4.8, 1230), (562, 'tt0471571', 'Athadu', '2005', 8.3, 9989), (563, 'tt2229842', 'Table No.21', '2013', 7.1, 9964), (564, 'tt0291376', 'Nayak: The Real Hero', '2001', 7.8, 11244), (565, 'tt0072783', 'Chupke Chupke', '1975', 8.4, 9535), (566, 'tt2556308', 'Holiday', '2014', 7.4, 22160), (567, 'tt4675030', 'Great Grand Masti', '2016', 3.7, 1838), (568, 'tt1194232', 'Jaane Kahan Se Aayi Hai', '2010', 4.0, 489), (569, 'tt0476527', 'Bluffmaster!', '2005', 6.7, 5143), (570, 'tt0976026', 'Cheeni Kum', '2007', 6.8, 4400), (571, 'tt1784589', 'Players', '2012', 4.1, 3390), (572, 'tt0306434', 'Aankhen', '2002', 7.6, 10802), (573, 'tt0098999', 'Agneepath', '1990', 7.7, 7208), (574, 'tt0494290', 'Vivah', '2006', 6.6, 7927), (575, 'tt1620719', 'Dabangg', '2010', 6.3, 24537), (576, 'tt1703958', 'Ek Main Aur Ekk Tu', '2012', 5.8, 5891), (577, 'tt4642936', 'Warrior Savitri', '2016', 5.0, 250), (578, 'tt0454396', 'Aryan: Unbreakable', '2006', 4.6, 267), (579, 'tt0109010', '1942: A Love Story', '1994', 7.3, 2900), (580, 'tt1728986', 'Bol Bachchan', '2012', 5.5, 7101), (581, 'tt2321163', 'ABCD Any Body Can Dance', '2013', 6.4, 5596), (582, 'tt1629391', 'Tere Bin Laden', '2010', 7.2, 9618), (583, 'tt1720254', 'Thank You', 'I 2011', 4.7, 4500), (584, 'tt2385104', 'D-Day', 'I 2013', 7.2, 8127), (585, 'tt1183917', 'Teen Patti', '2010', 4.1, 1277), (586, 'tt2344678', 'Himmatwala', '2013', 1.7, 7260), (587, 'tt3380264', 'Kill Dil', '2014', 4.4, 3465), (588, 'tt4300776', 'Mirror Game', '2017', 6.1, 174), (589, 'tt1532957', 'Paa', '2009', 7.2, 10475), (590, 'tt0337578', 'Baghban', '2003', 7.4, 13167), (591, 'tt6997642', 'Ribbon', '2017', 6.9, 256), (592, 'tt1396208', 'Action Replayy', '2010', 4.2, 4802), (593, 'tt1351224', 'Pyaar Impossible!', '2010', 4.7, 2736), (594, 'tt0113526', 'Karan Arjun', '1995', 6.9, 11276), (595, 'tt0086230', 'Sadma', '1983', 8.5, 2587), (596, 'tt0456165', 'Salaam Namaste', '2005', 6.2, 5927), (597, 'tt3619772', 'Hate Story 2', '2014', 4.4, 1812), (598, 'tt2339505', 'Kochadaiiyaan', '2014', 6.5, 4452), (599, 'tt1084972', 'Wanted', '2009', 6.6, 16301), (600, 'tt0444767', 'A Sublime Love Story: Barsaat', '2005', 4.1, 501), (601, 'tt1602476', 'Badmaa$h Company', '2010', 6.0, 5469), (602, 'tt0093578', 'Mr. India', '1987', 7.8, 11558), (603, 'tt3368222', 'Angry Indian Goddesses', '2015', 6.9, 2334), (604, 'tt2807410', 'Rudhramadevi', '2015', 6.1, 2315), (605, 'tt6027478', 'Dhruva', '2016', 8.0, 4470), (606, 'tt5457772', 'Sarrainodu', '2016', 6.4, 2999), (607, 'tt1573072', 'Housefull', '2010', 5.5, 10728), (608, 'tt2271641', 'Thaandavam', '2012', 6.1, 1472), (609, 'tt0419058', 'Phir Hera Pheri', '2006', 6.6, 11489), (610, 'tt1255951', 'De Dana Dan', '2009', 5.3, 5834), (611, 'tt0102825', 'Saajan', '1991', 7.3, 5154), (612, 'tt0497915', 'Gunda', '1998', 7.4, 9232), (613, 'tt0050188', 'Mother India', '1957', 8.1, 7327), (614, 'tt2341766', 'Nautanki Saala!', '2013', 5.8, 3721), (615, 'tt6118058', 'Maroon', '2016', 5.3, 158), (616, 'tt3595298', 'Prem Ratan Dhan Payo', '2015', 4.7, 15331), (617, 'tt0211934', 'Baadshah', '1999', 6.9, 14898), (618, 'tt0299108', 'Rehnaa Hai Terre Dil Mein', '2001', 7.5, 5239), (619, 'tt0419992', 'My Brother... Nikhil', '2005', 7.4, 999), (620, 'tt1992138', 'Force', '2011', 6.4, 6197), (621, 'tt0490210', 'Sarkar Raj', '2008', 6.8, 6625), (622, 'tt0220594', 'Kartoos', '1999', 5.3, 431), (623, 'tt0200087', 'Sarfarosh', '1999', 8.2, 21507), (624, 'tt1223922', 'Blue', 'I 2009', 3.8, 4021), (625, 'tt0196069', 'Samsara', '2001', 7.8, 6899), (626, 'tt1706317', 'Tezz', '2012', 3.9, 1893), (627, 'tt2137241', 'Rock On 2', '2016', 5.0, 1679), (628, 'tt0485272', 'Salaam-E-Ishq', '2007', 5.2, 4791), (629, 'tt1105733', 'Kismat Konnection', '2008', 5.2, 4659), (630, 'tt6958030', '1921', '2018', 4.3, 687), (631, 'tt6201292', 'Bairavaa', '2017', 6.2, 6628), (632, 'tt1714866', \"Midnight's Children\", '2012', 6.2, 2634), (633, 'tt0116950', 'Maachis', '1996', 7.8, 1707), (634, 'tt1884268', 'Chalo Dilli', '2011', 6.8, 2696), (635, 'tt1221139', 'Welcome to Sajjanpur', '2008', 6.9, 2819), (636, 'tt0824375', 'Nishabd', '2007', 5.5, 1436), (637, 'tt2826126', 'Shree', '2013', 6.4, 216), (638, 'tt0337971', 'Deewangee', '2002', 6.6, 2154), (639, 'tt0438875', 'Insan', '2005', 5.0, 1132), (640, 'tt0110222', 'Kabhi Haan Kabhi Naa', '1994', 7.9, 15525), (641, 'tt2211173', 'Yevadu', '2014', 5.4, 5727), (642, 'tt0395802', 'Veergati', '1995', 5.4, 700), (643, 'tt0454435', \"'D'\", '2005', 6.9, 1095), (644, 'tt5209420', 'LoveShhuda', '2016', 4.5, 493), (645, 'tt2222550', '1920: Evil Returns', '2012', 4.9, 1506), (646, 'tt3449292', 'Manjhi: The Mountain Man', '2015', 8.0, 15607), (647, 'tt0110076', 'Hum Aapke Hain Koun...!', '1994', 7.5, 16304), (648, 'tt2932606', 'Siddharth', '2013', 7.3, 546), (649, 'tt6395306', 'Union Leader', '2017', 7.8, 1434), (650, 'tt1239276', 'Rang Rasiya', '2008', 7.1, 1635), (651, 'tt4024944', 'Main Aur Charles', '2015', 6.3, 1444), (652, 'tt0313844', '16 December', '2002', 6.9, 1004), (653, 'tt0246879', 'Qurbani', '1980', 7.1, 723), (654, 'tt2871010', 'Sonali Cable', '2014', 5.6, 766), (655, 'tt0098167', 'Rakhwala', '1989', 5.1, 60), (656, 'tt0137361', 'Anupama', '1966', 7.5, 321), (657, 'tt3409440', 'De surprise', '2015', 6.7, 3259), (658, 'tt0488906', 'Zinda', '2006', 4.9, 2921), (659, 'tt2357926', 'Ramaiya Vastavaiya', '2013', 5.7, 2982), (660, 'tt0061974', 'Milan', '1967', 7.0, 124), (661, 'tt0069671', 'Abhimaan', '1973', 8.0, 2262), (662, 'tt0065764', 'Geet', '1970', 7.3, 83), (663, 'tt2357489', 'Zanjeer', '2013', 3.3, 2079), (664, 'tt0118654', 'Aunty No. 1', '1998', 4.2, 445), (665, 'tt0109922', 'Gopi Kishan', '1994', 5.5, 676), (666, 'tt0391165', 'Fun2shh... Dudes in the 10th Century', '2003', 4.5, 698), (667, 'tt0189592', 'Hum Aapke Dil Mein Rehte Hain', '1999', 5.8, 1328), (668, 'tt0339936', 'Zamane Se Kya Darna', '1994', 5.7, 51), (669, 'tt0266875', 'Raat', '1992', 7.2, 796), (670, 'tt0363666', 'Hatya', '1988', 7.1, 299), (671, 'tt0267905', 'Sangram', '1993', 3.3, 164), (672, 'tt0317811', 'Kasam', '2001', 4.3, 88), (673, 'tt2354407', 'Teri Meri Kahaani', '2012', 4.8, 3565), (674, 'tt0112553', 'Bombay', '1995', 8.1, 9661), (675, 'tt0845448', 'Dhamaal', '2007', 7.2, 10656), (676, 'tt3539966', 'Dishkiyaoon', '2014', 5.3, 880), (677, 'tt0860454', 'MP3: Mera Pehla Pehla Pyaar', '2007', 7.1, 1941), (678, 'tt1708532', 'Ready', '2011', 4.7, 13176), (679, 'tt0172574', 'Hulchul', '1995', 4.7, 365), (680, 'tt0187004', 'Farishtay', '1991', 4.9, 196), (681, 'tt0065936', 'Kati Patang', '1970', 7.2, 745), (682, 'tt0173369', 'Tum Mere Ho', '1990', 5.1, 748), (683, 'tt1805263', 'I Am Kalam', '2010', 8.0, 2992), (684, 'tt0419468', 'Baabul', '2006', 5.5, 2666), (685, 'tt1034449', 'Kidnap', '2008', 4.6, 2064), (686, 'tt3840534', 'Junooniyat', '2016', 4.3, 562), (687, 'tt0210659', 'Dulhe Raja', '1998', 6.6, 1916), (688, 'tt0109091', 'Amaanat', '1994', 5.5, 482), (689, 'tt0326600', 'Maine Dil Tujhko Diya', '2002', 3.8, 961), (690, 'tt1509732', 'Aisha', 'I 2010', 5.0, 3467), (691, 'tt0385782', 'Inteha', '2003', 5.4, 122), (692, 'tt1263679', 'Firaaq', '2008', 7.4, 1700), (693, 'tt0115568', 'Army', '1996', 4.4, 1392), (694, 'tt3036740', 'Humshakals', '2014', 1.8, 6976), (695, 'tt0378072', 'Hum Tum', '2004', 7.0, 12026), (696, 'tt0178186', 'Aandhi', '1975', 8.2, 1167), (697, 'tt0415768', 'Dus', '2005', 5.5, 2712), (698, 'tt1890363', 'Buddha in a Traffic Jam', '2016', 6.9, 1520), (699, 'tt1060249', 'Drona', '2008', 2.1, 2216), (700, 'tt0495032', 'Gangster', '2006', 7.1, 4619), (701, 'tt0118751', 'Border', 'I 1997', 7.9, 11431), (702, 'tt0107060', 'Gumrah', '1993', 6.4, 481), (703, 'tt0490170', 'Love Story 2050', '2008', 2.6, 2080), (704, 'tt4177040', 'Baankey Ki Crazy Baraat', '2015', 6.5, 239), (705, 'tt0382385', 'Zameen', '2003', 5.3, 1239), (706, 'tt0156985', 'Sadak', '1991', 7.1, 1061), (707, 'tt0110438', 'Main Khiladi Tu Anari', '1994', 6.9, 4988), (708, 'tt1391544', 'Dhoondte Reh Jaoge', '2009', 5.6, 569), (709, 'tt0109710', 'Eena Meena Deeka', '1994', 4.6, 266), (710, 'tt0234141', 'Love Story', '1981', 6.5, 333), (711, 'tt0147811', 'Ankhen', '1968', 7.1, 271), (712, 'tt0214841', 'Karz', '1980', 7.5, 1800), (713, 'tt6080914', 'Dora', '2017', 5.4, 581), (714, 'tt0216817', 'Hum Saath-Saath Hain: We Stand United', '1999', 6.2, 5528), (715, 'tt0214665', 'Ek Phool Do Mali', '1969', 6.7, 150), (716, 'tt1754137', 'Little Terrors', '2014', 7.2, 83), (717, 'tt0337652', 'Jeena Sirf Merre Liye', '2002', 4.6, 620), (718, 'tt7274806', 'Kaashi in Search of Ganga', '2018', 7.2, 80), (719, 'tt0050870', 'Pyaasa', '1957', 8.5, 4498), (720, 'tt2369154', 'R... Rajkumar', '2013', 5.2, 4618), (721, 'tt0156043', 'Sanam Teri Kasam', '1982', 6.3, 105), (722, 'tt0172617', 'Izzat Ki Roti', '1993', 5.2, 56), (723, 'tt2939912', 'Traffic', 'I 2016', 6.7, 2254), (724, 'tt0187279', 'Meri Biwi Ka Jawab Nahin', '2004', 4.6, 613), (725, 'tt0886539', 'Luck by Chance', '2009', 7.1, 8646), (726, 'tt4818930', 'Waiting', 'III 2015', 7.2, 1382), (727, 'tt0078241', 'Shalimar', '1978', 6.3, 445), (728, 'tt0305689', 'Jab Jab Phool Khile', '1965', 7.1, 236), (729, 'tt1241195', 'Aamir', '2008', 7.7, 9284), (730, 'tt0072860', 'Deewaar', '1975', 8.1, 8625), (731, 'tt0374271', 'Tere Naam', '2003', 7.1, 14448), (732, 'tt0070253', 'Jugnu', '1973', 6.9, 134), (733, 'tt0379375', 'Matrubhoomi: A Nation Without Women', '2003', 7.7, 1779), (734, 'tt0095440', 'Khatron Ke Khiladi', '1988', 4.8, 149), (735, 'tt1522329', 'Tum Mile', '2009', 5.0, 1232), (736, 'tt0173102', 'Raju Ban Gaya Gentleman', '1992', 6.9, 6568), (737, 'tt3817652', 'Ranna', '2015', 6.3, 455), (738, 'tt1146325', 'Singh Is Kinng', '2008', 5.8, 12616), (739, 'tt5056470', 'Teraa Surroor', '2016', 2.8, 759), (740, 'tt0255212', 'Hamara Dil Aapke Paas Hai', '2000', 5.6, 1578), (741, 'tt0121395', 'Imtihaan', '1994', 5.3, 257), (742, 'tt7742704', 'Meri Nimmo', '2018', 7.2, 197), (743, 'tt1830786', 'Rascals', '2011', 2.9, 2249), (744, 'tt0177473', 'Aa Gale Lag Jaa', '1973', 7.1, 252), (745, 'tt0294264', 'Aalavandhan', '2001', 7.1, 1664), (746, 'tt4302956', 'Kis Kisko Pyaar Karoon', '2015', 5.7, 5067), (747, 'tt2343621', \"Liar's Dice\", '2013', 6.9, 521), (748, 'tt2622130', 'Jayantabhai Ki Luv Story', '2013', 5.3, 1094), (749, 'tt3671294', 'Khilona Bana Khalnayak', '1995', 7.0, 54), (750, 'tt0151206', 'Khoon Bhari Maang', '1988', 6.7, 849), (751, 'tt2308773', 'Ishaqzaade', '2012', 6.6, 5865), (752, 'tt0248135', 'Khudgarz', '1987', 5.9, 239), (753, 'tt0247394', 'Gumnaam', '1965', 7.2, 1067), (754, 'tt1578261', 'Break Ke Baad', '2010', 5.2, 3570), (755, 'tt0246456', 'Bazaar', '1982', 7.3, 355), (756, 'tt2806788', 'Fukrey', '2013', 6.9, 9314), (757, 'tt0284456', 'Samadhi', '1972', 7.5, 85), (758, 'tt0102746', 'Kurbaan', '1991', 4.2, 350), (759, 'tt0164550', 'Duplicate', '1998', 5.5, 6206), (760, 'tt7190460', 'Thupparivaalan', '2017', 7.5, 2474), (761, 'tt1227762', 'Aladin', '2009', 4.7, 2343), (762, 'tt0473310', 'Ram Gopal Varma Ki Aag', '2007', 1.9, 6344), (763, 'tt0049041', 'C.I.D.', '1956', 7.5, 524), (764, 'tt0485522', 'Umrao Jaan', '2006', 5.5, 1723), (765, 'tt4811706', 'Six X', '2016', 6.7, 121), (766, 'tt2106537', 'Matru ki Bijlee ka Mandola', '2013', 5.7, 5690), (767, 'tt3590416', 'Global Baba', '2016', 5.5, 144), (768, 'tt0116153', 'Dushman Duniya Ka', '1996', 4.6, 581), (769, 'tt0255309', 'Kurukshetra', '2000', 6.1, 815), (770, 'tt2675978', 'Dedh Ishqiya', '2014', 7.1, 6432), (771, 'tt0493417', 'Fool N Final', '2007', 3.4, 1637), (772, 'tt0305714', 'Kadhal Desam', '1996', 6.9, 361), (773, 'tt0084667', 'Shakti', '1982', 7.8, 1665), (774, 'tt1042499', 'Filth and Wisdom', '2008', 5.6, 2319), (775, 'tt0326576', 'Humraaz', '2002', 6.3, 2560), (776, 'tt3611266', 'Bank Chor', '2017', 5.9, 1957), (777, 'tt0138411', 'Dushman', '1971', 7.0, 151), (778, 'tt1855268', 'Main Krishna Hoon', '2013', 5.9, 156), (779, 'tt1039995', 'U Me Aur Hum', '2008', 6.0, 2267), (780, 'tt1727496', 'Dil Toh Baccha Hai Ji', '2011', 5.7, 3217), (781, 'tt0084866', 'Vidhaata', '1982', 7.1, 265), (782, 'tt0072100', 'Roti Kapada Aur Makaan', '1974', 7.0, 587), (783, 'tt0105394', 'Shola Aur Shabnam', '1992', 6.7, 651), (784, 'tt7341406', 'Rukh', '2017', 6.5, 316), (785, 'tt3681414', 'Purani Jeans', '2014', 6.1, 639), (786, 'tt0121989', 'Baazi', '1995', 6.6, 1883), (787, 'tt0986213', 'Aaja Nachle', '2007', 6.4, 2560), (788, 'tt0053706', 'Chaudhvin Ka Chand', '1960', 7.2, 333), (789, 'tt0383975', 'Chameli', '2003', 7.1, 1666), (790, 'tt1671446', 'Aadhi Bhagavan', '2013', 5.4, 478), (791, 'tt0268141', 'Ananda Ashram', '1977', 7.2, 94), (792, 'tt0839742', 'Bhopal: A Prayer for Rain', '2014', 7.3, 1803), (793, 'tt0317312', 'Dance Dance', '1987', 6.0, 212), (794, 'tt6058394', 'Anaarkali of Aarah', '2017', 6.5, 1020), (795, 'tt0116763', 'Khamoshi: The Musical', '1996', 7.6, 2238), (796, 'tt1891757', 'Bol', '2011', 8.3, 8090), (797, 'tt0059246', 'Guide', '1965', 8.5, 6049), (798, 'tt4500734', 'Ek Paheli Leela', '2015', 3.8, 1521), (799, 'tt3369670', 'Children of War', '2014', 6.8, 826), (800, 'tt1156148', 'Karzzzz', '2008', 2.2, 1075), (801, 'tt0077451', 'Don', '1978', 7.8, 9554), (802, 'tt2302416', 'Gali Guleiyan', '2017', 7.8, 139), (803, 'tt1334470', 'Raavan', '2010', 5.4, 3778), (804, 'tt0067183', 'Haré Rama Haré Krishna', '1971', 7.3, 677), (805, 'tt0414714', 'Girlfriend', '2004', 3.3, 304), (806, 'tt0210609', 'China Gate', '1998', 6.7, 1144), (807, 'tt0454431', 'Chocolate: Deep Dark Secrets', '2005', 4.8, 1732), (808, 'tt5615116', 'Dear Dad', 'I 2016', 6.4, 231), (809, 'tt1288638', 'Dasvidaniya', '2008', 7.8, 5220), (810, 'tt0111068', 'Sangharsh', '1999', 7.1, 4711), (811, 'tt0154998', 'Pataal Bhairavi', '1985', 5.7, 59), (812, 'tt0073191', 'Jai Santoshi Maa', '1975', 6.8, 92), (813, 'tt0263491', 'Jis Desh Mein Ganga Rehta Hain', '2000', 4.9, 957), (814, 'tt0083578', 'Arth', '1982', 8.0, 1046), (815, 'tt1848771', 'Aarakshan', '2011', 6.2, 4447), (816, 'tt0220757', 'Shool', '1999', 7.7, 2220), (817, 'tt5637188', 'Saat Uchakkey', '2016', 5.7, 550), (818, 'tt0433425', 'Parzania', '2005', 7.7, 1256), (819, 'tt0348656', 'Khushi', 'I 2003', 4.5, 1411), (820, 'tt0082797', 'Naseeb', '1981', 7.1, 1237), (821, 'tt1667823', 'Bhindi Baazaar', '2011', 6.2, 587), (822, 'tt0230055', 'Bulandi', '2000', 5.4, 527), (823, 'tt3302962', 'Shaadi Ke Side Effects', '2014', 5.6, 3501), (824, 'tt0175530', 'Chor Machaye Shor', '1974', 6.6, 104), (825, 'tt5120640', 'Sarbjit', '2016', 7.3, 3544), (826, 'tt0117437', 'Raja Hindustani', '1996', 6.1, 6111), (827, 'tt0083081', 'Silsila', '1981', 7.4, 2242), (828, 'tt0409527', 'Shabd', '2005', 5.2, 1211), (829, 'tt0267130', 'Yodha', '1991', 4.4, 108), (830, 'tt0456481', \"Jaan-E-Mann: Let's Fall in Love... Again\", '2006', 6.4, 7322), (831, 'tt0156891', 'Phir Teri Kahani Yaad Aayee', '1993', 6.0, 194), (832, 'tt2246595', 'Blood Money', 'II 2012', 5.1, 869), (833, 'tt1191130', 'Right Yaaa Wrong', '2010', 6.4, 764), (834, 'tt0290331', 'Waqt Hamara Hai', '1993', 6.2, 1270), (835, 'tt0078222', 'Seeta Aur Geeta', '1972', 6.9, 1409), (836, 'tt0396563', 'Dil Ne Jise Apna Kaha', '2004', 4.5, 1252), (837, 'tt0253194', 'Kunwara', '2000', 5.1, 665), (838, 'tt4603640', 'The Silence', 'III 2015', 7.2, 179), (839, 'tt0090812', 'Chameli Ki Shaadi', '1986', 7.6, 1312), (840, 'tt0453671', 'Garam Masala', '2005', 6.8, 11552), (841, 'tt0215517', 'Angoor', '1982', 8.4, 4196), (842, 'tt0041619', 'Mahal', '1949', 7.1, 310), (843, 'tt0154685', 'Johny Mera Naam', '1970', 7.4, 610), (844, 'tt0443708', 'Page 3', '2005', 7.3, 6123), (845, 'tt0290326', 'Tum Bin...: Love Will Find a Way', '2001', 7.5, 2714), (846, 'tt0222270', 'Phir Bhi Dil Hai Hindustani', '2000', 6.3, 5875), (847, 'tt1082075', 'Pusher', '2010', 6.2, 578), (848, 'tt2404519', '3G - A Killer Connection', '2013', 3.7, 1025), (849, 'tt0286421', 'Achanak', '1998', 4.3, 202), (850, 'tt0107986', 'Roop Ki Rani Choron Ka Raja', '1993', 4.9, 726), (851, 'tt0307873', 'Raaz', '2002', 6.5, 2263), (852, 'tt0330843', 'Saathiya', '2002', 6.9, 6098), (853, 'tt0172175', 'Bhabhi', '1991', 4.6, 104), (854, 'tt0276309', 'Maa', '1976', 6.4, 72), (855, 'tt0105166', 'Prem Deewane', '1992', 5.2, 177), (856, 'tt2073070', 'Saheb Biwi Aur Gangster', '2011', 7.2, 3587), (857, 'tt0499041', 'Kalyug', '2005', 6.5, 2041), (858, 'tt0319020', 'Awara Paagal Deewana', '2002', 6.4, 6786), (859, 'tt6620324', 'Indu Sarkar', '2017', 6.0, 1137), (860, 'tt0089194', 'Geraftaar', '1985', 6.1, 373), (861, 'tt2417560', 'Filmistaan', '2012', 7.3, 3066), (862, 'tt0392360', 'Love', '1991', 6.0, 819), (863, 'tt0172170', 'Bekhudi', '1992', 4.9, 175), (864, 'tt0106653', 'Dalaal', '1993', 6.0, 84), (865, 'tt4683366', 'Ghayal Once Again', '2016', 6.5, 4566), (866, 'tt2188805', 'Login', '2012', 6.3, 84), (867, 'tt0489486', 'Good Boy, Bad Boy', '2007', 3.8, 1457), (868, 'tt0110546', 'Mohra', '1994', 7.1, 6888), (869, 'tt5785200', 'Machine', '2017', 2.7, 552), (870, 'tt2633598', 'Rise of the Zombie', '2013', 4.0, 311), (871, 'tt0246914', 'Sargam', '1979', 6.7, 114), (872, 'tt0154565', 'Haqeeqat', '1964', 7.8, 386), (873, 'tt0116391', 'Gang', '2000', 6.3, 180), (874, 'tt1099196', 'Dil Dosti Etc', '2007', 7.0, 2549), (875, 'tt0104404', 'Heer Ranjha', '1992', 5.4, 110), (876, 'tt0882967', 'Strangers', 'III 2007', 5.1, 315), (877, 'tt0375791', 'Gundaraj', '1995', 4.4, 293), (878, 'tt0178184', 'Aan Milo Sajna', '1970', 6.5, 170), (879, 'tt0346507', 'Andaaz', '2003', 5.6, 2043), (880, 'tt0348187', 'Tejasvini', '1994', 6.7, 59), (881, 'tt1907761', 'Stanley Ka Dabba', '2011', 7.9, 4924), (882, 'tt0111780', 'Yaar Gaddar', '1994', 4.4, 85), (883, 'tt0306855', 'Style', '2001', 6.7, 1525), (884, 'tt6046994', 'Dongri Ka Raja', '2016', 4.3, 118), (885, 'tt2905768', 'Sahasam', '2013', 6.6, 686), (886, 'tt1174041', 'Main Aurr Mrs Khanna', '2009', 3.6, 2371), (887, 'tt0347416', 'LOC: Kargil', '2003', 5.2, 2163), (888, 'tt4262516', 'Sanam Re', '2016', 3.2, 2483), (889, 'tt3615160', 'Gyakusatsu kikan', '2017', 6.4, 461), (890, 'tt0071800', 'Majboor', '1974', 7.2, 544), (891, 'tt0172761', 'Major Saab', '1998', 5.3, 1848), (892, 'tt0084630', 'Satte Pe Satta', '1982', 7.2, 2585), (893, 'tt1455811', 'Khatta Meetha', '2010', 5.8, 8132), (894, 'tt2406676', 'Ek Thi Daayan', '2013', 5.7, 3480), (895, 'tt1754920', 'Yeh Saali Zindagi', '2011', 7.5, 4108), (896, 'tt3716142', 'Lekar Hum Deewana Dil', '2014', 4.9, 642), (897, 'tt1170404', 'Mere Baap Pehle Aap', '2008', 5.3, 1610), (898, 'tt0281102', 'Pyaar Tune Kya Kiya...', '2001', 5.7, 688), (899, 'tt0229193', 'Aastha: In the Prison of Spring', '1997', 6.4, 156), (900, 'tt2215163', 'Issaq', '2013', 4.6, 394), (901, 'tt0444820', 'Fareb', '2005', 4.1, 141), (902, 'tt0995718', 'Ek Chalis Ki Last Local', '2007', 7.3, 4159), (903, 'tt2077833', 'Rowdy Rathore', '2012', 5.8, 18480), (904, 'tt0453729', 'Iqbal', '2005', 8.1, 13954), (905, 'tt1773015', 'Phas Gaye Re Obama', '2010', 7.5, 4463), (906, 'tt0265690', 'Samraat', '1982', 6.0, 119), (907, 'tt0115042', 'Zamaana Deewana', '1995', 5.1, 1741), (908, 'tt0097563', 'Ilaaka', '1989', 4.7, 112), (909, 'tt0823451', 'Dus Kahaniyaan', '2007', 5.8, 1398), (910, 'tt1582466', 'Adhurs', '2010', 6.8, 1945), (911, 'tt1980986', 'Housefull 2', '2012', 5.4, 11406), (912, 'tt0150433', 'Earth', '1998', 7.8, 6797), (913, 'tt2178508', 'Son of Sardaar', '2012', 4.0, 7895), (914, 'tt0075669', 'Amar Akbar Anthony', '1977', 7.5, 5538), (915, 'tt0341562', 'Satta', '2003', 6.9, 480), (916, 'tt0303661', 'Anita & Me', '2002', 6.5, 1547), (917, 'tt1451797', 'Rann', '2010', 7.0, 2085), (918, 'tt0385351', 'Yeh Lamhe Judaai Ke', '2004', 4.3, 1472), (919, 'tt0456500', 'Kyaa Kool Hai Hum', '2005', 6.1, 2821), (920, 'tt0107321', 'King Uncle', '1993', 5.2, 2464), (921, 'tt0349620', 'Inteqam: The Perfect Game', '2004', 5.4, 69), (922, 'tt1188982', 'Jai Ho', 'I 2014', 5.3, 15100), (923, 'tt5785116', 'Begum Jaan', '2017', 5.5, 2185), (924, 'tt0300475', 'Shirdi Ke Sai Baba', '1977', 7.0, 196), (925, 'tt0196635', 'International Khiladi', '1999', 5.0, 1852), (926, 'tt0085886', 'Mahaan', '1983', 6.2, 356), (927, 'tt0833553', 'Ta Ra Rum Pum', '2007', 5.6, 3622), (928, 'tt0121352', 'Henna', '1991', 6.4, 565), (929, 'tt1328634', 'New York', '2009', 6.8, 9370), (930, 'tt0322653', 'Mere Yaar Ki Shaadi Hai', '2002', 5.3, 1891), (931, 'tt0139525', 'Phool Aur Patthar', '1966', 6.6, 185), (932, 'tt1242530', \"What's Your Raashee?\", '2009', 4.7, 2331), (933, 'tt0470614', 'Yun Hota Toh Kya Hota', '2006', 6.9, 551), (934, 'tt3501994', 'Hawaa Hawaai', '2014', 7.4, 1508), (935, 'tt0251356', 'Qahar', '1997', 4.0, 118), (936, 'tt0189492', 'Dushman', '1998', 6.9, 1547), (937, 'tt0284811', 'Aarzoo', '1999', 5.7, 1274), (938, 'tt0486615', 'London Dreams', '2009', 5.4, 3721), (939, 'tt0118957', 'Deewana Mastana', '1997', 6.7, 2021), (940, 'tt0374660', 'Footpath', '2003', 5.4, 670), (941, 'tt6095994', 'Khoj', '2017', 7.0, 340), (942, 'tt0177502', 'Aaj Ki Taaza Khabar', '1973', 7.4, 77), (943, 'tt0295603', 'Shirdi Sai Baba', '2001', 3.5, 161), (944, 'tt0473567', '...Yahaan', '2005', 7.5, 978), (945, 'tt0114031', 'Oh Darling Yeh Hai India', '1995', 4.4, 1390), (946, 'tt0348843', 'Jism', '2003', 5.2, 1863), (947, 'tt0126871', 'Deewana', '1992', 7.0, 5306), (948, 'tt1185412', 'Veer', '2010', 4.7, 6498), (949, 'tt0495034', 'Golmaal: Fun Unlimited', '2006', 7.4, 12222), (950, 'tt1252596', 'Ajab Prem Ki Ghazab Kahani', '2009', 6.3, 10310), (951, 'tt0190419', 'Ghulam', '1998', 7.3, 9734), (952, 'tt0311411', 'Kranti', '2002', 3.8, 318), (953, 'tt2630076', 'Super Nani', '2014', 4.7, 326), (954, 'tt0106221', 'Aadmi Khilona Hai', '1993', 5.2, 100), (955, 'tt4386684', '31st October', '2015', 5.8, 182), (956, 'tt1618430', 'Dum Maaro Dum', '2011', 6.2, 3573), (957, 'tt3107246', 'Irandam Ulagam', '2013', 5.8, 983), (958, 'tt0077609', 'Ghar', '1978', 6.9, 137), (959, 'tt2112131', 'Dabangg 2', '2012', 4.9, 13143), (960, 'tt6734984', 'Duvvada Jagannadham', '2017', 5.4, 2316), (961, 'tt0371735', 'Hungama', '2003', 7.5, 10340), (962, 'tt7549484', 'Firangi', '2017', 4.7, 740), (963, 'tt0102071', 'Hum', '1991', 6.9, 2243), (964, 'tt0369516', 'Hawa', '2003', 3.4, 261), (965, 'tt0061842', 'Jewel Thief', '1967', 7.9, 1460), (966, 'tt7262178', 'Veerey Ki Wedding', '2018', 3.1, 250), (967, 'tt0062177', 'Ram Aur Shyam', '1967', 7.4, 734), (968, 'tt5212160', 'Chhota Bheem Himalayan Adventure', '2016', 6.2, 72), (969, 'tt1727535', 'Rakhta Charitra 2', '2010', 6.4, 2112), (970, 'tt0072777', 'Chhoti Si Baat', '1976', 8.3, 3085), (971, 'tt2094854', 'Inkaar', '2013', 6.1, 2278), (972, 'tt2361148', 'Club 60', '2013', 7.3, 368), (973, 'tt0437238', 'Hulchul', '2004', 6.9, 5264), (974, 'tt0359325', 'Ghazab', '1982', 6.7, 110), (975, 'tt0320097', 'Hum Kisi Se Kum Nahin', '2002', 4.4, 1167), (976, 'tt0339864', 'Vijeta', '1996', 4.2, 55), (977, 'tt0136352', 'Koyla', '1997', 6.3, 7326), (978, 'tt0109134', 'Anjaam', '1994', 7.0, 5185), (979, 'tt3531852', 'Bhoothnath Returns', '2014', 6.8, 4245), (980, 'tt0374184', 'Daai zek lou', '2003', 6.7, 2566), (981, 'tt0340178', 'Jaal: The Trap', '2003', 4.1, 375), (982, 'tt0314557', 'Rajkumar', '1964', 7.0, 121), (983, 'tt0106725', 'Dil Tera Aashiq', '1993', 5.1, 548), (984, 'tt0064506', 'Ittefaq', '1969', 7.4, 733), (985, 'tt1290054', 'Bhupathi', '2007', 6.6, 56), (986, 'tt0154749', 'Kudrat', '1981', 7.1, 223), (987, 'tt3159708', 'Welcome Back', '2015', 4.2, 4921), (988, 'tt4338154', 'Hawaizaada', '2015', 5.6, 1383), (989, 'tt1627924', 'Mausam', '2011', 4.8, 4840), (990, 'tt4117066', 'Meeruthiya Gangsters', '2015', 7.2, 497), (991, 'tt0151466', 'Main Tulsi Tere Aangan Ki', '1978', 7.1, 97), (992, 'tt0155805', 'Khhotte Sikkay', '1974', 6.7, 83), (993, 'tt3524410', 'Yeh Hai Bakrapur', '2014', 3.8, 225), (994, 'tt0811066', 'Shootout at Lokhandwala', '2007', 7.1, 8407), (995, 'tt5600714', 'Laal Rang', '2016', 8.2, 3526), (996, 'tt0255097', 'Deewane', '2000', 4.7, 497), (997, 'tt0071145', 'Ankur', '1974', 8.2, 747), (998, 'tt0074417', 'Do Anjaane', '1976', 6.9, 345), (999, 'tt3563156', 'Anjaan', '2014', 4.9, 3351), (1000, 'tt0133024', 'Ishq', '1997', 6.9, 8820), (1001, 'tt0247909', 'Aaj Ka Goonda Raaj', '1992', 5.5, 119), (1002, 'tt0342624', 'Karz: The Burden of Truth', '2002', 3.9, 329), (1003, 'tt7260848', 'Khajoor Pe Atke', '2018', 5.3, 129), (1004, 'tt2678948', 'Jilla', '2014', 6.1, 8808), (1005, 'tt0180846', 'Tales of the KamaSutra 2: Monsoon', '1999', 3.7, 271), (1006, 'tt5743888', 'Julie 2', '2017', 2.8, 234), (1007, 'tt0991346', 'Bhoothnath', '2008', 6.3, 6435), (1008, 'tt0233226', 'Anokha Bandhan', '1982', 7.2, 64), (1009, 'tt0348078', 'Souten', '1983', 5.6, 159), (1010, 'tt0120456', 'Virasat', '1997', 7.5, 2207), (1011, 'tt0455829', 'Vettaiyaadu Vilaiyaadu', '2006', 8.0, 5506), (1012, 'tt0095936', 'Qayamat Se Qayamat Tak', '1988', 7.6, 9078), (1013, 'tt3419894', 'Sin-ui hansu', '2014', 6.7, 1611), (1014, 'tt1345777', 'Ishqiya', '2010', 7.3, 8884), (1015, 'tt1772872', 'Game', 'I 2011', 5.1, 1726), (1016, 'tt0301215', 'Doosara Aadmi', '1977', 6.8, 95), (1017, 'tt7154994', 'Ranchi Diaries', '2017', 7.2, 64), (1018, 'tt2064849', 'London Paris New York', '2012', 5.7, 3551), (1019, 'tt1415252', 'Agyaat', '2009', 2.9, 513), (1020, 'tt0070434', 'Namak Haraam', '1973', 7.3, 1308), (1021, 'tt0237395', 'Lal Patthar', '1971', 6.8, 82), (1022, 'tt0073104', 'Hera Pheri', '1976', 6.9, 836), (1023, 'tt0364049', 'Adventures of Tarzan', '1985', 5.3, 99), (1024, 'tt0405266', 'Raincoat', '2004', 7.6, 3402), (1025, 'tt0106541', 'Chandra Mukhi', '1993', 3.9, 407), (1026, 'tt0349878', 'Netaji Subhas Chandra Bose: The Forgotten Hero', '2005', 7.5, 832), (1027, 'tt0099869', 'Izzatdaar', '1990', 5.3, 141), (1028, 'tt0118680', 'Banarasi Babu', '1997', 4.5, 174), (1029, 'tt0833561', 'Woh Lamhe', '2006', 6.7, 1053), (1030, 'tt1385824', '13B: Fear Has a New Address', '2009', 7.2, 4528), (1031, 'tt0048613', 'Shree 420', '1955', 8.1, 2183), (1032, 'tt0369637', 'Kuch Naa Kaho', '2003', 5.5, 2080), (1033, 'tt0386650', 'Mounam Pesiyadhe', '2002', 7.7, 1256), (1034, 'tt0091256', 'Ijaazat', '1987', 8.3, 1116), (1035, 'tt0357328', 'Yeh Dil', '2003', 4.2, 168), (1036, 'tt0363567', 'Dilwaala', '1986', 3.7, 1127), (1037, 'tt2876408', 'Mr Joe B. Carvalho', '2014', 3.7, 431), (1038, 'tt0050665', 'Madhumati', '1958', 8.1, 1005), (1039, 'tt1083988', 'Journey Bombay to Goa: Laughter Unlimited', '2007', 4.4, 281), (1040, 'tt2629322', 'Sri Siddhartha Gautama', '2013', 7.1, 172), (1041, 'tt0061073', 'Teesri Manzil', '1966', 7.7, 908), (1042, 'tt0060306', 'Devar', '1966', 7.4, 61), (1043, 'tt0411469', 'Hazaaron Khwaishein Aisi', '2003', 8.0, 4263), (1044, 'tt1392744', 'Chance Pe Dance', '2010', 4.7, 2437), (1045, 'tt2255934', 'Aiyyaa', '2012', 4.4, 1487), (1046, 'tt6080746', 'Raag Desh', '2017', 8.2, 329), (1047, 'tt0337613', 'Divya Shakti', '1993', 4.0, 163), (1048, 'tt0266757', 'Mehboob Ki Mehndi', '1971', 7.1, 100), (1049, 'tt0216707', 'Dil Kya Kare', '1999', 5.2, 958), (1050, 'tt1182884', 'Dulha Mil Gaya', '2010', 4.4, 2233), (1051, 'tt0041161', 'Barsaat', '1949', 7.9, 459), (1052, 'tt0059893', 'Waqt', '1965', 7.8, 944), (1053, 'tt0287111', 'Wajood', '1998', 6.8, 455), (1054, 'tt0214832', 'Kalicharan', '1976', 6.6, 212), (1055, 'tt5638474', 'Wajah Tum Ho', '2016', 4.4, 944), (1056, 'tt0255111', 'Dhadkan', '2000', 6.8, 6041), (1057, 'tt0244528', 'Hadh Kar Di Aapne', '2000', 5.3, 1395), (1058, 'tt1465493', 'Hum Tum Aur Ghost', '2010', 4.8, 582), (1059, 'tt7028460', 'Teen Aur Aadha', '2018', 8.4, 104), (1060, 'tt1637691', 'Khelein Hum Jee Jaan Sey', '2010', 5.9, 1101), (1061, 'tt0102705', 'Prem Pratigyaa', '1989', 5.9, 177), (1062, 'tt0094979', 'The Deceivers', '1988', 6.1, 847), (1063, 'tt1808221', 'Kaalo', '2010', 3.1, 186), (1064, 'tt0250690', 'Refugee', '2000', 5.5, 1685), (1065, 'tt0384491', 'Samay: When Time Strikes', '2003', 6.9, 1433), (1066, 'tt1322257', 'Jhootha Hi Sahi', '2010', 6.2, 1661), (1067, 'tt0232960', 'Woh Kaun Thi?', '1964', 7.7, 674), (1068, 'tt0117983', 'Tu Chor Main Sipahi', '1996', 5.5, 1153), (1069, 'tt1773109', 'Yamla Pagla Deewana', '2011', 5.7, 3575), (1070, 'tt1159926', 'City of Life', '2009', 7.1, 1257), (1071, 'tt4512230', 'Hey Bro', '2015', 4.8, 98), (1072, 'tt5310090', 'Saala Khadoos', '2016', 7.6, 9734), (1073, 'tt0284137', 'Gadar: Ek Prem Katha', '2001', 7.1, 8927), (1074, 'tt1562859', 'Golmaal 3', '2010', 5.4, 6358), (1075, 'tt0104607', 'Khuda Gawah', '1992', 6.6, 1571), (1076, 'tt4621100', 'Nanak Shah Fakir', '2014', 8.8, 223), (1077, 'tt2392447', 'Ungli', '2014', 5.8, 3089), (1078, 'tt0416120', 'Police Force: An Inside Story', '2004', 5.2, 679), (1079, 'tt0061960', 'Pretty Polly', '1967', 6.7, 127), (1080, 'tt1629295', 'I Am', 'II 2010', 7.0, 686), (1081, 'tt0430702', 'Tumsa Nahin Dekha', '2004', 4.7, 594), (1082, 'tt0435079', 'Jangal Mein Mangal', '1972', 6.9, 52), (1083, 'tt0049378', 'Jagte Raho', '1956', 8.4, 751), (1084, 'tt0068305', 'Bombay to Goa', '1972', 6.9, 908), (1085, 'tt0297416', 'Superman', '1987', 4.2, 227), (1086, 'tt0054910', 'Gunga Jumna', '1961', 7.6, 432), (1087, 'tt1512321', 'Vaada Raha... I Promise', '2009', 6.1, 290), (1088, 'tt9007142', 'The Dark Side of Life: Mumbai City', '2018', 8.2, 499), (1089, 'tt0319736', 'The Legend of Bhagat Singh', '2002', 8.1, 12508), (1090, 'tt1629715', 'Miss Lovely', '2012', 6.6, 2655), (1091, 'tt0119428', 'Judwaa', '1997', 6.0, 4461), (1092, 'tt0289685', 'Daava', '1997', 5.3, 515), (1093, 'tt2814372', 'Gippi', '2013', 5.6, 709), (1094, 'tt2727028', 'Total Siyapaa', '2014', 5.0, 2658), (1095, 'tt1370429', '99', 'I 2009', 7.3, 2420), (1096, 'tt1664809', 'Rakhta Charitra', '2010', 7.6, 3361), (1097, 'tt0813996', 'Alag: He Is Different.... He Is Alone...', '2006', 4.1, 1048), (1098, 'tt0371922', 'Saaya', '2003', 5.8, 785), (1099, 'tt1324078', 'Kal Kissne Dekha', '2009', 3.6, 351), (1100, 'tt0205445', 'Tamanna', '1998', 7.1, 243), (1101, 'tt0314006', 'Dum', '2003', 5.6, 1427), (1102, 'tt2796318', 'Mickey Virus', '2013', 6.1, 2395), (1103, 'tt3636776', 'Samrat & Co.', '2014', 6.4, 940), (1104, 'tt3547616', 'All Is Well', '2015', 3.8, 1360), (1105, 'tt0232079', 'Mela', '2000', 3.8, 3559), (1106, 'tt2166214', 'Khiladi 786', '2012', 4.4, 5496), (1107, 'tt7560484', 'Gauru: Journey of Courage', '2016', 8.8, 62), (1108, 'tt2064816', 'I, Me aur Main', '2013', 4.6, 1656), (1109, 'tt0291883', 'Ek Ladka Ek Ladki', '1992', 5.8, 785), (1110, 'tt0946999', 'Deadline: Sirf 24 Ghante', '2006', 6.4, 386), (1111, 'tt0056436', 'Sahib Bibi Aur Ghulam', '1962', 8.4, 1304), (1112, 'tt0260494', 'Veerana', '1988', 6.7, 535), (1113, 'tt4323504', 'Jazbaa', '2015', 5.8, 2921), (1114, 'tt4906984', 'Azhar', '2016', 5.8, 4687), (1115, 'tt0429289', 'Tauba Tauba', '2004', 2.7, 82), (1116, 'tt0311413', 'Krodh', '2000', 4.3, 167), (1117, 'tt0326805', 'Chori Chori', '2003', 4.9, 752), (1118, 'tt0286752', 'Keemat: They Are Back', '1998', 6.1, 1015), (1119, 'tt0105866', 'Yalgaar', '1992', 5.4, 326), (1120, 'tt3776484', 'Zubaan', '2015', 6.2, 230), (1121, 'tt8932884', 'Baarish Aur Chowmein', '2018', 6.8, 99), (1122, 'tt1092005', 'Golmaal Returns', '2008', 5.0, 6050), (1123, 'tt0348824', 'Chalo Ishq Ladaaye', '2002', 4.4, 440), (1124, 'tt0251320', 'Pehchaan', '1993', 4.5, 116), (1125, 'tt0064241', 'Do Raaste', '1969', 7.0, 196), (1126, 'tt0233264', 'Avtaar', '1983', 7.2, 447), (1127, 'tt0114615', 'Taqdeerwala', '1995', 6.6, 280), (1128, 'tt0060689', 'Mera Saaya', '1966', 7.7, 705), (1129, 'tt0422950', 'Phir Milenge', '2004', 6.2, 1484), (1130, 'tt0983990', '1971', '2007', 8.0, 1080), (1131, 'tt0343850', 'Jai Vikraanta', '1995', 5.4, 69), (1132, 'tt0347332', 'Khakee', '2004', 7.5, 13533), (1133, 'tt5513098', 'Autohead', '2016', 6.1, 251), (1134, 'tt6642396', 'Jattu Engineer', '2017', 6.2, 3294), (1135, 'tt0152256', 'Saagar', '1985', 7.2, 750), (1136, 'tt4501576', 'Dirty Politics', '2015', 2.6, 524), (1137, 'tt0476550', 'God Tussi Great Ho', '2008', 3.6, 3080), (1138, 'tt0152278', 'Sanam', '1997', 4.9, 62), (1139, 'tt0335044', 'Dil Vil Pyar Vyar', '2002', 5.3, 402), (1140, 'tt1666308', 'Golimar', '2010', 5.9, 370), (1141, 'tt0128985', 'Chalti Ka Naam Gaadi', '1958', 8.0, 1165), (1142, 'tt1828289', 'Shagird', '2011', 7.0, 1561), (1143, 'tt2571140', 'Boss', 'I 2013', 5.5, 9364), (1144, 'tt0119285', 'Hero No. 1', '1997', 6.1, 2008), (1145, 'tt0807758', 'Partner', '2007', 5.7, 9674), (1146, 'tt1179781', 'Mission Istaanbul: Darr Ke Aagey Jeet Hai!', '2008', 3.4, 923), (1147, 'tt5805252', 'Budhia Singh: Born to Run', '2016', 7.8, 1087), (1148, 'tt3886444', 'Khamoshiyan', '2015', 4.1, 1031), (1149, 'tt5784860', 'Aksar 2', '2017', 4.1, 333), (1150, 'tt2429640', 'Murder 3', '2013', 4.9, 2101), (1151, 'tt0102636', 'Parinda', '1989', 7.9, 2670), (1152, 'tt1373156', 'Karthik Calling Karthik', '2010', 7.0, 9008), (1153, 'tt0383719', 'Xcuse Me', '2003', 5.7, 629), (1154, 'tt0058257', 'Kashmir Ki Kali', '1964', 7.1, 395), (1155, 'tt0059354', 'Khandan', '1965', 6.8, 121), (1156, 'tt0243472', 'Paap Ki Duniya', '1988', 5.7, 76), (1157, 'tt1916728', 'Shor in the City', '2010', 7.3, 3114), (1158, 'tt0422236', 'Fida', '2004', 5.4, 2118), (1159, 'tt4187650', \"Trip to Bhangarh: Asia's Most Haunted Place\", '2014', 2.6, 133), (1160, 'tt1606267', 'Lafangey Parindey', '2010', 5.3, 1801), (1161, 'tt1372266', 'Kisaan', '2009', 5.5, 235), (1162, 'tt0110140', 'Insaniyat', '1994', 3.3, 222), (1163, 'tt3422462', 'Bangistan', '2015', 4.6, 962), (1164, 'tt0345177', 'Ek Doctor Ki Maut', '1990', 8.4, 963), (1165, 'tt0872190', 'Cash', '2007', 3.5, 1635), (1166, 'tt0110254', 'Khuddar', '1994', 5.3, 247), (1167, 'tt1999935', 'Not a Love Story', '2011', 5.0, 660), (1168, 'tt0288427', 'Angaaray', '1998', 5.9, 708), (1169, 'tt0205380', 'Sanam Teri Kasam', '2009', 5.1, 107), (1170, 'tt3683702', 'Fugly', '2014', 4.7, 911), (1171, 'tt0485243', 'Jodi', '1999', 6.2, 137), (1172, 'tt0397742', 'Yeh Vaada Raha', '1982', 7.1, 220), (1173, 'tt0168529', 'Bombay Boys', '1998', 6.9, 529), (1174, 'tt0805184', 'Bhagam Bhag', '2006', 6.4, 8563), (1175, 'tt0459605', 'Eklavya', '2007', 6.2, 2916), (1176, 'tt0265148', 'Main Prem Ki Diwani Hoon', '2003', 4.0, 3723), (1177, 'tt2978626', \"It's Entertainment\", '2014', 4.7, 6521), (1178, 'tt2988272', 'Shuddh Desi Romance', '2013', 5.8, 5481), (1179, 'tt3309662', 'Jackpot', 'II 2013', 2.2, 635), (1180, 'tt0105515', 'Tadipaar', '1993', 6.0, 99), (1181, 'tt0445022', 'Jurm', '2005', 5.6, 386), (1182, 'tt0151511', 'Mausam', '1975', 8.1, 741), (1183, 'tt4979160', 'Ishq Forever', '2016', 3.9, 82), (1184, 'tt0266439', 'Dacait', '1987', 6.7, 207), (1185, 'tt0101244', '100 Days', '1991', 6.5, 762), (1186, 'tt0354922', 'Snegithiye', '2000', 6.8, 217), (1187, 'tt2358412', 'David', 'I 2013', 6.3, 2340), (1188, 'tt0272688', 'Kasoor', '2001', 6.1, 758), (1189, 'tt0055783', 'Bees Saal Baad', '1962', 7.1, 279), (1190, 'tt1937092', 'Always Kabhi Kabhi', '2011', 4.3, 1202), (1191, 'tt0061046', 'Suraj', '1966', 7.2, 96), (1192, 'tt6296236', 'Running Shaadi', '2017', 6.8, 2189), (1193, 'tt2615584', 'Phata Poster Nikhla Hero', '2013', 4.9, 3741), (1194, 'tt3848938', 'Mr. X', '2015', 3.8, 1483), (1195, 'tt1841542', 'Chillar Party', '2011', 7.5, 5517), (1196, 'tt2323964', 'Mere Dad Ki Maruti', '2013', 6.5, 4135), (1197, 'tt0451631', 'Apaharan', '2005', 7.4, 3667), (1198, 'tt2406636', 'Bhoot Returns', '2012', 2.5, 458), (1199, 'tt0995827', 'The Train: Some Lines Should Never Be Crossed...', '2007', 4.3, 890), (1200, 'tt0954914', 'Athidhi', '2007', 5.7, 1526), (1201, 'tt0114236', 'Ravan Raaj: A True Story', '1995', 7.3, 83), (1202, 'tt0278522', 'Jodi No.1', '2001', 5.9, 1880), (1203, 'tt0099652', 'Ghayal', '1990', 7.7, 3386), (1204, 'tt2510874', 'Yamla Pagla Deewana 2', '2013', 3.5, 1576), (1205, 'tt5125414', 'Amazon Obhijaan', '2017', 5.0, 880), (1206, 'tt0187188', 'Johnny', '1980', 7.3, 297), (1207, 'tt0366985', 'Rudraksh', '2004', 2.7, 960), (1208, 'tt0102645', 'Patthar Ke Phool', '1991', 5.7, 691), (1209, 'tt0833484', 'The Killer', '2006', 4.6, 887), (1210, 'tt1890513', 'Ragini MMS', '2011', 5.0, 2190), (1211, 'tt0995752', 'Tashan', '2008', 3.9, 4396), (1212, 'tt0373856', 'Gangaajal', '2003', 7.9, 13314), (1213, 'tt0244585', 'Khubsoorat', '1980', 7.7, 1264), (1214, 'tt0289845', 'Aks', '2001', 5.9, 1473), (1215, 'tt0239495', 'Jaal', '1986', 5.7, 69), (1216, 'tt0457875', 'Ek Ajnabee', '2005', 5.3, 1441), (1217, 'tt0443331', 'Milenge Milenge', '2010', 4.1, 1530), (1218, 'tt0087975', 'Raaj Tilak', '1984', 6.7, 99), (1219, 'tt0301231', 'Duniya', '1984', 7.0, 102), (1220, 'tt0172519', 'Guddu', '1995', 4.5, 2019), (1221, 'tt2082436', 'Sultanat', '2014', 6.3, 135), (1222, 'tt1039989', 'Sunday', 'I 2008', 5.3, 2257), (1223, 'tt0248428', 'Shiva', '1989', 8.1, 2540), (1224, 'tt0216110', 'Pratiggya', '1975', 7.3, 217), (1225, 'tt0089413', 'Khamosh', '1986', 7.5, 794), (1226, 'tt1126516', 'Money Hai Toh Honey Hai', '2008', 2.8, 489), (1227, 'tt0979891', 'Hattrick', '2007', 4.2, 386), (1228, 'tt2319889', 'Jannat 2', '2012', 6.1, 4578), (1229, 'tt3803860', 'Puli', '2015', 4.7, 6631), (1230, 'tt0233422', 'Chachi 420', '1997', 7.4, 10925), (1231, 'tt0114231', 'Ram Jaane', '1995', 5.5, 3285), (1232, 'tt5886216', 'Veeram', '2017', 6.8, 181), (1233, 'tt0349703', 'Kyun! Ho Gaya Na...', '2004', 4.3, 1579), (1234, 'tt0320736', 'Vikram', 'I 1986', 7.3, 373), (1235, 'tt0488381', 'Corporate', '2006', 6.5, 1757), (1236, 'tt0101732', 'Dil Aashna Hai ...The Heart Knows', '1992', 5.1, 1354), (1237, 'tt0331256', 'Gunaah', '2002', 3.4, 311), (1238, 'tt0155985', 'Pyari Behna', '1985', 6.6, 52), (1239, 'tt0329393', 'Mr. and Mrs. Iyer', '2002', 8.0, 4445), (1240, 'tt0201840', 'Phörpa', '1999', 6.9, 2755), (1241, 'tt0770214', 'Kabul Express', '2006', 6.8, 2828), (1242, 'tt0361772', 'Jeete Hain Shaan Se', '1988', 4.7, 102), (1243, 'tt0147925', 'Brahmachari', 'I 1968', 6.9, 307), (1244, 'tt2573750', 'Gulaab Gang', '2014', 6.1, 1924), (1245, 'tt0067164', 'Guddi', '1971', 7.4, 820), (1246, 'tt0230347', 'Jaanwar', '1999', 6.3, 2219), (1247, 'tt5639388', 'Raaz Reboot', '2016', 4.5, 1434), (1248, 'tt5784852', 'Tera Intezaar', '2017', 1.7, 135), (1249, 'tt2404027', 'Arjun: The Warrior Prince', '2012', 6.9, 1225), (1250, 'tt0292740', 'Yeh Raaste Hain Pyaar Ke', '2001', 4.1, 574), (1251, 'tt0277981', 'Rahul', '2001', 5.4, 212), (1252, 'tt0046799', 'Boot Polish', '1954', 8.1, 623), (1253, 'tt6499258', 'Wedding Anniversary', 'I 2017', 5.4, 52), (1254, 'tt0108109', 'Shatranj', '1993', 6.5, 77), (1255, 'tt1229390', 'Shortkut - The Con Is On', '2009', 3.4, 771), (1256, 'tt5909308', 'Days of Tafree', '2016', 7.7, 208), (1257, 'tt5978194', 'Bhouri', '2016', 6.9, 241), (1258, 'tt0206921', 'Mann', '1999', 6.3, 4223), (1259, 'tt0421277', 'Julie', '2004', 4.2, 348), (1260, 'tt2909396', 'Santa Banta Pvt Ltd', '2016', 3.0, 184), (1261, 'tt0108001', 'Rudaali', '1993', 7.2, 399), (1262, 'tt0422689', 'Madurey', '2004', 4.6, 1180), (1263, 'tt4045666', 'Mumbai Delhi Mumbai', '2014', 5.9, 280), (1264, 'tt0172980', 'Pehla Nasha', '1993', 5.2, 284), (1265, 'tt2621000', 'Jolly LLB', '2013', 7.5, 13756), (1266, 'tt0451803', 'Maine Pyaar Kyun Kiya', '2005', 5.6, 3946), (1267, 'tt1535467', 'Patiala House', '2011', 5.9, 8044), (1268, 'tt0152836', 'Taal', '1999', 6.8, 4469), (1269, 'tt0347278', 'Jhankaar Beats', '2003', 7.2, 1733), (1270, 'tt3038772', 'Kaanchi', '2014', 4.5, 521), (1271, 'tt0245825', 'Chal Mere Bhai', '2000', 4.7, 2481), (1272, 'tt0083820', 'Desh Premee', '1982', 6.0, 464), (1273, 'tt2186731', 'Department', '2012', 3.3, 872), (1274, 'tt2208248', 'Machhli Jal Ki Rani Hai', '2014', 4.3, 99), (1275, 'tt0347779', 'Pinjar: Beyond Boundaries...', '2003', 8.1, 2283), (1276, 'tt0098461', 'Thanedaar', '1990', 5.7, 321), (1277, 'tt1492872', 'Hide & Seek', '2010', 4.1, 166), (1278, 'tt0246009', 'Kondaveeti Raja', '1986', 7.1, 289), (1279, 'tt0292742', 'Zakhmi Dil', '1994', 5.2, 419), (1280, 'tt1725795', '7 Aum Arivu', '2011', 6.1, 8678), (1281, 'tt4596814', 'S/O Satyamurthy', '2015', 6.8, 2321), (1282, 'tt0397882', 'Ab Tumhare Hawale Watan Saathiyo', '2004', 4.8, 1252), (1283, 'tt0112313', 'Akele Hum Akele Tum', '1995', 7.1, 4045), (1284, 'tt5370442', 'Balu Mahi', '2017', 6.6, 448), (1285, 'tt1245774', 'Jail', '2009', 6.2, 1058), (1286, 'tt1283956', 'Student No. 1', '2001', 6.8, 1364), (1287, 'tt0239235', 'Anji', '2004', 5.7, 405), (1288, 'tt1600439', 'Shaapit: The Cursed', '2010', 5.6, 645), (1289, 'tt0408837', 'Hawas', '2004', 2.7, 190), (1290, 'tt0209263', 'Pyaar Koi Khel Nahin', '1999', 3.4, 318), (1291, 'tt0213753', 'Kadhalar Dinam', '1999', 6.2, 474), (1292, 'tt0251194', 'Mahaanta: The Film', '1997', 4.6, 134), (1293, 'tt0211126', 'Zakhm', '1998', 8.0, 2552), (1294, 'tt0477252', '36 China Town', '2006', 5.6, 3632), (1295, 'tt0907674', 'Amal', '2007', 7.6, 1748), (1296, 'tt0267363', 'Chandni Bar', '2001', 7.6, 2196), (1297, 'tt0091284', 'Janbaaz', '1986', 6.2, 578), (1298, 'tt0081401', 'Ram Balram', 'I 1980', 6.4, 400), (1299, 'tt0361436', 'Chhota Chetan', '1998', 5.8, 110), (1300, 'tt3822600', 'Amit Sahni Ki List', '2014', 5.5, 323), (1301, 'tt0157316', 'Ankush', '1986', 7.6, 351), (1302, 'tt0892874', 'Halla Bol', '2008', 6.1, 1962), (1303, 'tt6352548', 'Poster Boys', '2017', 5.6, 1183), (1304, 'tt0271748', 'Raju Chacha', '2000', 5.1, 1607), (1305, 'tt0187271', 'Masterji', '1985', 6.6, 64), (1306, 'tt0061650', 'Farz', '1967', 6.4, 78), (1307, 'tt0251756', 'Jungle', '2000', 6.2, 954), (1308, 'tt0086925', 'Baazi', '1984', 6.4, 51), (1309, 'tt0857381', 'I See You', '2006', 4.3, 1044), (1310, 'tt4287778', 'Shab', '2017', 4.4, 175), (1311, 'tt2122340', 'Ferrari Ki Sawaari', '2012', 6.4, 4516), (1312, 'tt0135140', 'Anth', '1994', 5.5, 90), (1313, 'tt0094299', 'Watan Ke Rakhwale', '1987', 6.1, 87), (1314, 'tt0349333', 'Darna Mana Hai', '2003', 6.3, 2544), (1315, 'tt1343362', 'Do Knot Disturb', '2009', 3.7, 956), (1316, 'tt0189633', 'Kareeb', '1998', 6.6, 946), (1317, 'tt0290820', 'Raakh', '1989', 7.7, 946), (1318, 'tt0443594', 'Naina', '2005', 4.3, 704), (1319, 'tt0286707', 'Humse Badhkar Kaun: The Entertainer', '1998', 3.9, 94), (1320, 'tt0376080', 'Platform', '1993', 3.9, 156), (1321, 'tt0341549', 'Rishtey', '2002', 4.6, 750), (1322, 'tt5313980', 'Tere Bin Laden: Dead Or Alive', '2016', 4.3, 745), (1323, 'tt0107777', 'Parampara', '1993', 5.9, 896), (1324, 'tt1242782', 'Luck', 'I 2009', 4.8, 2041), (1325, 'tt0137100', 'Maya', '1993', 5.7, 1884), (1326, 'tt0122427', 'Chaalbaaz', '1989', 6.8, 1203), (1327, 'tt5717110', 'Tum Bin 2', '2016', 4.9, 959), (1328, 'tt3382148', 'Yaariyan', '2014', 2.7, 2786), (1329, 'tt0043307', 'Baazi', '1951', 7.4, 261), (1330, 'tt0341455', 'Makdee', '2002', 7.4, 1417), (1331, 'tt0454429', 'Chingaari', '2006', 4.6, 159), (1332, 'tt0399942', 'Paap', '2003', 5.2, 703), (1333, 'tt3645014', 'The Xpose', '2014', 3.3, 1387), (1334, 'tt1014672', 'Bheja Fry', '2007', 7.7, 9894), (1335, 'tt0476884', 'Taxi No. 9 2 11: Nau Do Gyarah', '2006', 7.2, 8016), (1336, 'tt0298607', 'Yeh Zindagi Ka Safar', '2001', 3.3, 122), (1337, 'tt0391079', 'Dil Pardesi Ho Gayaa', '2003', 4.8, 71), (1338, 'tt0252358', 'Darwaza', '1978', 5.9, 69), (1339, 'tt0157339', 'Baton Baton Mein', '1979', 7.6, 854), (1340, 'tt0158534', 'The Burning Train', '1980', 6.8, 1133), (1341, 'tt0363472', 'Bewafaa', '2005', 4.6, 2119), (1342, 'tt0101733', 'Dil Hai Ki Manta Nahin', '1991', 7.4, 3420), (1343, 'tt6078866', 'Soni', '2018', 7.7, 53), (1344, 'tt0084532', 'Prem Rog', '1982', 7.2, 704), (1345, 'tt1447508', 'Peepli Live', '2010', 7.4, 10627), (1346, 'tt0155074', 'Pyar Ka Mandir', '1988', 6.5, 73), (1347, 'tt0295682', 'The Warrior', '2001', 6.8, 2300), (1348, 'tt2389974', 'Aatma', '2013', 3.7, 533), (1349, 'tt3290688', 'Revolver Rani', '2014', 4.8, 1418), (1350, 'tt0331479', 'Ek Chhotisi Love Story', '2002', 2.9, 516), (1351, 'tt0155088', 'Rajput', '1982', 6.5, 118), (1352, 'tt2375567', 'Super Model', '2013', 2.0, 140), (1353, 'tt0060104', 'Amrapali', '1966', 6.7, 140), (1354, 'tt0175423', 'Aap Ki Kasam', '1974', 7.0, 334), (1355, 'tt0208903', 'Disco Dancer', '1982', 6.4, 998), (1356, 'tt0172234', 'Chaahat', '1996', 5.6, 2321), (1357, 'tt5458088', 'Kammatti Paadam', '2016', 8.1, 2716), (1358, 'tt4541102', 'Uvaa', '2015', 5.0, 120), (1359, 'tt2691010', 'Mumbai 125 KM 3D', '2014', 2.8, 155), (1360, 'tt1090650', 'Dhokha', '2007', 6.3, 388), (1361, 'tt0111384', 'Teesra Kaun?', '1994', 5.4, 100), (1362, 'tt1485072', 'Teree Sang: A Kidult Love Story', '2009', 6.2, 502), (1363, 'tt0053637', 'Barsaat Ki Raat', '1960', 7.0, 104), (1364, 'tt0453978', 'Yakeen', '2005', 5.2, 614), (1365, 'tt0485513', 'Main, Meri Patni... Aur Woh!', '2005', 7.3, 1671), (1366, 'tt0444849', 'Karam', '2005', 5.6, 1162), (1367, 'tt0225515', 'Dillagi', '1999', 5.4, 879), (1368, 'tt0102119', 'Indrajeet', '1991', 4.7, 219), (1369, 'tt0361505', 'Dharmatma', '1975', 6.7, 230), (1370, 'tt0060659', 'Mamta', '1966', 7.2, 92), (1371, 'tt0824316', 'Dor', '2006', 8.0, 4969), (1372, 'tt0148375', 'Khilona', '1970', 7.2, 264), (1373, 'tt4358344', 'Rough Book', '2016', 7.5, 123), (1374, 'tt5791986', 'III Smoking Barrels', '2017', 8.7, 83), (1375, 'tt0843328', 'Chatrapathi', '2005', 7.6, 3339), (1376, 'tt1202540', 'Dil Bole Hadippa!', '2009', 4.6, 3340), (1377, 'tt1216300', 'Jannat: In Search of Heaven...', '2008', 6.9, 4856), (1378, 'tt0096827', 'Apoorva Sagodharargal', '1989', 8.3, 2570), (1379, 'tt0214866', 'Krodhi', '1981', 6.6, 130), (1380, 'tt0328671', 'Yeh Hai Jalwa', '2002', 4.8, 1861), (1381, 'tt0240084', 'Thammudu', '1999', 7.8, 2083), (1382, 'tt0156659', 'Jurm', '1990', 6.2, 87), (1383, 'tt0260077', 'Hotel', '1981', 6.0, 76), (1384, 'tt1158700', 'Desh Drohi', '2008', 1.8, 1425), (1385, 'tt0033616', 'Footsteps in the Dark', '1941', 6.9, 821), (1386, 'tt0117623', 'Shastra', '1996', 5.0, 60), (1387, 'tt3848888', 'The Shaukeens', '2014', 5.2, 2518), (1388, 'tt0402230', 'The Great New Wonderful', '2005', 5.7, 1363), (1389, 'tt0476649', 'Darna Zaroori Hai', '2006', 5.4, 1567), (1390, 'tt1904875', 'Dharti', '2011', 7.2, 315), (1391, 'tt0322027', 'Dil Tera Diwana', '1996', 5.1, 140), (1392, 'tt3375154', 'Mangalashtak Once More', '2013', 7.0, 80), (1393, 'tt0260155', 'Mahakaal', '1993', 4.5, 172), (1394, 'tt0206020', 'Hote Hote Pyar Hogaya', '1999', 5.2, 255), (1395, 'tt0845494', 'Nanhe Jaisalmer: A Dream Come True', '2007', 6.1, 348), (1396, 'tt4853926', 'Guddu Ki Gun', '2015', 4.8, 520), (1397, 'tt0091598', 'Naam', '1986', 7.7, 780), (1398, 'tt1508950', 'Baabarr', '2009', 5.1, 283), (1399, 'tt0155775', 'Jawani Diwani', '1972', 7.0, 71), (1400, 'tt0305455', 'Pitaah', '2002', 6.5, 611), (1401, 'tt0113913', 'Naajayaz', '1995', 5.6, 483), (1402, 'tt0313495', 'Prem Geet', '1981', 7.8, 52), (1403, 'tt3449320', 'Darr @ the Mall', '2014', 5.0, 845), (1404, 'tt5289170', 'Laali Ki Shaadi Mein Laaddoo Deewana', '2017', 5.2, 96), (1405, 'tt0112738', 'Criminal', '1995', 5.7, 351), (1406, 'tt0261001', 'Khoj', '1989', 7.1, 216), (1407, 'tt0327761', 'Gemini', '2002', 6.1, 737), (1408, 'tt0076527', 'Parvarish', '1977', 7.0, 776), (1409, 'tt0205968', 'Gharwali Baharwali', '1998', 4.5, 515), (1410, 'tt0107166', 'Hum Hain Rahi Pyar Ke', '1993', 7.4, 4738), (1411, 'tt0101283', 'Ajooba', '1991', 5.2, 1010), (1412, 'tt0477857', 'Sehar', '2005', 7.8, 1770), (1413, 'tt1446071', 'Gallit Gondhal, Dillit Mujra', '2009', 7.5, 62), (1414, 'tt0459475', 'Naam Gum Jaayega', '2005', 5.9, 70), (1415, 'tt0116002', 'Daayraa', '1996', 7.2, 67), (1416, 'tt4760750', 'Bezubaan Ishq', '2015', 4.1, 53), (1417, 'tt0172300', 'Daulat Ki Jung', '1992', 6.0, 709), (1418, 'tt1605790', 'Zokkomon', '2011', 4.2, 257), (1419, 'tt1781838', 'Isi Life Mein...!', '2010', 5.8, 324), (1420, 'tt0233206', 'Amrit', '1986', 6.8, 106), (1421, 'tt0309618', 'Gaddaar', '1995', 4.3, 70), (1422, 'tt0324951', '23rd March 1931: Shaheed', '2002', 5.1, 544), (1423, 'tt1479667', 'Aagey Se Right', '2009', 5.1, 344), (1424, 'tt0408976', 'Krishna Cottage', '2004', 5.1, 708), (1425, 'tt0095955', 'Ram-Avtar', '1988', 4.5, 175), (1426, 'tt0286936', 'Salaakhen', '1998', 5.3, 326), (1427, 'tt0285958', 'Pyaar Ishq Aur Mohabbat', '2001', 4.1, 507), (1428, 'tt0456413', 'Fight Club: Members Only', '2006', 3.3, 1217), (1429, 'tt2191721', 'Jodi Breakers', '2012', 3.5, 1034), (1430, 'tt0053965', 'Jis Desh Men Ganga Behti Hai', '1960', 7.4, 354), (1431, 'tt2976176', 'Lakshmi', '2014', 8.1, 1160), (1432, 'tt5896934', 'Aa Gaya Hero', '2017', 3.7, 283), (1433, 'tt0081491', 'Shaan', '1980', 7.1, 2163), (1434, 'tt2275794', 'Gori Tere Pyaar Mein!', '2013', 5.0, 3118), (1435, 'tt5632164', 'Veerappan', '2016', 5.2, 536), (1436, 'tt2292625', 'Chakravyuh', '2012', 6.9, 2784), (1437, 'tt0255112', 'Dhaai Akshar Prem Ke', '2000', 3.8, 1148), (1438, 'tt0049072', 'Chori Chori', '1956', 7.8, 388), (1439, 'tt0364604', 'Plan', '2004', 4.3, 754), (1440, 'tt0332313', 'Periyanna', '1999', 4.4, 103), (1441, 'tt0359157', 'Dil Hai Betaab', '1993', 4.6, 88), (1442, 'tt0978642', 'Masoom', '1996', 6.0, 64), (1443, 'tt0246420', 'Amanush', '1975', 6.9, 163), (1444, 'tt0309087', 'Yeh Mohabbat Hai', '2002', 5.4, 54), (1445, 'tt0247936', 'Arpan', '1983', 6.2, 54), (1446, 'tt7246718', 'Subedar Joginder Singh', '2018', 7.5, 280), (1447, 'tt3619710', 'Bhaag Johnny', '2015', 5.1, 821), (1448, 'tt0148266', 'Heer Raanjha', '1970', 7.0, 156), (1449, 'tt3862004', 'Fuddu', '2016', 5.6, 124), (1450, 'tt0050758', 'Naya Daur', '1957', 8.1, 918), (1451, 'tt0246839', 'Papa Kahte Hain', '1996', 5.1, 158), (1452, 'tt0279021', 'Chhupa Rustam: A Musical Thriller', '2001', 4.4, 68), (1453, 'tt1575672', 'Samarasimha Reddy', '1999', 7.1, 330), (1454, 'tt0392883', 'Valley of Flowers', '2006', 7.0, 1671), (1455, 'tt0176105', 'Roti', '1974', 6.9, 320), (1456, 'tt2073138', 'Vedi', '2011', 3.7, 254), (1457, 'tt1920986', 'Khuda Kasam', '2010', 4.0, 74), (1458, 'tt0110584', 'Mr. Azaad', '1994', 4.1, 71), (1459, 'tt0226847', 'Jaanam Samjha Karo', '1999', 4.9, 1409), (1460, 'tt3059106', 'Satya 2', '2013', 6.0, 857), (1461, 'tt2861320', 'Policegiri', '2013', 4.1, 1226), (1462, 'tt0043306', 'Awaara', '1951', 8.0, 3030), (1463, 'tt0337611', 'Dilwale', '1994', 5.6, 2458), (1464, 'tt4168188', 'Chaar Sahibzaade', '2014', 8.3, 7415), (1465, 'tt2140315', 'Raaz 3: The Third Dimension', '2012', 4.0, 3113), (1466, 'tt0110942', 'Raja Babu', '1994', 6.4, 2059), (1467, 'tt2171454', 'Zila Ghaziabad', '2013', 3.6, 1130), (1468, 'tt0250122', 'Yeshwant', '1997', 7.3, 956), (1469, 'tt0080322', 'Aakrosh', '1980', 8.0, 634), (1470, 'tt0250452', 'Joru Ka Ghulam', '2000', 5.1, 790), (1471, 'tt0215911', 'Koshish', '1972', 8.7, 1250), (1472, 'tt0230737', 'Satyamev Jayate', '1987', 7.1, 73), (1473, 'tt0086156', 'Pukar', '1983', 6.2, 279), (1474, 'tt7972674', 'Hey Ram Hamne Gandhi Ko maar Diya', '2018', 8.3, 132), (1475, 'tt0290429', 'Ashaant', '1993', 6.3, 474), (1476, 'tt0260345', 'Sansar', '1987', 7.0, 126), (1477, 'tt1891884', 'Once Upon a Time in Mumbai Dobaara!', '2013', 4.8, 5984), (1478, 'tt0088099', 'Sharaabi', '1984', 7.4, 2272), (1479, 'tt0099043', 'Anjali', '1990', 8.3, 2245), (1480, 'tt3514330', 'Youngistaan', '2014', 5.2, 1693), (1481, 'tt4114302', 'Phir Se...', '2018', 5.8, 163), (1482, 'tt0273406', 'Aashiq', '2001', 3.9, 324), (1483, 'tt0911024', 'The Pool', 'I 2007', 7.0, 775), (1484, 'tt0115484', 'Agni Sakshi', '1996', 6.1, 557), (1485, 'tt0368400', 'Vaalee', '1999', 7.9, 1824), (1486, 'tt0815890', 'Khan kluay', '2006', 6.2, 1049), (1487, 'tt1425618', 'Apartment: Rent at Your Own Risk', '2010', 3.4, 191), (1488, 'tt0330977', 'Thamizhan', '2002', 5.4, 913), (1489, 'tt0338690', 'Aag', '1994', 4.8, 170), (1490, 'tt1087856', 'Hello', 'II 2008', 3.4, 1704), (1491, 'tt0085361', 'Coolie', 'I 1983', 6.7, 1656), (1492, 'tt0379250', 'Dharam Kanta', '1982', 6.3, 55), (1493, 'tt2202086', 'Ab Tak Chhappan 2', '2015', 5.8, 1103), (1494, 'tt2929840', 'Zindagi 50 50', '2013', 2.8, 200), (1495, 'tt0387989', \"Deewaar: Let's Bring Our Heroes Home\", '2004', 6.2, 1225), (1496, 'tt1065099', 'My Friend Ganesha', '2007', 4.9, 95), (1497, 'tt2402631', 'I.D.', '2012', 6.9, 135), (1498, 'tt0404537', 'Vansh', '1992', 6.1, 51), (1499, 'tt0213775', 'Kohram', '1999', 5.4, 520), (1500, 'tt0096252', 'Tezaab', '1988', 7.0, 1600), (1501, 'tt5543746', 'Maatr', '2017', 4.4, 256), (1502, 'tt1431122', 'No Problem', 'I 2010', 4.2, 1824), (1503, 'tt0106204', 'Aankhen', '1993', 6.7, 1536), (1504, 'tt0328998', 'Dil Hai Tumhaara', '2002', 5.4, 1412), (1505, 'tt2027138', 'Kyaa Super Kool Hain Hum', '2012', 5.0, 3369), (1506, 'tt2454134', 'Karamati Coat', '1993', 7.9, 81), (1507, 'tt1020978', 'Naqaab', '2007', 5.4, 1181), (1508, 'tt0363833', 'Mumbai Se Aaya Mera Dost', '2003', 3.4, 489), (1509, 'tt1334254', 'Maharathi', '2008', 7.2, 796), (1510, 'tt2575290', 'Train Station', 'I 2015', 6.0, 133), (1511, 'tt0056379', 'Professor', '1962', 7.0, 173), (1512, 'tt1050739', 'Roadside Romeo', '2008', 5.3, 1093), (1513, 'tt1233600', 'The Wishing Tree', '2017', 5.7, 59), (1514, 'tt3531604', 'Gang of Ghosts', '2014', 3.7, 308), (1515, 'tt1372681', 'Aa Dekhen Zara', '2009', 5.1, 792), (1516, 'tt2309600', 'Singam 2', '2013', 6.3, 4891), (1517, 'tt7249874', 'LIE', '2017', 5.9, 534), (1518, 'tt1883121', '404: Error Not Found', '2011', 7.2, 1671), (1519, 'tt0260411', 'Tahkhana', '1986', 5.2, 110), (1520, 'tt0085913', 'Masoom', '1983', 8.5, 3032), (1521, 'tt0149573', 'Aashiqui', '1990', 6.3, 1328), (1522, 'tt0096415', 'Waqt Ki Awaz', '1988', 6.5, 66), (1523, 'tt1039969', 'The Last Lear', '2007', 7.1, 980), (1524, 'tt0368811', 'Jajantaram Mamantaram', '2003', 5.7, 395), (1525, 'tt0089599', 'Mirch Masala', '1987', 7.8, 953), (1526, 'tt0148706', 'Saraswatichandra', '1968', 6.5, 110), (1527, 'tt4107858', 'MSG: The Messenger of God', '2015', 6.7, 11076), (1528, 'tt0100441', 'Pyar Ka Devta', '1990', 4.8, 84), (1529, 'tt0088031', 'Saaransh', '1984', 8.2, 1470), (1530, 'tt0089201', 'Ghulami', '1985', 7.1, 369), (1531, 'tt3607198', 'Punjab 1984', '2014', 8.5, 1612), (1532, 'tt3796006', 'ROAR: Tigers of the Sundarbans', '2014', 5.7, 626), (1533, 'tt0074730', 'Kabhie Kabhie', '1976', 7.5, 1641), (1534, 'tt2576450', 'Besharam', '2013', 3.6, 4836), (1535, 'tt1105747', 'Yuvvraaj', '2008', 4.2, 3520), (1536, 'tt0488836', 'Hanuman', '2005', 7.1, 424), (1537, 'tt3184756', 'Maanikya', '2014', 6.5, 446), (1538, 'tt5162476', '6-5=2', '2014', 4.8, 87), (1539, 'tt5037626', 'Crash Test Aglaé', '2017', 6.5, 429), (1540, 'tt0085178', 'Ardh Satya', '1983', 8.2, 1357), (1541, 'tt0359971', 'Sheshnaag', '1990', 4.8, 52), (1542, 'tt0297241', 'Kyo Kii... Main Jhuth Nahin Bolta', '2001', 5.3, 1556), (1543, 'tt0808164', 'Bas Ek Pal', '2006', 5.9, 443), (1544, 'tt4684028', 'Guddu Rangeela', '2015', 5.3, 770), (1545, 'tt3338188', 'Jia aur Jia', '2017', 4.9, 108), (1546, 'tt1699525', 'Winner', '2003', 6.3, 200), (1547, 'tt0293342', 'Lajja', '2001', 6.8, 2269), (1548, 'tt0451983', 'Zeher', '2005', 5.4, 1625), (1549, 'tt4467202', 'Hero', 'V 2015', 3.7, 1950), (1550, 'tt0111793', 'Yeh Dillagi', '1994', 6.6, 4084), (1551, 'tt0448206', 'Bunty Aur Babli', '2005', 6.2, 5499), (1552, 'tt0226786', 'Hum Tum Pe Marte Hain', '1999', 5.3, 405), (1553, 'tt0157337', 'Azaad', '1978', 7.2, 61), (1554, 'tt0378025', 'Hawayein', '2003', 6.0, 133), (1555, 'tt0326722', 'Agni Varsha', '2002', 4.9, 328), (1556, 'tt0083636', 'Bemisal', '1982', 7.1, 316), (1557, 'tt2708550', 'Jal', '2013', 6.7, 397), (1558, 'tt1418380', 'The Domino Effect', '2012', 6.2, 232), (1559, 'tt0172089', 'Aatank Hi Aatank', '1995', 5.5, 673), (1560, 'tt0116898', 'Loafer', '1996', 5.1, 416), (1561, 'tt0406977', 'Masti', '2004', 6.2, 5152), (1562, 'tt0255713', 'Zubeidaa', '2001', 6.2, 1349), (1563, 'tt0140377', 'Michael Madana Kamarajan', '1990', 8.5, 2991), (1564, 'tt0358063', 'Raja Jani', '1972', 6.8, 146), (1565, 'tt6001942', 'Thikka', '2016', 5.3, 60), (1566, 'tt0286683', 'Hero Hindustani', '1998', 4.9, 249), (1567, 'tt0808306', 'Honeymoon Travels Pvt. Ltd.', '2007', 6.1, 1990), (1568, 'tt4688446', 'Shuddhi', '2017', 8.0, 278), (1569, 'tt0096477', 'Yateem', '1988', 6.3, 141), (1570, 'tt1202517', 'Barah Aana', '2009', 6.9, 889), (1571, 'tt0317985', 'Pehla Pehla Pyar', '1994', 5.0, 72), (1572, 'tt0364303', 'Dev', '2004', 7.0, 1204), (1573, 'tt0349122', 'Bahurani', '1989', 6.7, 63), (1574, 'tt0337633', 'Hathyar: Face to Face with Reality', '2002', 5.8, 522), (1575, 'tt0327028', 'Majunu', '2001', 5.4, 87), (1576, 'tt0255305', 'Khiladi 420', '2000', 5.6, 1538), (1577, 'tt4335954', 'My Birthday Song', '2018', 5.5, 178), (1578, 'tt0475645', 'Vaah! Life Ho Toh Aisi!', '2005', 4.4, 1732), (1579, 'tt0254179', 'Betaab', '1983', 6.7, 398), (1580, 'tt0106203', 'Aaina', '1993', 6.0, 343), (1581, 'tt0286640', 'Gair', '1999', 3.7, 149), (1582, 'tt5569160', 'Mantostaan', '2016', 6.1, 52), (1583, 'tt4485366', 'Monsoon', 'I 2015', 3.4, 54), (1584, 'tt0341554', 'Saath Saath', '1982', 7.1, 169), (1585, 'tt1080568', 'Cape Karma', '2007', 4.5, 114), (1586, 'tt1176961', 'Paying Guests', '2009', 4.2, 444), (1587, 'tt1301698', '1920', '2008', 6.4, 2403), (1588, 'tt4897908', 'Calendar Girls', '2015', 4.2, 705), (1589, 'tt0430381', 'Musafir', '2004', 5.5, 1523), (1590, 'tt0432536', 'Hatya: The Murder', '2004', 5.3, 453), (1591, 'tt1986040', 'Ishkq in Paris', '2013', 4.1, 674), (1592, 'tt0284328', 'Mujhe Kucch Kehna Hai', '2001', 5.1, 961), (1593, 'tt0378647', 'Ramana', 'I 2002', 8.1, 1557), (1594, 'tt0252385', 'Do Gaz Zameen Ke Neeche', '1972', 6.3, 62), (1595, 'tt0102200', 'Khel', '1992', 6.4, 464), (1596, 'tt0362696', 'Haasil', '2003', 7.7, 2328), (1597, 'tt0100002', 'Lekin...', '1990', 7.5, 282), (1598, 'tt2571022', 'Yaan', '2014', 3.5, 293), (1599, 'tt1105709', '8 x 10 Tasveer', '2009', 5.9, 4961), (1600, 'tt4228746', 'Zid', '2014', 4.4, 870), (1601, 'tt0331639', 'Shakthi: The Power', '2002', 6.1, 4197), (1602, 'tt1667076', 'F.A.L.T.U', '2011', 4.6, 2691), (1603, 'tt0104605', 'Khiladi', '1992', 7.4, 6736), (1604, 'tt0313395', 'Kitne Door... Kitne Paas', '2002', 4.6, 263), (1605, 'tt0476888', 'Tom, Dick, and Harry', '2006', 3.2, 493), (1606, 'tt0423286', 'Sullan', '2004', 3.1, 249), (1607, 'tt0386029', 'Shivamani', '2003', 6.3, 298), (1608, 'tt0405046', 'Insaaf: The Justice', '2004', 3.9, 79), (1609, 'tt2848824', 'Bajatey Raho', '2013', 5.4, 675), (1610, 'tt0157944', 'Love in Tokyo', '1966', 6.8, 116), (1611, 'tt0137459', 'Deedar', '1951', 7.4, 169), (1612, 'tt0104072', 'Deedar', '1992', 5.1, 504), (1613, 'tt0442855', 'Vaada', '2005', 5.7, 729), (1614, 'tt0093604', 'Nazrana', '1987', 6.3, 68), (1615, 'tt0430328', 'Madhoshi', '2004', 4.7, 333), (1616, 'tt0096390', 'Vijay', '1988', 5.6, 163), (1617, 'tt0246413', 'Agar Tum Na Hote', '1983', 6.6, 124), (1618, 'tt0084199', 'Khud-Daar', '1982', 7.0, 300), (1619, 'tt2066062', 'Shortcut Romeo', '2013', 2.7, 232), (1620, 'tt0080926', 'Insaf Ka Tarazu', '1980', 6.7, 182), (1621, 'tt0084393', 'Nastik', '1983', 5.6, 311), (1622, 'tt0430202', 'Sandwich', '2006', 5.5, 648), (1623, 'tt0140399', 'Muthu', '1995', 7.5, 2088), (1624, 'tt1728239', 'Double Dhamaal', '2011', 3.8, 2799), (1625, 'tt0444781', 'Deewane Huye Paagal', '2005', 5.4, 3891), (1626, 'tt0402014', 'Ab Tak Chhappan', '2004', 7.9, 6929), (1627, 'tt0085896', 'Mandi', '1983', 7.6, 570), (1628, 'tt0995823', 'Khoya Khoya Chand', '2007', 6.4, 462), (1629, 'tt0109116', 'Andaz', '1994', 4.4, 318), (1630, 'tt0101742', 'Deewana Mujh Sa Nahin', '1990', 6.0, 820), (1631, 'tt0266765', 'Mere Mehboob', '1963', 7.1, 158), (1632, 'tt0086597', 'Woh 7 Din', '1983', 7.6, 957), (1633, 'tt0268129', 'Ajanabee', '1974', 6.7, 121), (1634, 'tt1047519', 'Red Swastik', '2007', 2.8, 206), (1635, 'tt0095375', 'Inteqam', '1988', 5.5, 52), (1636, 'tt2941856', 'Enemmy', '2013', 5.2, 193), (1637, 'tt0073034', 'Garm Hava', '1974', 8.2, 543), (1638, 'tt2800452', 'Dilliwaali Zaalim Girlfriend', '2015', 3.8, 161), (1639, 'tt6033536', 'Dear Maya', '2017', 5.0, 258), (1640, 'tt0292552', 'Hum Ho Gaye Aap Ke', '2001', 4.1, 347), (1641, 'tt0476848', 'Ramji Londonwaley', '2005', 6.8, 842), (1642, 'tt0328729', 'Dil Maange More!!!', '2004', 5.2, 1592), (1643, 'tt0327071', 'Om Jai Jagadish', '2002', 5.6, 1625), (1644, 'tt1442583', 'Tum Milo Toh Sahi', '2010', 5.8, 267), (1645, 'tt0077797', 'Khatta Meetha', '1978', 7.6, 661), (1646, 'tt0113933', 'Naaraaz', '1994', 6.1, 93), (1647, 'tt1512220', 'My Friend Pinto', '2011', 5.3, 317), (1648, 'tt1132589', 'Hijack', '2008', 3.7, 277), (1649, 'tt0268220', 'China Town', '1962', 6.9, 157), (1650, 'tt0813540', 'Naksha', '2006', 3.1, 857), (1651, 'tt0106655', 'Damini', '1993', 7.9, 2497), (1652, 'tt0135621', 'Rakshak', '1996', 4.7, 232), (1653, 'tt0390614', 'Waqt: The Race Against Time', '2005', 7.0, 8873), (1654, 'tt0341266', 'Bhoot', '2003', 6.4, 2854), (1655, 'tt0099429', 'Dil', '1990', 6.8, 4307), (1656, 'tt1340838', 'Raaz: The Mystery Continues', '2009', 5.7, 2833), (1657, 'tt0451787', 'Kyon Ki...', '2005', 5.5, 3120), (1658, 'tt1194236', 'Get Educated: Paathshaala', '2010', 4.9, 1318), (1659, 'tt0100816', 'Tridev', '1989', 6.3, 1502), (1660, 'tt0173156', 'Saajan Ka Ghar', '1994', 5.1, 101), (1661, 'tt0157845', 'Bajrangbali', '1976', 8.2, 58), (1662, 'tt4556848', 'Barkhaa', '2015', 5.2, 260), (1663, 'tt1324076', 'Fox', 'I 2009', 5.4, 496), (1664, 'tt4354740', 'Dharam Sankat Mein', '2015', 6.3, 1697), (1665, 'tt0130350', 'Vishwatma', '1992', 6.5, 964), (1666, 'tt0227194', 'One 2 Ka 4', '2001', 5.5, 4267), (1667, 'tt0253514', 'Purana Mandir', '1984', 6.5, 382), (1668, 'tt5219770', 'Dev Bhoomi', '2016', 7.2, 140), (1669, 'tt0119862', 'Pardesi Babu', '1998', 5.1, 307), (1670, 'tt0268216', 'Charas', '1976', 6.6, 136), (1671, 'tt1038915', 'Shoot on Sight', '2007', 6.1, 1555), (1672, 'tt0254927', 'Aaghaaz', '2000', 4.4, 224), (1673, 'tt0178192', 'Aap Aye Bahaar Ayee', '1971', 6.7, 64), (1674, 'tt0359990', 'Sindoor', '1987', 5.6, 77), (1675, 'tt5928576', 'Wah Taj', '2016', 6.2, 66), (1676, 'tt0316406', 'Pinjra', '1972', 7.6, 179), (1677, 'tt0440199', 'Cheetah', '1994', 6.1, 59), (1678, 'tt0315632', 'Road', '2002', 5.6, 1068), (1679, 'tt0327403', 'Afsana Pyar Ka', '1991', 6.2, 668), (1680, 'tt0366630', 'Janasheen', '2003', 3.1, 308), (1681, 'tt0118660', 'Auzaar', '1997', 4.6, 803), (1682, 'tt0810058', 'Aap Ki Khatir', '2006', 3.6, 753), (1683, 'tt0833476', 'Jhoom Barabar Jhoom', '2007', 3.8, 2791), (1684, 'tt0069810', 'Bobby', '1973', 7.1, 1405), (1685, 'tt0101616', 'Coolie No. 1', '1995', 6.2, 1938), (1686, 'tt0435437', 'Taarzan: The Wonder Car', '2004', 4.4, 2062), (1687, 'tt0985636', 'Laaga Chunari Mein Daag: Journey of a Woman', '2007', 5.4, 2086), (1688, 'tt0102844', 'Saudagar', '1991', 6.8, 1010), (1689, 'tt0173080', 'Pyaar Kiya To Darna Kya', '1998', 6.7, 3397), (1690, 'tt0059709', 'Shakespeare-Wallah', '1965', 6.9, 529), (1691, 'tt1267500', 'Phoonk', '2008', 3.9, 1165), (1692, 'tt0066766', 'Andaz', '1971', 6.8, 220), (1693, 'tt1227524', 'Aashayein', '2010', 7.2, 1075), (1694, 'tt0107440', 'Lootere', '1993', 5.3, 221), (1695, 'tt2278422', 'Kamaal Dhamaal Malamaal', '2012', 4.2, 652), (1696, 'tt0948462', 'Priyamanavale', '2000', 7.0, 888), (1697, 'tt6040012', 'MSG the Warrior: Lion Heart', '2016', 7.6, 7307), (1698, 'tt0348662', 'Kucch To Hai', '2003', 4.3, 728), (1699, 'tt0073458', 'Nishant', '1975', 7.7, 406), (1700, 'tt0059028', 'Chemmeen', '1965', 7.8, 418), (1701, 'tt0444769', 'Blackmail', '2005', 4.3, 548), (1702, 'tt0071811', 'Manoranjan', '1974', 6.8, 175), (1703, 'tt0371974', 'Swayamvar', '1980', 7.5, 51), (1704, 'tt0148523', 'Neel Kamal', '1968', 6.9, 216), (1705, 'tt0142431', 'Khamoshi', '1970', 7.8, 459), (1706, 'tt0330505', 'Kal Aaj Aur Kal', '1971', 6.9, 250), (1707, 'tt0225013', 'Aa Gale Lag Jaa', '1994', 5.0, 79), (1708, 'tt0082162', 'Chashme Buddoor', '1981', 8.0, 1951), (1709, 'tt0021594', 'Alam Ara', '1931', 7.4, 97), (1710, 'tt0328503', 'Taj Mahal: An Eternal Love Story', '2005', 5.0, 150), (1711, 'tt0067555', 'Parwana', '1971', 6.9, 151), (1712, 'tt0444840', 'Humko Deewana Kar Gaye', '2006', 5.4, 3582), (1713, 'tt0437407', 'Parineeta', '2005', 7.3, 8030), (1714, 'tt0120540', 'Yes Boss', '1997', 6.9, 11373), (1715, 'tt0074949', 'Nagin', '1976', 6.2, 351), (1716, 'tt6180274', 'Poorna', '2017', 7.8, 1012), (1717, 'tt1564870', 'Not Today', '2013', 5.6, 604), (1718, 'tt3359290', 'Bobby Jasoos', '2014', 5.4, 1859), (1719, 'tt0375997', 'Nandha', '2001', 7.6, 1246), (1720, 'tt5756052', 'Jeena Isi Ka Naam Hai', 'I 2017', 5.5, 74), (1721, 'tt0119491', 'Lahoo Ke Do Rang', '1997', 5.2, 532), (1722, 'tt0266926', 'Samundar', '1986', 5.4, 115), (1723, 'tt0079938', 'Sparsh', '1980', 8.2, 533), (1724, 'tt0287537', 'Love Ke Liye Kuch Bhi Karega', '2001', 6.1, 1360), (1725, 'tt0055039', 'Kabuliwala', '1961', 7.6, 276), (1726, 'tt0057568', 'Tere Ghar Ke Samne', '1963', 7.4, 334), (1727, 'tt7207156', 'Sameer', '2017', 5.8, 163), (1728, 'tt0176012', 'Patthar Ke Sanam', '1967', 6.9, 102), (1729, 'tt0278291', 'Ajnabee', '2001', 6.5, 7180), (1730, 'tt1869296', 'Bbuddah... Hoga Terra Baap', '2011', 5.6, 2481), (1731, 'tt0070947', 'Zanjeer', '1973', 7.6, 3235), (1732, 'tt0819810', 'Traffic Signal', '2007', 6.6, 1292), (1733, 'tt5963908', 'T for Taj Mahal', '2018', 9.3, 71), (1734, 'tt1002963', 'Y.M.I. Yeh Mera India', '2008', 7.8, 832), (1735, 'tt0072870', 'Dharam Karam', '1975', 5.9, 101), (1736, 'tt4643520', 'The Violin Player', '2016', 6.9, 218), (1737, 'tt1612573', 'Rokkk', '2010', 4.5, 109), (1738, 'tt0234542', 'Purab Aur Pachhim', '1970', 7.5, 719), (1739, 'tt0456549', 'Mr Ya Miss', '2005', 3.7, 399), (1740, 'tt0444874', 'Main Aisa Hi Hoon', '2005', 4.8, 886), (1741, 'tt2057441', 'Mujhse Fraaandship Karoge', '2011', 6.9, 3609), (1742, 'tt0250483', 'Kya Kehna', '2000', 5.9, 1696), (1743, 'tt0119239', 'Gupt: The Hidden Truth', '1997', 7.3, 4006), (1744, 'tt0079749', 'Alibaba Aur 40 Chor', '1980', 6.2, 406), (1745, 'tt0076259', 'Khoon Pasina', '1977', 6.2, 317), (1746, 'tt0143202', 'Dahan', '1998', 6.9, 211), (1747, 'tt0061072', 'Teesri Kasam', '1966', 8.0, 639), (1748, 'tt0065064', 'Talash', '1969', 6.8, 135), (1749, 'tt0187195', 'Kalaakaar', '1983', 6.2, 54), (1750, 'tt2857500', 'Saare Jahaan Se Mehnga...', '2013', 7.1, 823), (1751, 'tt0338960', 'Ek Tha Raja', '1996', 4.4, 93), (1752, 'tt0099943', 'Kishen Kanhaiya', '1990', 6.0, 516), (1753, 'tt2449318', 'Hyderabad Nawabs', '2006', 7.2, 94), (1754, 'tt0098168', 'Ram Lakhan', '1989', 6.9, 1862), (1755, 'tt0476761', 'Deha', '2007', 6.9, 52), (1756, 'tt0218533', 'Pukar', '2000', 6.8, 2006), (1757, 'tt0047990', 'Devdas', '1955', 8.0, 1533), (1758, 'tt3398048', 'Tevar', '2015', 4.2, 2660), (1759, 'tt0068938', 'Mere Jeevan Saathi', '1972', 7.0, 138), (1760, 'tt0240879', 'Sardar', '1993', 7.9, 582), (1761, 'tt0331216', 'Chor Machaaye Shor', '2002', 4.2, 961), (1762, 'tt0230348', 'Jab Pyar Kisise Hota Hai', '1961', 7.2, 93), (1763, 'tt0106986', 'Gardish', '1993', 7.4, 690), (1764, 'tt7291466', 'Sniff!!!', '2017', 5.9, 96), (1765, 'tt0451833', 'No Entry', '2005', 6.6, 9334), (1766, 'tt1578116', 'Atithi Tum Kab Jaoge?', '2010', 6.4, 4317), (1767, 'tt5755606', 'Banjo', '2016', 5.1, 882), (1768, 'tt0289725', 'Jung', '2000', 4.3, 308), (1769, 'tt0064842', 'Prince', '1969', 7.2, 97), (1770, 'tt4960754', 'Kadhal Sadugudu', '2003', 6.1, 116), (1771, 'tt0070241', 'Jheel Ke Us Paar', '1973', 7.2, 73), (1772, 'tt0067478', 'Naya Zamana', '1971', 7.0, 78), (1773, 'tt3877652', 'Desi Kattey', '2014', 4.3, 106), (1774, 'tt1309561', 'Lamhaa: The Untold Story of Kashmir', '2010', 5.1, 577), (1775, 'tt0290213', 'Hum Hain Bemisaal', '1994', 5.8, 514), (1776, 'tt0135666', 'Takkar', '1995', 4.6, 152), (1777, 'tt0090358', 'Yudh', '1985', 6.1, 140), (1778, 'tt5343678', 'Moh Maya Money', '2016', 5.7, 304), (1779, 'tt0959281', 'Allari Pidugu', '2005', 2.8, 300), (1780, 'tt0101432', 'Benaam Badsha', '1991', 5.0, 195), (1781, 'tt0332766', 'Sur: The Melody of Life', '2002', 6.5, 391), (1782, 'tt2437954', 'Singh Saab the Great', '2013', 5.8, 1973), (1783, 'tt3398052', 'Daawat-e-Ishq', '2014', 6.2, 4935), (1784, 'tt0065416', 'Aradhana', '1969', 7.7, 1370), (1785, 'tt0152139', 'Ram Teri Ganga Maili', '1985', 6.7, 805), (1786, 'tt0247910', 'Aakhir Kyon?', '1985', 6.7, 120), (1787, 'tt0306065', 'Shagird', '1967', 7.2, 93), (1788, 'tt0345043', 'Chura Liyaa Hai Tumne', '2003', 3.8, 433), (1789, 'tt2498312', 'Bachchan', '2013', 6.6, 827), (1790, 'tt0108144', 'Sir', '1993', 6.3, 265), (1791, 'tt0156014', 'Red Rose', '1980', 6.3, 132), (1792, 'tt0252038', 'Tarkieb', '2000', 4.9, 178), (1793, 'tt0045467', 'Aah', '1953', 7.1, 216), (1794, 'tt0843372', 'Sainikudu', '2006', 5.2, 1434), (1795, 'tt0195814', 'Hyderabad Blues', '1998', 7.2, 863), (1796, 'tt0080982', 'Kalyug', '1981', 7.8, 428), (1797, 'tt0259170', 'Abdullah', '1980', 6.5, 152), (1798, 'tt0387678', 'Waisa Bhi Hota Hai Part II', '2003', 7.3, 658), (1799, 'tt0230670', 'Pyar Ka Mausam', '1969', 6.9, 89), (1800, 'tt0149850', 'Bewaffa Se Waffa', '1992', 5.1, 53), (1801, 'tt1137979', 'Rama Rama Kya Hai Dramaaa', '2008', 4.0, 222), (1802, 'tt1455816', 'Prince', '2010', 4.2, 1677), (1803, 'tt1433905', 'All the Best: Fun Begins', '2009', 6.1, 4803), (1804, 'tt0107249', 'Jaagruti', '1993', 3.5, 382), (1805, 'tt0041123', 'Andaz', '1949', 7.4, 380), (1806, 'tt2708644', 'I Love New Year', '2015', 3.9, 455), (1807, 'tt2905606', 'Vishnuvardhana', '2011', 7.2, 536), (1808, 'tt6941560', 'Phullu', '2017', 6.0, 131), (1809, 'tt1182908', 'Krazzy 4', '2008', 4.2, 1766), (1810, 'tt0117374', 'PremGranth', '1996', 5.7, 358), (1811, 'tt1896781', 'Masti Express', '2011', 5.2, 73), (1812, 'tt7723022', 'Nirdosh', '2018', 4.1, 57), (1813, 'tt0146335', 'Upkar', '1967', 7.7, 474), (1814, 'tt0234724', 'Shor', '1972', 7.3, 278), (1815, 'tt0267548', 'Haseena Maan Jaayegi', '1999', 6.2, 2379), (1816, 'tt0301179', 'Deewaanapan', '2001', 4.7, 360), (1817, 'tt0063532', 'Saathi', '1968', 7.5, 62), (1818, 'tt0155068', 'Punnagai Mannan', '1986', 7.9, 678), (1819, 'tt0246095', 'Mutamestri', '1993', 7.1, 242), (1820, 'tt4396648', 'Time Out', '2015', 6.1, 266), (1821, 'tt0268146', 'Anurodh', '1979', 6.8, 129), (1822, 'tt0265452', 'Officer', '2001', 5.4, 92), (1823, 'tt1836015', 'Kaccha Limboo', '2011', 6.3, 70), (1824, 'tt1438486', 'Chal Chala Chal', '2009', 4.7, 228), (1825, 'tt0066241', 'Prem Pujari', '1970', 6.8, 170), (1826, 'tt0077783', 'Junoon', '1979', 7.6, 428), (1827, 'tt1404573', 'Daddy Cool: Join the Fun', '2009', 3.9, 310), (1828, 'tt0270880', 'Champion', '2000', 4.2, 414), (1829, 'tt0058287', 'Leader', '1964', 7.4, 158), (1830, 'tt0068217', 'Anuraag', '1972', 7.3, 68), (1831, 'tt2347289', 'Bittoo Boss', '2012', 4.2, 331), (1832, 'tt1971365', 'Dangerous Ishhq', '2012', 4.0, 749), (1833, 'tt2579680', '100% Love', '2012', 6.0, 261), (1834, 'tt0052560', 'Anari', '1959', 7.9, 578), (1835, 'tt0097045', 'Chandni', '1989', 6.8, 1350), (1836, 'tt0456558', 'Nazar', '2005', 4.8, 97), (1837, 'tt0082796', 'Naram Garam', '1981', 7.5, 752), (1838, 'tt0139398', 'Kaajal', '1965', 7.0, 92), (1839, 'tt1228726', 'Mr. White Mr. Black', '2008', 3.5, 175), (1840, 'tt2063745', 'Kya Dilli Kya Lahore', '2014', 7.5, 1716), (1841, 'tt2321492', 'Krishna Aur Kans', '2012', 6.4, 148), (1842, 'tt4219300', 'Gollu aur Pappu', '2014', 4.4, 78), (1843, 'tt0426075', 'Lakeer - Forbidden Lines', '2004', 4.4, 776), (1844, 'tt0338490', '3 Deewarein', '2003', 7.9, 1970), (1845, 'tt0117470', 'Return of Jewel Thief', '1996', 5.1, 88), (1846, 'tt0480572', 'Pyaar Ke Side Effects', '2006', 6.7, 2768), (1847, 'tt0154706', 'Black Mail', '1973', 6.9, 148), (1848, 'tt0068257', 'Bawarchi', '1972', 8.0, 2986), (1849, 'tt0281913', 'Hogi Pyaar Ki Jeet', '1999', 4.3, 407), (1850, 'tt0112459', 'Barsaat', '1995', 5.4, 941), (1851, 'tt0297067', 'Daddy', 'I 2001', 6.6, 366), (1852, 'tt0289686', 'Dancer', '1991', 5.3, 556), (1853, 'tt0337625', 'Gambler', '1995', 5.1, 281), (1854, 'tt0443208', 'Rog', '2005', 5.0, 377), (1855, 'tt4651394', 'Wedding Pullav', '2015', 3.9, 160), (1856, 'tt1179782', 'Mithya', '2008', 7.1, 1810), (1857, 'tt0232169', 'Naukar Biwi Ka', '1983', 6.4, 120), (1858, 'tt0416282', 'Vaastu Shastra', '2004', 4.9, 555), (1859, 'tt2951576', 'One by Two', '2014', 4.2, 668), (1860, 'tt0319500', 'Himmat', '1996', 4.6, 147), (1861, 'tt0071436', 'Dost', '1974', 7.1, 178), (1862, 'tt0087417', 'Holi', '1985', 7.5, 493), (1863, 'tt4373886', 'Sharafat Gayi Tel Lene', '2015', 5.2, 238), (1864, 'tt0451824', \"My Wife's Murder\", '2005', 6.2, 812), (1865, 'tt0116035', 'Dastak', '1996', 5.0, 235), (1866, 'tt0363525', 'Commando', '1988', 4.4, 125), (1867, 'tt0100384', 'Police Public', '1990', 6.7, 112), (1868, 'tt1964837', 'Kempe Gowda', '2011', 7.0, 507), (1869, 'tt0325041', 'Badhaai Ho Badhaai', '2002', 4.4, 558), (1870, 'tt0364621', 'Qayamat: City Under Threat', '2003', 4.8, 1864), (1871, 'tt0420304', 'Viruddh... Family Comes First', '2005', 7.1, 1893), (1872, 'tt4365060', '7 Hours to Go', '2016', 5.6, 335), (1873, 'tt2361022', 'Deewana Main Deewana', '2013', 2.8, 285), (1874, 'tt0371775', 'Khel', '2003', 4.0, 412), (1875, 'tt2960004', 'Phoring', '2013', 7.6, 584), (1876, 'tt0178204', 'Aasha', '1980', 5.4, 163), (1877, 'tt0072064', 'Rajnigandha', '1974', 7.5, 430), (1878, 'tt0079963', 'Suhaag', '1979', 6.8, 845), (1879, 'tt0114726', 'Trimurti', '1995', 4.9, 2195), (1880, 'tt0079590', 'Muqaddar Ka Sikandar', '1978', 7.5, 2205), (1881, 'tt0207341', 'Biwi No. 1', '1999', 5.7, 4053), (1882, 'tt0090575', 'Aakhree Raasta', '1986', 7.1, 1022), (1883, 'tt0396690', 'Kisna: The Warrior Poet', '2005', 4.7, 1228), (1884, 'tt0067840', 'Tere Mere Sapne', '1971', 7.3, 193), (1885, 'tt0114232', 'Ram Shastra', '1995', 4.3, 55), (1886, 'tt0907619', 'Arjun', '2004', 6.6, 1399), (1887, 'tt0085776', 'Katha', '1983', 8.0, 913), (1888, 'tt0488840', 'Koi Aap Sa: But Lovers Have to Be Friends', '2005', 5.6, 179), (1889, 'tt0244776', 'Satyakam', '1969', 8.2, 570), (1890, 'tt0306228', \"Where's the Party Yaar?\", '2003', 5.0, 991), (1891, 'tt0416712', 'Elaan', '2005', 4.4, 604), (1892, 'tt1191138', 'Thoda Pyaar Thoda Magic', '2008', 4.9, 1985), (1893, 'tt0234054', 'Kranti', '1981', 7.2, 709), (1894, 'tt0953306', 'Anwar', '2007', 6.7, 738), (1895, 'tt0093949', 'Shahenshah', '1988', 6.4, 1512), (1896, 'tt0112884', 'The Don', '1995', 6.1, 58), (1897, 'tt0119720', 'The Death Sentence: Mrityu Dand', '1997', 7.4, 372), (1898, 'tt0340122', 'Gangotri', '2003', 4.7, 368), (1899, 'tt1629241', 'Bumm Bumm Bole', '2010', 6.0, 432), (1900, 'tt0187043', 'Gair Kaanooni', '1989', 5.5, 64), (1901, 'tt0246286', 'The Train', '1970', 6.7, 107), (1902, 'tt0046673', 'Aar-Paar', '1954', 6.7, 185), (1903, 'tt0327005', 'Kyaa Dil Ne Kahaa', '2002', 3.4, 219), (1904, 'tt1127870', 'Mumbai Salsa', '2007', 4.9, 176), (1905, 'tt0051806', 'Kala Pani', '1958', 7.0, 224), (1906, 'tt0833510', 'Navra Mazha Navsacha', '2004', 7.0, 236), (1907, 'tt0306840', 'Koi Mere Dil Se Poochhe', '2002', 3.7, 230), (1908, 'tt0157460', 'Chitchor', '1976', 7.5, 446), (1909, 'tt1606253', 'Itlu Sravani Subramanyam', '2001', 6.5, 143), (1910, 'tt0481241', 'Trinetra', '1991', 5.1, 51), (1911, 'tt2903778', 'Dracula 2012', '2013', 2.3, 74), (1912, 'tt0254972', 'Aryan', '1988', 7.2, 222), (1913, 'tt0264210', 'Zor: Never Underestimate the Force', '1998', 4.5, 216), (1914, 'tt0103689', 'Angaar', '1992', 6.5, 239), (1915, 'tt0282268', 'Yeh Hai Mumbai Meri Jaan', '1999', 4.6, 205), (1916, 'tt0459293', 'Gandhi, My Father', '2007', 7.4, 1454), (1917, 'tt0158107', 'Pratighaat', '1987', 7.1, 55), (1918, 'tt0056850', 'Bandini', '1963', 7.7, 670), (1919, 'tt0347167', 'The Hero: Love Story of a Spy', '2003', 5.3, 1774), (1920, 'tt0363409', 'Aan: Men at Work', '2004', 5.9, 2517), (1921, 'tt0137455', 'Dastak', '1970', 7.1, 100), (1922, 'tt0214931', 'Mast', '1999', 5.5, 774), (1923, 'tt0313743', 'Vadh', '2002', 6.2, 102), (1924, 'tt0075632', 'Aadalat', '1976', 6.5, 228), (1925, 'tt0091258', 'Ilzaam', '1986', 6.2, 66), (1926, 'tt0261243', 'Rockford', '1999', 7.4, 1261), (1927, 'tt0106205', 'Aasoo Bane Angaarey', '1993', 5.0, 60), (1928, 'tt0261545', 'Astitva', '2000', 7.3, 508), (1929, 'tt0185335', 'Ghulam-E-Musthafa', '1997', 6.7, 978), (1930, 'tt0254354', 'Ghar Ghar Ki Kahani', '1970', 7.6, 52), (1931, 'tt0210920', 'Prem Aggan', '1998', 2.0, 417), (1932, 'tt0214751', 'Hathyar', '1989', 5.9, 228), (1933, 'tt0177489', 'Aag Ka Gola', '1990', 5.5, 75), (1934, 'tt0096922', 'Bhrashtachar', '1989', 5.8, 95), (1935, 'tt2401719', 'Prague', '2013', 5.8, 304), (1936, 'tt2633826', 'Mumbai Mirror', '2013', 5.2, 102), (1937, 'tt0255289', 'Kala Pani', '1996', 8.4, 2898), (1938, 'tt0089552', 'Mard', '1985', 6.2, 1248), (1939, 'tt0244794', 'Sirf Tum', '1999', 6.1, 1189), (1940, 'tt0106206', 'Aashik Aawara', '1993', 5.4, 240), (1941, 'tt0248216', 'Na Tum Jaano Na Hum', '2002', 5.1, 2454), (1942, 'tt7335176', 'Haseena', '2018', 4.3, 63), (1943, 'tt4916048', 'Island City', '2015', 6.7, 277), (1944, 'tt0048392', \"Mr. & Mrs. '55\", '1955', 7.2, 362), (1945, 'tt0246433', 'Apnapan', '1977', 6.6, 59), (1946, 'tt0379818', 'I Proud to Be an Indian', '2004', 6.3, 468), (1947, 'tt0101284', 'Akayla', '1991', 5.1, 352), (1948, 'tt0155227', 'Swarag Se Sunder', '1986', 5.5, 76), (1949, 'tt1579806', 'Maska', '2009', 5.2, 224), (1950, 'tt0051747', 'Howrah Bridge', '1958', 7.0, 147), (1951, 'tt1137434', 'Aabra Ka Daabra', '2004', 2.4, 178), (1952, 'tt2777548', 'Lateef', '2013', 3.7, 109), (1953, 'tt3334348', 'Heartless', '2014', 5.3, 404), (1954, 'tt1709731', 'Pairon Talle', '2010', 5.6, 74), (1955, 'tt0275363', 'Gambler', '1971', 6.7, 116), (1956, 'tt1725803', 'Delhi in a Day', '2011', 6.2, 327), (1957, 'tt1397492', 'Well Done Abba!', '2009', 7.3, 1530), (1958, 'tt1722425', 'Antardwand', '2008', 7.4, 431), (1959, 'tt0274944', 'Teen Devian', '1965', 6.9, 146), (1960, 'tt0237022', 'Balika Badhu', '1976', 6.8, 154), (1961, 'tt1252488', 'Haal-e-Dil', '2008', 3.5, 217), (1962, 'tt2679480', 'Sulemani Keeda', '2014', 7.2, 2394), (1963, 'tt0373972', 'Khullam Khulla Pyaar Karen', '2005', 4.2, 275), (1964, 'tt0438981', 'Silsiilay', '2005', 4.2, 656), (1965, 'tt0362213', 'Taaqatwar', '1989', 5.7, 77), (1966, 'tt1388903', 'Mirch', '2010', 6.6, 778), (1967, 'tt0118673', 'Bade Miyan Chote Miyan', '1998', 5.8, 2269), (1968, 'tt0095198', 'Gangaa Jamunaa Saraswathi', '1988', 4.7, 472), (1969, 'tt1201557', 'Acid Factory', '2009', 5.1, 606), (1970, 'tt5326230', 'Aadupuliyattam', '2016', 4.2, 216), (1971, 'tt0358992', 'Bedardi', '1993', 4.0, 119), (1972, 'tt2706264', 'Rangrezz', '2013', 5.2, 367), (1973, 'tt0814082', 'Dil Diya Hai', '2006', 3.6, 413), (1974, 'tt0276628', 'Yehi Hai Zindagi', '1977', 7.6, 96), (1975, 'tt0286620', 'Fareb', '1996', 5.4, 97), (1976, 'tt0337650', 'Jawani Zindabad', '1990', 5.9, 556), (1977, 'tt0053999', 'Kohinoor', '1960', 6.6, 122), (1978, 'tt0080514', 'Chakra', '1981', 7.1, 109), (1979, 'tt4944434', 'Jogi the King', '2005', 8.2, 70), (1980, 'tt0256692', 'Chori Chori Chupke Chupke', '2001', 5.5, 3679), (1981, 'tt0415003', 'Garv: Pride and Honour', '2004', 5.5, 2872), (1982, 'tt0076696', 'Shatranj Ke Khilari', '1977', 7.8, 2643), (1983, 'tt0079236', 'The Great Gambler', '1979', 6.6, 496), (1984, 'tt0377738', 'Dariya Dil', '1988', 4.8, 134), (1985, 'tt2387495', 'Jalpari: The Desert Mermaid', '2012', 7.1, 318), (1986, 'tt0157540', 'Dillagi', '1978', 7.0, 166), (1987, 'tt0172163', 'Bambai Ka Babu', '1996', 5.2, 164), (1988, 'tt0200435', 'Anari No. 1', '1999', 5.1, 529), (1989, 'tt1085806', 'Darling', 'III 2007', 3.9, 526), (1990, 'tt0064915', 'Saat Hindustani', '1969', 6.6, 177), (1991, 'tt0045506', 'Anarkali', '1953', 7.4, 55), (1992, 'tt0104927', 'Mr. Bond', '1992', 5.6, 570), (1993, 'tt2317135', 'Tamanchey: Pyar Mein Dil Pe Maar De Goli', '2014', 4.6, 258), (1994, 'tt0831840', 'Ahista Ahista', '2006', 7.1, 1234), (1995, 'tt0246809', 'New Delhi', '1987', 7.7, 706), (1996, 'tt0366880', 'Parwana', '2003', 2.2, 229), (1997, 'tt0073391', 'Mili', '1975', 7.6, 1073), (1998, 'tt0267928', 'Silsila Hai Pyar Ka', '1999', 2.5, 173), (1999, 'tt0769499', 'Aathi', '2006', 3.9, 1257), (2000, 'tt2188749', 'Gali Gali Chor Hai', '2012', 5.2, 504), (2001, 'tt0116690', 'Jeet', '1996', 5.7, 1821), (2002, 'tt3640942', 'Koyelaanchal', '2014', 5.6, 224), (2003, 'tt0995035', 'Dhol', '2007', 6.1, 3859), (2004, 'tt0157556', 'Dream Girl', '1977', 6.1, 148), (2005, 'tt4453214', 'Mrs. Scooter', '2015', 6.6, 158), (2006, 'tt0117436', 'Rajkumar', '1996', 4.5, 237), (2007, 'tt1596723', 'Click', 'I 2010', 3.5, 285), (2008, 'tt0087474', 'Inquilaab', '1984', 5.9, 207), (2009, 'tt0436451', 'Jo Bole So Nihaal', '2005', 3.5, 538), (2010, 'tt0390667', 'Zulm Ki Hukumat', '1992', 5.5, 112), (2011, 'tt2137064', 'Lanka', '2011', 5.8, 181), (2012, 'tt0097466', 'Guru', 'I 1989', 6.5, 87), (2013, 'tt3477214', 'Kaun Kitney Paani Mein', '2015', 6.2, 388), (2014, 'tt0414713', 'Amu', '2005', 7.4, 775), (2015, 'tt4197476', 'Pattathu Yaanai', '2013', 4.3, 97), (2016, 'tt0453201', 'Sachein', '2005', 6.9, 3314), (2017, 'tt0449306', 'Lucky: No Time for Love', '2005', 4.8, 2844), (2018, 'tt0233147', 'Ayee Milan Ki Bela', '1964', 6.8, 90), (2019, 'tt0187109', 'Gurudev', '1993', 4.9, 105), (2020, 'tt0377340', 'Aetbaar', '2004', 4.7, 878), (2021, 'tt5467702', '1982 - A Love Marriage', '2017', 7.8, 72), (2022, 'tt0811613', 'Meridian Lines', '2013', 6.9, 84), (2023, 'tt0331851', 'Armaan', '2003', 5.3, 1002), (2024, 'tt0405024', 'Hum Kaun Hai?', '2004', 3.9, 266), (2025, 'tt1877691', 'Love Breakups Zindagi', '2011', 5.8, 897), (2026, 'tt1279105', 'Ramaa: The Saviour', '2010', 4.1, 125), (2027, 'tt0475627', 'Shikhar', '2005', 5.0, 962), (2028, 'tt3138602', 'Warning', 'I 2013', 4.8, 198), (2029, 'tt0897413', 'Bhoot Unkle', '2006', 4.3, 104), (2030, 'tt0108256', 'Suraj Ka Satvan Ghoda', '1992', 8.0, 600), (2031, 'tt0053985', 'Kanoon', '1960', 7.7, 287), (2032, 'tt0040067', 'Aag', '1948', 7.4, 296), (2033, 'tt0118931', 'Daud: Fun on the Run', '1997', 6.0, 813), (2034, 'tt0476805', 'Malamaal Weekly', '2006', 6.8, 4516), (2035, 'tt0112359', 'Andolan', '1995', 5.3, 204), (2036, 'tt1221142', 'Mumbai Cutting', '2008', 7.5, 94), (2037, 'tt0385982', 'Raja Bhaiya', '2003', 3.9, 168), (2038, 'tt0044318', 'Aan', '1952', 6.9, 210), (2039, 'tt0244590', 'Kissi Se Na Kehna', '1983', 7.3, 419), (2040, 'tt0321494', 'Bollywood Queen', '2002', 4.7, 426), (2041, 'tt0074858', 'Manthan', '1976', 7.7, 354), (2042, 'tt0058270', 'Kohraa', '1964', 6.7, 100), (2043, 'tt0099076', 'Awaargi', '1990', 6.4, 114), (2044, 'tt0280465', 'Bawandar', '2000', 7.0, 272), (2045, 'tt0077792', 'Kasme Vaade', '1978', 6.8, 359), (2046, 'tt0051117', 'Tumsa Nahin Dekha', '1957', 7.0, 85), (2047, 'tt6467380', 'Hind Ka Napak Ko Jawab', '2017', 7.2, 3942), (2048, 'tt3465488', 'Karle Pyaar Karle', '2014', 3.1, 103), (2049, 'tt0158213', 'Shaukeen', '1982', 7.2, 354), (2050, 'tt0814014', 'Apne', '2007', 6.0, 2335), (2051, 'tt0104645', 'Kshatriya', '1993', 6.0, 463), (2052, 'tt0430589', 'Shaadi Karke Phas Gaya Yaar', '2006', 4.1, 842), (2053, 'tt1740662', 'Benny and Babloo', '2010', 6.1, 290), (2054, 'tt0255401', 'Muskurahat', '1992', 7.1, 248), (2055, 'tt0096139', 'Sone Pe Suhaaga', '1988', 5.0, 64), (2056, 'tt0438894', 'Kisse Pyaar Karoon?', '2009', 3.7, 115), (2057, 'tt0057120', 'Gumrah', '1963', 6.7, 97), (2058, 'tt0044769', 'Jhansi Ki Rani', '1953', 7.2, 69), (2059, 'tt0060145', 'Baharen Phir Bhi Aayengi', '1966', 7.0, 65), (2060, 'tt3129282', 'War Chod Na Yaar', '2013', 5.5, 836), (2061, 'tt2612924', 'Lucky Kabootar', '2014', 5.5, 62), (2062, 'tt1373098', 'Bollywood Hero', '2009', 5.9, 111), (2063, 'tt0047561', 'Taxi Driver', '1954', 7.0, 227), (2064, 'tt0050132', 'Apradhi Kaun?', '1957', 7.8, 52), (2065, 'tt0187193', 'Justice Chaudhury', '1983', 5.6, 60), (2066, 'tt0286478', 'Bade Dilwala', '1999', 3.6, 1258), (2067, 'tt0357710', 'Gora Aur Kala', '1972', 5.4, 85), (2068, 'tt2418644', 'Testing Movie', '2012', 7.1, 69), (2069, 'tt0381408', 'Market', '2003', 3.7, 123), (2070, 'tt0080901', 'Hum Paanch', '1980', 6.5, 150), (2071, 'tt0078204', 'Satyam Shivam Sundaram: Love Sublime', '1978', 7.2, 819), (2072, 'tt0211634', 'Soldier', 'II 1998', 6.1, 2168), (2073, 'tt0213544', 'Chacha Bhatija', '1977', 5.7, 174), (2074, 'tt3720634', '3 AM: A Paranormal Experience', '2014', 5.3, 459), (2075, 'tt0097609', 'Jaadugar', '1989', 4.0, 413), (2076, 'tt0352830', 'Sssshhh...', '2003', 4.5, 283), (2077, 'tt0215859', 'Hulchul', '1971', 7.3, 59), (2078, 'tt0290685', 'Jhuk Gaya Aasman', '1968', 6.8, 136), (2079, 'tt0267845', 'Prem Shakti', '1994', 3.8, 67), (2080, 'tt0100092', 'Maha-Sangram', '1990', 5.9, 120), (2081, 'tt2438764', 'Rajjo', '2013', 3.5, 244), (2082, 'tt0271397', 'Bhoot Bungla', '1965', 6.8, 88), (2083, 'tt0251143', 'Insaaf: The Final Justice', '1997', 5.0, 814), (2084, 'tt0154531', 'Ghar Ek Mandir', '1984', 5.3, 56), (2085, 'tt0376182', 'Shaktiman', '1993', 2.9, 146), (2086, 'tt1471247', 'Road to Sangam', '2010', 7.5, 613), (2087, 'tt0274019', 'Pyaar Diwana Hota Hai', '2002', 3.9, 270), (2088, 'tt0273535', 'Dil Ne Phir Yaad Kiya', '2001', 4.2, 85), (2089, 'tt0044392', 'Baiju Bawra', '1952', 7.5, 279), (2090, 'tt0476861', 'Sarhad Paar', '2006', 4.7, 98), (2091, 'tt0225961', 'Hum Dono', '1961', 7.4, 328), (2092, 'tt0053980', 'Kala Bazar', '1960', 7.2, 199), (2093, 'tt2190820', 'Charlie Kay Chakkar Mein', '2015', 4.5, 145), (2094, 'tt0118546', 'Aar Ya Paar', '1997', 6.2, 182), (2095, 'tt0158587', 'Dhund', '1973', 7.3, 397), (2096, 'tt4228400', 'Sar Ankhon Par', '1999', 6.8, 54), (2097, 'tt0084385', 'Namak Halaal', '1982', 7.4, 1940), (2098, 'tt0449228', 'Dosti: Friends Forever', '2005', 5.9, 2206), (2099, 'tt2976172', 'Sixteen', 'II 2013', 6.3, 1538), (2100, 'tt0078418', 'Trishul', '1978', 7.8, 1710), (2101, 'tt0154183', 'Awwal Number', '1990', 5.4, 987), (2102, 'tt0476819', 'Nehlle Pe Dehlla', '2007', 3.9, 421), (2103, 'tt1334437', 'Blue Oranges', '2009', 6.5, 153), (2104, 'tt0146266', 'Seema', '1955', 7.6, 80), (2105, 'tt0097784', 'Love Love Love', '1989', 5.8, 742), (2106, 'tt0282275', 'Zulmi', '1999', 5.3, 582), (2107, 'tt0359750', 'Ninne Pelladatha', '1996', 7.7, 568), (2108, 'tt0320100', 'Kanyadaan', '1968', 7.4, 60), (2109, 'tt1202344', 'Ab... Bas!', '2004', 3.4, 63), (2110, 'tt0337585', 'Bomb the System', '2002', 6.6, 960), (2111, 'tt0286705', 'Hum Dono', '1995', 6.1, 146), (2112, 'tt0158294', 'Tumse Achha Kaun Hai', '1969', 6.4, 65), (2113, 'tt0470521', 'Saawan... The Love Season', '2006', 2.2, 671), (2114, 'tt1194608', 'Ek Vivaah... Aisa Bhi', '2008', 5.4, 372), (2115, 'tt0148692', 'Safar', '1970', 7.4, 299), (2116, 'tt0239452', 'Haseena Maan Jayegi', '1968', 7.2, 80), (2117, 'tt0280968', 'My Dear Kuttichaathan', '1984', 7.4, 306), (2118, 'tt0073627', 'Rikki-Tikki-Tavi', '1979', 7.7, 486), (2119, 'tt0418096', 'Rakht', '2004', 3.9, 537), (2120, 'tt0115616', 'Bal Bramhachari', '1996', 3.7, 87), (2121, 'tt0073569', 'Prem Kahani', '1975', 6.6, 122), (2122, 'tt0149814', 'Be-Imaan', '1972', 7.1, 67), (2123, 'tt0246740', 'Kora Kagaz', '1974', 7.3, 213), (2124, 'tt0368580', 'Asambhav', '2004', 3.5, 513), (2125, 'tt0847889', 'Narasimhudu', '2005', 3.2, 397), (2126, 'tt0062640', 'Aadmi', '1968', 7.0, 159), (2127, 'tt7282370', 'Who', 'I 2018', 7.1, 140), (2128, 'tt0152720', 'Swami', '1977', 6.8, 108), (2129, 'tt0206554', 'Anpadh', '1962', 7.3, 79), (2130, 'tt0286910', 'Rajaji', '1999', 4.5, 242), (2131, 'tt1363363', 'Chatur Singh Two Star', '2011', 1.8, 530), (2132, 'tt5933706', 'Saansein: The Last Breath', '2016', 3.9, 88), (2133, 'tt0896968', 'Red: The Dark Side', '2007', 3.4, 273), (2134, 'tt5068892', 'Kerry on Kutton', '2016', 5.6, 188), (2135, 'tt1221133', 'EMI: Liya Hai To Chukana Padega', '2008', 4.3, 498), (2136, 'tt6545212', 'Leera the Soulmate', '2018', 9.2, 789), (2137, 'tt1663647', 'Help', '2010', 4.5, 317), (2138, 'tt6041458', 'Tutak Tutak Tutiya', '2016', 5.4, 608), (2139, 'tt2362778', 'Saheb Biwi Aur Gangster Returns', '2013', 6.8, 3466), (2140, 'tt0116407', 'Ghatak: Lethal', '1996', 7.4, 2670), (2141, 'tt0348258', 'Tujhe Meri Kasam', '2003', 6.5, 307), (2142, 'tt0476729', 'Aksar', '2006', 4.9, 934), (2143, 'tt0411815', 'Run', '2004', 4.8, 1124), (2144, 'tt1016159', 'Chain Kulii Ki Main Kulii', '2007', 5.4, 693), (2145, 'tt3619854', 'Creature', 'I 2014', 3.2, 1117), (2146, 'tt0100857', 'Vardi', '1989', 4.8, 124), (2147, 'tt2256514', 'Rush', '2012', 3.8, 772), (2148, 'tt0204823', 'Angrakshak', '1995', 4.3, 127), (2149, 'tt0268449', 'Mehbooba', '1976', 6.6, 112), (2150, 'tt0068532', 'Ek Nazar', '1972', 6.1, 79), (2151, 'tt0154178', 'Aaina', '1977', 7.0, 78), (2152, 'tt0154844', 'Maqsad', '1984', 5.9, 53), (2153, 'tt0115009', 'Yaraana', '1995', 4.6, 275), (2154, 'tt3309844', 'Titoo MBA', '2014', 4.8, 84), (2155, 'tt1266545', 'Chamku', '2008', 4.2, 450), (2156, 'tt1047459', 'Dharm', '2007', 7.9, 871), (2157, 'tt0045693', 'Do Bigha Zamin', '1953', 8.4, 1464), (2158, 'tt0477253', 'Aashiq Banaya Aapne: Love Takes Over', '2005', 4.2, 1312), (2159, 'tt1744641', 'Ramayana: The Epic', '2010', 5.9, 345), (2160, 'tt0083349', 'Yaarana', '1981', 7.1, 1315), (2161, 'tt0115019', 'Yeh Majhdhaar', '1996', 3.6, 278), (2162, 'tt0075747', 'Bhumika', '1977', 7.5, 413), (2163, 'tt0080376', 'Barsaat Ki Ek Raat', '1981', 6.4, 255), (2164, 'tt0290183', 'Elaan', '1994', 6.0, 762), (2165, 'tt2033947', 'Aazaan', '2011', 4.5, 323), (2166, 'tt1573482', 'Striker', '2010', 7.1, 949), (2167, 'tt0131852', 'Azaad', '1955', 6.8, 107), (2168, 'tt1895476', 'Memories in March', '2010', 7.1, 334), (2169, 'tt1579526', 'Anjaneyulu', '2009', 4.8, 257), (2170, 'tt1245732', 'Red Alert: The War Within', '2009', 6.4, 312), (2171, 'tt0105313', 'Sangeet', '1992', 5.8, 128), (2172, 'tt1646217', 'Mr. Singh/Mrs. Mehta', '2010', 4.4, 174), (2173, 'tt0225029', 'Aflatoon', '1997', 4.9, 1110), (2174, 'tt0326745', 'Baaghi', '2000', 4.4, 157), (2175, 'tt1190894', 'Road, Movie', '2009', 6.9, 1566), (2176, 'tt2560016', 'Rajdhani Express', '2013', 3.1, 238), (2177, 'tt3255586', 'P Se PM Tak', '2015', 4.0, 63), (2178, 'tt0187202', 'Kaun Sachcha Kaun Jhootha', '1997', 5.7, 60), (2179, 'tt1170399', 'C Kkompany', '2008', 5.4, 826), (2180, 'tt2187972', 'Shirin Farhad Ki Toh Nikal Padi', '2012', 4.3, 966), (2181, 'tt0105599', 'Tirangaa', '1992', 6.9, 1588), (2182, 'tt0362771', 'Ishq Vishk', '2003', 6.1, 3529), (2183, 'tt0063404', 'Padosan', '1968', 8.1, 4954), (2184, 'tt0268401', 'Kahin Pyaar Na Ho Jaaye', '2000', 4.4, 1585), (2185, 'tt0410952', 'Charas: A Joint Effort', '2004', 5.3, 278), (2186, 'tt6941654', \"Hanuman Da' Damdaar\", '2017', 6.6, 73), (2187, 'tt0251426', 'Tarazu', '1997', 5.1, 538), (2188, 'tt0090138', 'Tawaif', '1985', 7.0, 72), (2189, 'tt0395701', 'Papa the Great', '2000', 5.5, 51), (2190, 'tt0307866', 'Filhaal...', '2002', 5.6, 217), (2191, 'tt0112269', 'Aao Pyaar Karen', '1994', 4.1, 122), (2192, 'tt0352489', 'Kash... Aap Hamare Hote', '2003', 5.3, 76), (2193, 'tt0231348', 'Cheluvi', '1992', 7.7, 71), (2194, 'tt1608768', 'City of Gold - Mumbai 1982: Ek Ankahee Kahani', '2010', 6.8, 334), (2195, 'tt4513812', 'Gour Hari Dastaan: The Freedom File', '2015', 7.5, 400), (2196, 'tt0244409', 'Asli-Naqli', '1962', 7.4, 211), (2197, 'tt0260066', 'Haathkadi', '1995', 4.4, 100), (2198, 'tt0357926', 'Mr. X in Bombay', '1964', 7.1, 80), (2199, 'tt0366616', 'Inkaar', '1977', 7.1, 146), (2200, 'tt0120117', 'Shikari', '2000', 4.8, 364), (2201, 'tt2998196', 'Kuku Mathur Ki Jhand Ho Gayi', '2014', 5.3, 228), (2202, 'tt0446889', 'Aatank', '1996', 4.1, 72), (2203, 'tt1579748', 'Chenna Kesava Reddy', '2002', 6.0, 314), (2204, 'tt0845492', 'My Name Is Anthony Gonsalves', '2008', 3.7, 431), (2205, 'tt0414040', 'Gayab', '2004', 4.2, 677), (2206, 'tt0442764', 'Dil Jo Bhi Kahey...', '2005', 5.0, 299), (2207, 'tt0439714', 'Mumbai Express', '2005', 5.8, 954), (2208, 'tt0119427', 'Judaai', '1997', 6.1, 1848), (2209, 'tt4545630', 'Sardaar Gabbar Singh', '2016', 5.0, 1960), (2210, 'tt0061916', 'The Long Duel', '1967', 5.9, 446), (2211, 'tt0067668', 'Reshma Aur Shera', '1971', 6.7, 162), (2212, 'tt0062897', 'Diwana', '1967', 6.8, 86), (2213, 'tt0244443', 'Buddha Mil Gaya', '1971', 7.1, 181), (2214, 'tt1178657', 'Superstar', '2008', 6.0, 625), (2215, 'tt0292606', 'Nazar Ke Samne', '1995', 6.0, 430), (2216, 'tt0230991', 'Zabardast', '1985', 5.5, 67), (2217, 'tt0449447', 'Zameer', '2005', 3.0, 288), (2218, 'tt3483596', 'O Teri', '2014', 3.5, 506), (2219, 'tt1669648', 'Kshay', '2011', 6.9, 275), (2220, 'tt0362798', 'Kasam Paida Karne Wale Ki', '1984', 6.3, 95), (2221, 'tt1368453', 'Chargesheet', '2011', 6.3, 81), (2222, 'tt0454481', 'James', '2005', 3.7, 303), (2223, 'tt5359624', 'Crd', '2016', 7.5, 51), (2224, 'tt0104548', 'Jeevan Ek Sanghursh', '1990', 4.9, 134), (2225, 'tt0995840', 'Aap Kaa Surroor: The Moviee - The Real Luv Story', '2007', 2.3, 1518), (2226, 'tt1189006', 'Toonpur Ka Superrhero', '2010', 4.3, 572), (2227, 'tt0080653', 'Dostana', '1980', 6.8, 787), (2228, 'tt5439424', 'Railway Children', '2016', 8.1, 126), (2229, 'tt0170704', 'Train to Pakistan', '1998', 6.9, 216), (2230, 'tt1234250', 'Mehbooba', '2008', 3.6, 295), (2231, 'tt0301250', 'Tera Mera Saath Rahen', '2001', 5.0, 266), (2232, 'tt0044761', 'Jaal', '1952', 7.5, 87), (2233, 'tt0366761', 'Meenaxi: Tale of 3 Cities', '2004', 6.3, 277), (2234, 'tt0077226', 'Besharam', '1978', 6.2, 77), (2235, 'tt0368233', 'Sahasa Veerudu Sagara Kanya', '1996', 6.2, 123), (2236, 'tt0262714', 'Pyaasa Sawan', '1981', 7.7, 54), (2237, 'tt0044527', 'Daag', '1952', 6.9, 132), (2238, 'tt0178195', 'Aap Ke Deewane', '1980', 5.7, 166), (2239, 'tt0317699', 'Ekka Raja Rani', '1994', 5.3, 55), (2240, 'tt0377131', 'Raghu Romeo', '2003', 7.0, 571), (2241, 'tt2356959', 'Ajab Gazabb Love', '2012', 4.2, 475), (2242, 'tt0457940', 'Home Delivery: Aapko... Ghar Tak', '2005', 3.4, 525), (2243, 'tt0207415', 'Doli Saja Ke Rakhna', '1998', 6.3, 402), (2244, 'tt1612611', 'Phoonk 2', '2010', 3.3, 447), (2245, 'tt1176911', 'Quick Gun Murugun: Misadventures of an Indian Cowboy', '2009', 6.0, 741), (2246, 'tt0119721', 'Mrityudaata', '1997', 2.5, 333), (2247, 'tt0444913', 'Tango Charlie', '2005', 6.0, 1365), (2248, 'tt0284083', 'Ek Rishtaa: The Bond of Love', '2001', 5.4, 1376), (2249, 'tt2193145', 'Maharana Pratap: The First Freedom Fighter', '2012', 8.1, 90), (2250, 'tt0475735', 'Super', '2005', 5.1, 511), (2251, 'tt0109391', 'Chaand Kaa Tukdaa', '1994', 3.9, 350), (2252, 'tt0469027', 'Bangaram', '2006', 4.8, 673), (2253, 'tt0290258', 'Paandav', '1995', 5.5, 508), (2254, 'tt0449389', 'Shaadi No. 1', '2005', 3.6, 746), (2255, 'tt1083447', 'Buddha Mar Gaya', '2007', 2.5, 316), (2256, 'tt5130348', 'Love Shagun', '2016', 5.8, 88), (2257, 'tt0814295', 'Rocky', '2006', 4.1, 286), (2258, 'tt0885398', 'Anamika: The Untold Story', '2008', 3.3, 229), (2259, 'tt0216067', 'Parichay', '1972', 7.6, 711), (2260, 'tt0046703', 'Amar', '1954', 6.9, 126), (2261, 'tt1176960', 'One Two Three', '2008', 5.2, 2123), (2262, 'tt0082594', 'Kaalia', '1981', 6.9, 1057), (2263, 'tt0310254', 'Tumko Na Bhool Paayenge', '2002', 5.6, 2213), (2264, 'tt5780570', 'A Scandall', '2016', 4.5, 94), (2265, 'tt0079584', 'Mr. Natwarlal', '1979', 6.6, 1181), (2266, 'tt0056052', 'Half Ticket', '1962', 7.5, 399), (2267, 'tt1656171', 'Dam999', '2011', 3.7, 884), (2268, 'tt0077596', 'Ganga Ki Saugand', '1978', 5.9, 101), (2269, 'tt0275608', 'Pyaar Mein Kabhi Kabhi...', '1999', 4.5, 145), (2270, 'tt0435390', 'Shukriya: Till Death Do Us Apart', '2004', 5.5, 125), (2271, 'tt3826186', 'Jaanisaar', '2015', 6.7, 98), (2272, 'tt0071730', 'Kunwara Baap', '1974', 7.2, 309), (2273, 'tt0158845', 'Pati Patni Aur Woh', '1978', 7.1, 274), (2274, 'tt0283005', 'Sri Shirdi Saibaba Mahathyam', '1986', 7.7, 75), (2275, 'tt0158074', 'Patita', '1953', 7.9, 54), (2276, 'tt0779768', 'Teesri Aankh: The Hidden Camera', '2006', 2.7, 257), (2277, 'tt2198101', 'Peddlers', '2012', 7.2, 71), (2278, 'tt0158092', 'Piya Ka Ghar', '1972', 6.8, 150), (2279, 'tt1980970', 'Gandhi to Hitler', '2011', 2.7, 181), (2280, 'tt1582637', 'Varudu', '2010', 3.7, 745), (2281, 'tt0845535', 'Shakalaka Boom Boom', '2007', 2.6, 485), (2282, 'tt0232734', 'Sunny', '1984', 5.6, 104), (2283, 'tt0215902', 'Kinara', '1977', 6.8, 100), (2284, 'tt0121232', 'Dulaara', '1994', 4.7, 214), (2285, 'tt0168516', 'Biwi Ho To Aisi', '1988', 6.0, 652), (2286, 'tt0246729', 'Khiladiyon Ka Khiladi', '1996', 6.4, 5171), (2287, 'tt0896934', 'Gafla', '2006', 7.1, 347), (2288, 'tt0058759', 'Yaadein', '1964', 7.2, 60), (2289, 'tt0080343', 'Albert Pinto Ko Gussa Kyon Ata Hai', '1980', 7.5, 341), (2290, 'tt0216019', 'Namkeen', '1982', 7.7, 321), (2291, 'tt0324217', 'Thanga Magan', '1983', 6.6, 88), (2292, 'tt2224254', '3 Bachelors', '2012', 3.4, 148), (2293, 'tt0300980', 'Baseraa', '1981', 6.9, 104), (2294, 'tt1340778', 'I Am 24', '2010', 6.7, 1130), (2295, 'tt0816657', 'Speed', '2007', 2.9, 464), (2296, 'tt0392950', 'Ishq Hai Tumse', '2004', 3.3, 134), (2297, 'tt0093293', 'Jalwa', '1987', 6.8, 117), (2298, 'tt0086170', 'Rang Birangi', '1983', 7.3, 374), (2299, 'tt0150251', 'Daag: A Poem of Love', '1973', 7.0, 345), (2300, 'tt0497573', 'Sandai Kozhi', '2005', 6.8, 361), (2301, 'tt0252277', 'Bichhoo', '2000', 4.6, 1652), (2302, 'tt1537032', 'Kanthaswamy', '2009', 5.0, 1294), (2303, 'tt0206067', 'Kachche Dhaage', '1999', 6.0, 1177), (2304, 'tt4707940', 'Miss Tanakpur Haazir Ho', '2015', 5.3, 288), (2305, 'tt0397551', 'My Bollywood Bride', '2006', 4.7, 592), (2306, 'tt2261829', 'Marry Me - Aber bitte auf Indisch', '2015', 5.1, 80), (2307, 'tt1314367', 'Ru-Ba-Ru', '2008', 5.5, 217), (2308, 'tt2402131', 'Kismet Love Paisa Dilli', '2012', 4.4, 522), (2309, 'tt0070097', 'Gehri Chaal', '1973', 5.9, 99), (2310, 'tt0264415', 'Bhairava Dweepam', '1994', 7.7, 399), (2311, 'tt0140784', 'Aakhri Khat', '1966', 7.2, 100), (2312, 'tt0089959', 'Sanjog', '1985', 7.1, 61), (2313, 'tt0348521', 'Aapko Pehle Bhi Kahin Dekha Hai', '2003', 4.9, 207), (2314, 'tt0072026', 'Prem Nagar', '1974', 7.2, 83), (2315, 'tt0053629', 'Bombai Ka Babu', '1960', 7.1, 115), (2316, 'tt3777164', 'Chauranga', '2014', 6.6, 277), (2317, 'tt2400377', 'Kaksparsh', '2012', 7.8, 667), (2318, 'tt0304204', 'Isi Ka Naam Zindagi', '1992', 6.0, 537), (2319, 'tt0186871', 'Bhagwaan Dada', '1986', 4.6, 146), (2320, 'tt0286907', 'Qila', '1998', 5.6, 86), (2321, 'tt0069711', 'Anamika', '1973', 7.1, 238), (2322, 'tt0947037', 'Ladies Tailor', '2006', 3.6, 103), (2323, 'tt0367921', 'Kahani Kismat Ki', '1973', 6.1, 68), (2324, 'tt0099651', 'Ghar Ho To Aisa', '1990', 5.9, 265), (2325, 'tt4272420', 'Kuch Kuch Locha Hai', '2015', 2.6, 672), (2326, 'tt0111308', 'Suhaag', '1994', 6.1, 1515), (2327, 'tt0214178', 'Sooryavansham', '1999', 6.2, 2183), (2328, 'tt0156663', 'Kabzaa', '1988', 6.2, 136), (2329, 'tt0141135', 'Gopi', '1970', 6.8, 153), (2330, 'tt1575640', 'Narasimha Nayudu', '2001', 7.1, 400), (2331, 'tt0359135', 'Dus Numbri', '1976', 6.0, 89), (2332, 'tt0215196', 'Split Wide Open', '1999', 6.4, 212), (2333, 'tt0221982', 'Gaja Gamini', '2000', 5.2, 674), (2334, 'tt1740092', 'Chitkabrey', '2011', 4.1, 123), (2335, 'tt0156641', 'Jeevan Mrityu', '1970', 7.1, 153), (2336, 'tt1633212', 'Kajraare', '2010', 3.0, 354), (2337, 'tt4010306', 'Jigariyaa', '2014', 4.9, 284), (2338, 'tt0489560', 'Shaadi Se Pehle', '2006', 4.1, 669), (2339, 'tt0395453', 'Anubhav', '1986', 6.4, 154), (2340, 'tt1238735', 'Dhoom Dadakka', '2008', 3.5, 74), (2341, 'tt0172534', 'Hameshaa', '1997', 5.4, 523), (2342, 'tt0137958', 'Mirza Ghalib', '1954', 7.7, 88), (2343, 'tt0154591', 'Hindustan Ki Kasam', '1973', 6.4, 108), (2344, 'tt0099080', 'Baaghi: A Rebel for Love', '1990', 5.9, 936), (2345, 'tt0070935', 'Yaadon Ki Baaraat', '1973', 7.4, 1681), (2346, 'tt1753640', \"Crook: It's Good to Be Bad\", '2010', 5.1, 1597), (2347, 'tt4857886', 'Do Lafzon Ki Kahani', '2016', 6.5, 1120), (2348, 'tt0270321', 'Dr. Babasaheb Ambedkar', '2000', 8.3, 680), (2349, 'tt0374848', 'Main Madhuri Dixit Banna Chahti Hoon!', '2003', 6.1, 483), (2350, 'tt0097640', 'Kala Bazaar', '1989', 5.6, 163), (2351, 'tt0074503', 'An Evening in Paris', '1967', 6.9, 238), (2352, 'tt0156585', 'Gunaah', '1993', 5.0, 59), (2353, 'tt0156258', 'Aavishkar', '1974', 7.1, 148), (2354, 'tt0449189', 'Banaras', '2006', 6.2, 294), (2355, 'tt0845529', 'Risk', '2007', 6.7, 502), (2356, 'tt0391274', 'Kismat', '2004', 3.8, 389), (2357, 'tt1577050', 'Fired', 'I 2010', 4.7, 197), (2358, 'tt0984122', 'Swami', '2007', 7.1, 317), (2359, 'tt0220596', 'Khauff', '2000', 4.6, 309), (2360, 'tt0315319', 'Boxer', '1984', 6.1, 108), (2361, 'tt2236494', 'Shudra the Rising', '2012', 6.9, 183), (2362, 'tt0156256', 'Aaj', '1987', 7.1, 251), (2363, 'tt1696191', 'Tere Mere Phere', '2011', 5.3, 238), (2364, 'tt0364530', 'Mr. Romeo', '1996', 5.4, 129), (2365, 'tt0102706', 'Prem Qaidi', '1991', 2.2, 111), (2366, 'tt0444915', 'Tathastu', '2006', 4.5, 376), (2367, 'tt1437367', 'Thanks Maa', '2009', 7.9, 660), (2368, 'tt0298606', 'Yeh Teraa Ghar Yeh Meraa Ghar', '2001', 5.7, 671), (2369, 'tt1363125', 'Naughty @ 40', '2011', 2.6, 261), (2370, 'tt0449159', '15 Park Avenue', '2005', 7.2, 935), (2371, 'tt0097264', 'Eeshwar', '1989', 7.3, 403), (2372, 'tt1454461', 'Ek: The Power of One', '2009', 5.0, 264), (2373, 'tt2100543', 'Hum Hai Raahi CAR Ke', '2013', 3.2, 132), (2374, 'tt0151156', 'Jugnu', '1947', 7.5, 60), (2375, 'tt0215900', 'Khushboo', '1975', 7.1, 195), (2376, 'tt1172587', 'Delhi Safari', '2012', 5.7, 1429), (2377, 'tt0110280', 'Krantiveer', '1994', 7.4, 1573), (2378, 'tt0097812', 'Main Azaad Hoon', '1989', 7.3, 623), (2379, 'tt4610308', 'Ishqedarriyaan', '2015', 5.0, 125), (2380, 'tt0046427', 'Thunder in the East', '1953', 6.5, 228), (2381, 'tt1095421', 'Ramchand Pakistani', '2008', 7.6, 1157), (2382, 'tt0215245', 'Thakshak', '1999', 6.0, 500), (2383, 'tt0298078', 'Navrang', '1959', 6.5, 112), (2384, 'tt0063540', 'Sunghursh', '1968', 7.2, 72), (2385, 'tt1169841', 'Return of Hanuman', '2007', 7.0, 436), (2386, 'tt0290179', 'Dil Ki Baazi', '1993', 6.4, 441), (2387, 'tt1659205', 'Harud', '2010', 6.6, 185), (2388, 'tt1738293', 'Khichdi: The Movie', '2010', 7.2, 1609), (2389, 'tt0126234', 'Chamatkar', 'I 1992', 6.7, 5084), (2390, 'tt3431714', 'Mastram', '2013', 4.8, 399), (2391, 'tt0470398', 'Kasak', '2005', 4.7, 53), (2392, 'tt0245494', 'Teesri Aankh', '1982', 6.8, 61), (2393, 'tt0300160', 'Maalamaal', '1988', 7.0, 318), (2394, 'tt0148353', 'Jeene Ki Raah', '1969', 7.1, 58), (2395, 'tt1206283', 'Black & White', 'I 2008', 6.2, 528), (2396, 'tt0165610', 'Agent Vinod', '1977', 5.9, 66), (2397, 'tt0281656', 'Bhopal Express', '1999', 7.2, 346), (2398, 'tt0095300', 'Hifazat', '1987', 5.4, 118), (2399, 'tt1170411', 'Victory', 'I 2009', 3.8, 414), (2400, 'tt0076177', 'Immaan Dharam', '1977', 5.6, 141), (2401, 'tt0447301', 'The Memsahib', '2006', 6.5, 79), (2402, 'tt0137349', 'Albela', '1951', 6.6, 58), (2403, 'tt2365873', 'Kevi Rite Jaish', '2012', 8.1, 775), (2404, 'tt1772332', \"Pappu Can't Dance Saala\", '2010', 6.5, 384), (2405, 'tt0286503', 'Betaabi', '1997', 4.9, 76), (2406, 'tt0067421', 'Mere Apne', '1971', 7.6, 426), (2407, 'tt0376158', 'Sar Utha Ke Jiyo', '1998', 2.7, 121), (2408, 'tt0158579', 'Dastaan', '1972', 7.2, 81), (2409, 'tt0246254', 'State Rowdy', '1989', 7.5, 83), (2410, 'tt0454391', 'Antarmahal: Views of the Inner Chamber', '2005', 7.1, 414), (2411, 'tt0206238', 'Raaz', '1967', 7.0, 93), (2412, 'tt1242843', 'Contract', '2008', 4.3, 203), (2413, 'tt0814012', 'Anuranan', '2006', 6.7, 297), (2414, 'tt2191641', 'Chaar Din Ki Chandni', '2012', 4.4, 362), (2415, 'tt0485466', 'Jawani Diwani: A Youthful Joyride', '2006', 2.9, 416), (2416, 'tt0081968', '36 Chowringhee Lane', '1981', 7.6, 334), (2417, 'tt0154390', 'Des Pardes', '1978', 7.1, 126), (2418, 'tt0272607', 'Farz', '2001', 4.3, 355), (2419, 'tt0070637', 'Saudagar', '1973', 7.2, 622), (2420, 'tt0079386', 'Kaala Patthar', '1979', 7.7, 1793), (2421, 'tt0338477', 'Talaash: The Hunt Begins...', '2003', 5.1, 1260), (2422, 'tt0437182', 'Family: Ties of Blood', '2006', 5.7, 1696), (2423, 'tt0087886', 'Paroma', '1984', 6.7, 130), (2424, 'tt0497620', 'Fun: Can Be Dangerous Sometimes', '2005', 3.3, 65), (2425, 'tt0239244', 'Ashanti', '1982', 7.0, 86), (2426, 'tt0193246', 'Jaan Se Pyaara', '1992', 6.1, 80), (2427, 'tt0093263', 'Insaaf', '1987', 7.0, 52), (2428, 'tt1287847', 'Mukhbiir', '2008', 6.9, 351), (2429, 'tt0085138', 'Adi Shankaracharya', '1983', 7.4, 193), (2430, 'tt2339549', 'Maximum', '2012', 4.9, 183), (2431, 'tt0089382', 'Joshilaay', '1989', 5.7, 217), (2432, 'tt3095238', 'Utopia', '2015', 6.5, 70), (2433, 'tt0172646', 'Karz Chukana Hai', '1991', 5.6, 67), (2434, 'tt0458017', 'Mixed Doubles', '2006', 6.5, 595), (2435, 'tt0102201', 'Khoon Ka Karz', '1991', 5.0, 62), (2436, 'tt1538210', 'Aao Wish Karein', '2009', 5.3, 531), (2437, 'tt0206240', 'Raja Rani', '1973', 6.7, 74), (2438, 'tt2175671', 'Ghost', 'I 2012', 2.3, 203), (2439, 'tt0409724', 'Bardaasht', '2004', 6.1, 494), (2440, 'tt6016672', 'Mirza Juuliet', '2017', 5.0, 82), (2441, 'tt2554042', 'Maazii', '2013', 6.8, 61), (2442, 'tt0255543', 'Raja Ko Rani Se Pyar Ho Gaya', '2000', 3.3, 66), (2443, 'tt1814683', 'Diary of a Butterfly', '2012', 3.5, 79), (2444, 'tt1740670', 'Dus Tola', '2010', 6.9, 265), (2445, 'tt1372694', 'Gumnaam: The Mystery', '2008', 3.5, 68), (2446, 'tt1625856', 'Badrinath', '2011', 4.4, 1399), (2447, 'tt1582519', 'Khaleja', '2010', 7.6, 6972), (2448, 'tt0173081', 'Pyaar To Hona Hi Tha', '1998', 6.7, 2480), (2449, 'tt0113822', 'Milan', '1995', 5.5, 68), (2450, 'tt0073924', 'Zameer', '1975', 6.1, 271), (2451, 'tt0387164', 'Dhoop', '2003', 7.2, 214), (2452, 'tt1946280', 'Noukadubi', '2011', 7.6, 597), (2453, 'tt0296705', 'Kayda Kanoon', '1993', 6.3, 461), (2454, 'tt1260689', 'Summer 2007', '2008', 6.3, 219), (2455, 'tt0177505', 'Aakhri Adaalat', '1988', 5.4, 56), (2456, 'tt5008256', 'Saanjh', '2017', 8.6, 116), (2457, 'tt0319121', 'Bulundi', '1981', 7.0, 77), (2458, 'tt1327833', 'Sorry Bhai!', '2008', 6.1, 990), (2459, 'tt6346954', 'Shor Se Shuruaat', '2016', 6.7, 118), (2460, 'tt0495111', 'The Forest', 'I 2009', 6.2, 98), (2461, 'tt0335592', 'Pyaasa', '2002', 3.5, 84), (2462, 'tt1024839', 'De Taali', '2008', 4.0, 710), (2463, 'tt6189880', 'Alif', 'I 2017', 6.7, 116), (2464, 'tt0348172', 'Tehzeeb', '2003', 6.1, 459), (2465, 'tt0178218', 'Abhinetri', '1970', 6.8, 51), (2466, 'tt0105007', 'Nishchaiy', '1992', 4.2, 288), (2467, 'tt0270219', 'Baat Ban Jaye', '1986', 6.4, 130), (2468, 'tt0229952', 'Aaye Din Bahar Ke', '1966', 7.3, 78), (2469, 'tt0215338', 'Vishwanath', '1978', 6.5, 107), (2470, 'tt0454314', 'Vivekananda', '1994', 7.0, 109), (2471, 'tt0989725', 'Undertrial', '2007', 6.5, 199), (2472, 'tt0371747', 'Itni Si Baat', '1981', 6.9, 52), (2473, 'tt1292642', 'Allah Ke Banday', '2010', 6.3, 279), (2474, 'tt0439872', 'Wajahh: A Reason to Kill', '2004', 4.1, 58), (2475, 'tt1372304', 'Wafaa', '2008', 3.8, 64), (2476, 'tt0073236', 'Khel Khel Mein', '1975', 7.5, 302), (2477, 'tt0349679', 'Khwahish', '2003', 4.2, 184), (2478, 'tt0251395', 'Shapath', '1997', 5.6, 69), (2479, 'tt0418880', 'Mere Jeevan Saathi', '2006', 4.1, 810), (2480, 'tt0084324', 'Meri Jung', '1985', 7.3, 878), (2481, 'tt1218345', 'Dil Kabaddi', '2008', 5.8, 611), (2482, 'tt3426144', 'Babloo Happy Hai', '2014', 6.9, 72), (2483, 'tt5216810', 'Bollywood Diaries', '2016', 6.0, 228), (2484, 'tt0118578', 'Albela', '2001', 3.6, 629), (2485, 'tt0110177', 'Jai Kishen', '1994', 5.9, 572), (2486, 'tt0317117', 'Bade Dil Wala', '1983', 5.9, 80), (2487, 'tt0080638', 'Do Aur Do Paanch', '1980', 7.1, 845), (2488, 'tt1286766', 'Fatso!', '2012', 5.6, 288), (2489, 'tt0322936', 'Kehtaa Hai Dil Baar Baar', '2002', 4.8, 164), (2490, 'tt0028217', 'Sant Tukaram', '1936', 6.8, 191), (2491, 'tt0060310', 'Dil Diya Dard Liya', '1966', 6.8, 88), (2492, 'tt0371558', 'Bairaag', '1976', 7.0, 75), (2493, 'tt0139157', 'Dil Ek Mandir', '1963', 7.0, 110), (2494, 'tt0148841', 'Shikar', '1968', 7.1, 78), (2495, 'tt0186838', 'Aulad', '1987', 6.6, 84), (2496, 'tt0175450', 'Apradh', '1972', 7.2, 53), (2497, 'tt1620697', 'Bhavnao Ko Samjho', '2010', 4.1, 63), (2498, 'tt0323546', 'Paayum Puli', '1983', 6.9, 106), (2499, 'tt0482541', 'The Lookalike', '2006', 4.1, 162), (2500, 'tt0102846', 'Saugandh', '1991', 6.9, 794), (2501, 'tt0091328', 'Karma', 'I 1986', 7.3, 1552), (2502, 'tt2130242', 'Tere Naal Love Ho Gaya', '2012', 5.8, 3714), (2503, 'tt6246170', 'Chaar Sahibzaade 2: Rise of Banda Singh Bahadur', '2016', 8.4, 475), (2504, 'tt0053319', 'Sujata', '1960', 7.4, 253), (2505, 'tt4921860', 'MSG 2 the Messenger', '2015', 7.2, 8366), (2506, 'tt0157320', 'Apne Paraye', '1980', 6.8, 128), (2507, 'tt0178392', 'Droh Kaal', '1994', 7.7, 321), (2508, 'tt0102148', 'Jamai Raja', '1990', 5.4, 449), (2509, 'tt0461209', 'Ek Khiladi Ek Haseena', '2005', 5.9, 537), (2510, 'tt0158768', 'Mazdoor', '1983', 6.7, 76), (2511, 'tt3411076', '1:13:7 Ek Tera Saath', '2016', 5.8, 53), (2512, 'tt0156691', \"'Kaash'\", '1987', 6.5, 153), (2513, 'tt0233464', 'Clerk', '1989', 2.4, 100), (2514, 'tt0273164', 'Daman: A Victim of Marital Violence', '2001', 5.7, 74), (2515, 'tt0107820', 'Phool', '1993', 5.2, 83), (2516, 'tt0067742', 'Sharmeelee', '1971', 6.9, 118), (2517, 'tt0084840', 'Umbartha', '1982', 6.8, 108), (2518, 'tt0402607', 'Tum: A Dangerous Obsession', '2004', 2.5, 159), (2519, 'tt0108023', 'Sahibaan', '1993', 5.5, 149), (2520, 'tt0158449', 'Aadmi Aur Insaan', '1969', 6.7, 111), (2521, 'tt0259996', 'Ek Hi Bhool', '1981', 5.8, 116), (2522, 'tt0100401', 'Pratibandh', '1990', 6.3, 104), (2523, 'tt0341639', 'Vishwasghaat', '1996', 5.6, 58), (2524, 'tt1887763', 'Haunted - 3D', '2011', 6.2, 1798), (2525, 'tt0326983', 'Jaani Dushman: Ek Anokhi Kahani', '2002', 2.9, 2550), (2526, 'tt0118525', '...Aur Pyaar Ho Gaya', '1997', 4.7, 719), (2527, 'tt0247911', 'Aap Mujhe Achche Lagne Lage', '2002', 3.2, 2016), (2528, 'tt0102835', 'Sanam Bewafa', '1991', 5.5, 814), (2529, 'tt0341458', 'Maseeha', '2002', 4.2, 61), (2530, 'tt0329620', 'Soch', '2002', 4.5, 69), (2531, 'tt0067636', 'Pyar Ki Kahani', '1971', 6.3, 65), (2532, 'tt3257168', 'Shorts', '2013', 6.5, 406), (2533, 'tt0052736', 'Dhool Ka Phool', '1959', 6.9, 123), (2534, 'tt0338855', 'Chhote Sarkar', '1996', 4.8, 189), (2535, 'tt1673411', 'Kushti', '2010', 4.3, 136), (2536, 'tt0087492', 'Jaag Utha Insan', '1984', 6.2, 52), (2537, 'tt0359169', 'Do Qaidi', '1989', 5.4, 80), (2538, 'tt0359324', 'Gharana', '1989', 5.7, 51), (2539, 'tt3037260', 'The Good Road', '2013', 6.3, 508), (2540, 'tt1241334', 'Saas Bahu Aur Sensex', '2008', 5.2, 92), (2541, 'tt0154620', 'Hum Naujawan', '1985', 5.3, 62), (2542, 'tt1939654', 'Bheja Fry 2', '2011', 4.7, 1128), (2543, 'tt0231469', 'Do Badan', '1966', 7.3, 69), (2544, 'tt0050829', 'Paying Guest', '1957', 7.1, 177), (2545, 'tt0358302', 'Zalzala', '1988', 6.0, 63), (2546, 'tt0101437', 'Beta', '1992', 6.3, 1593), (2547, 'tt0286724', 'Itihaas', '1997', 3.4, 239), (2548, 'tt0373935', 'Jaisi Karni Waisi Bharni', '1989', 5.7, 163), (2549, 'tt0085165', 'Andhaa Kaanoon', '1983', 6.2, 548), (2550, 'tt1582601', 'Seetharamula Kalyanam Lankalo', '2010', 4.6, 91), (2551, 'tt0358020', 'Phir Wohi Raat', '1980', 7.1, 60), (2552, 'tt0147843', 'Anokhi Raat', '1968', 7.4, 70), (2553, 'tt2185084', 'From Sydney with Love', '2012', 6.3, 66), (2554, 'tt0273045', '2001: Do Hazaar Ek', '1998', 4.0, 96), (2555, 'tt0187108', 'Guru', '1980', 7.2, 435), (2556, 'tt0483701', 'Karma, Confessions and Holi', '2009', 3.1, 113), (2557, 'tt0785004', 'Just Married: Marriage Was Only the Beginning!', '2007', 5.5, 411), (2558, 'tt3847072', 'Ekkees Toppon Ki Salaami', '2014', 6.1, 462), (2559, 'tt0130991', 'Sangdil', '1952', 7.4, 70), (2560, 'tt0246825', 'Om Dar-B-Dar', '1988', 7.3, 262), (2561, 'tt0053708', 'Chhalia', '1960', 7.5, 166), (2562, 'tt0135393', 'Dhaal: The Battle of Law Against Law', '1997', 5.3, 55), (2563, 'tt0239448', 'Haath Ki Safai', '1974', 6.7, 86), (2564, 'tt0268416', 'Lalkar The Challenge', '1972', 7.2, 92), (2565, 'tt0097642', 'Kanoon Apna Apna', '1989', 5.6, 110), (2566, 'tt7856176', 'Shaadi Teri Bajayenge Hum Band', '2018', 5.3, 164), (2567, 'tt0254683', 'Phool Aur Kaante', '1991', 5.9, 1095), (2568, 'tt0149568', 'Aa Ab Laut Chalen', '1999', 5.5, 1386), (2569, 'tt0286593', 'Diljale', '1996', 5.8, 1299), (2570, 'tt0096896', 'Batwara', '1989', 6.5, 152), (2571, 'tt0266260', 'Arjun', '1985', 7.2, 696), (2572, 'tt0135515', 'Krishna', 'I 1996', 4.6, 244), (2573, 'tt0268131', 'Alag Alag', '1985', 6.5, 68), (2574, 'tt0130197', 'Prithvi', '1997', 5.5, 84), (2575, 'tt0273681', 'Insaniyat', '1955', 7.3, 59), (2576, 'tt2224313', 'Listen... Amaya', '2013', 6.9, 540), (2577, 'tt0302955', 'Qatl', '1986', 7.0, 148), (2578, 'tt1742307', 'Dunno Y Na Jaane Kyun...', '2010', 3.9, 131), (2579, 'tt4579982', 'Ishq Click', '2016', 4.1, 78), (2580, 'tt1674692', 'French Immersion', '2011', 5.1, 255), (2581, 'tt0178562', 'Hazaar Chaurasi Ki Maa', '1998', 7.2, 246), (2582, 'tt1629737', 'Panchakshari', '2010', 4.3, 79), (2583, 'tt2156785', 'Gattu', '2011', 7.2, 372), (2584, 'tt0099929', 'Khilaaf', '1991', 5.5, 64), (2585, 'tt0316154', 'Lal Salaam', '2002', 7.0, 76), (2586, 'tt0444415', 'Return to Rajapur', '2006', 6.3, 149), (2587, 'tt2032530', 'Shabri', '2011', 5.8, 116), (2588, 'tt0437279', 'Khamoshh... Khauff Ki Raat', '2005', 3.3, 207), (2589, 'tt0847180', 'Loot', '2011', 3.6, 278), (2590, 'tt0135647', 'Sharabi', '1964', 7.4, 56), (2591, 'tt0130803', 'Jhumroo', '1961', 6.9, 530), (2592, 'tt0156593', 'Hariyali Aur Rasta', '1962', 6.9, 56), (2593, 'tt6349528', 'Ek Haseena Thi Ek Deewana Tha', '2017', 4.9, 77), (2594, 'tt4600778', 'Chalk N Duster', 'I 2016', 6.6, 513), (2595, 'tt0213611', 'Dharam Veer', '1977', 6.9, 529), (2596, 'tt0233856', 'Hello Brother', '1999', 4.8, 3295), (2597, 'tt0079355', 'Jaani Dushman', '1979', 6.0, 355), (2598, 'tt0090581', 'Abodh', '1984', 6.2, 87), (2599, 'tt0176075', 'Pyar Kiye Jaa', '1966', 7.1, 134), (2600, 'tt0153545', 'Vijeta', '1982', 7.3, 224), (2601, 'tt0417010', 'Nenunnanu', '2004', 5.2, 197), (2602, 'tt0154563', 'Hanste Zakhm', '1973', 7.4, 62), (2603, 'tt0375705', 'Dhill', '2001', 7.1, 628), (2604, 'tt0267813', 'Papi Gudia', '1996', 2.0, 166), (2605, 'tt0366918', 'Premante Idera', '1998', 6.8, 212), (2606, 'tt6114952', 'Nagarahavu', '2016', 5.7, 63), (2607, 'tt2378057', '?: A Question Mark', '2012', 5.5, 272), (2608, 'tt0154942', 'Neecha Nagar', '1946', 7.2, 100), (2609, 'tt0359619', 'Love 86', '1986', 5.9, 63), (2610, 'tt0049243', 'Funtoosh', '1956', 7.3, 60), (2611, 'tt3407604', 'Will You Marry Me', '2012', 5.2, 93), (2612, 'tt4659548', 'Rebellious Flower', '2016', 6.7, 218), (2613, 'tt1153700', 'Patang', '2011', 6.1, 276), (2614, 'tt0079614', 'Nauker', '1979', 6.7, 79), (2615, 'tt0977636', 'Aarya', '2007', 4.8, 152), (2616, 'tt0099081', 'Baap Numbri Beta Dus Numbri', '1990', 6.0, 229), (2617, 'tt0066758', 'Amar Prem', '1972', 8.2, 1260), (2618, 'tt0300028', 'Indian', '2001', 5.3, 1900), (2619, 'tt0246687', 'Hu Tu Tu', '1999', 6.6, 385), (2620, 'tt1961530', 'Tell Me O Kkhuda', '2011', 2.5, 303), (2621, 'tt0052217', 'Solva Saal', '1958', 7.0, 108), (2622, 'tt1278447', 'Sikandar', '2009', 6.4, 244), (2623, 'tt0231150', 'Anita', '1967', 6.8, 86), (2624, 'tt0230993', 'Zamaane Ko Dikhana Hai', '1981', 7.2, 96), (2625, 'tt2009592', 'The Blueberry Hunt', '2016', 5.6, 180), (2626, 'tt1570417', 'Accident on Hill Road', '2010', 4.2, 266), (2627, 'tt0341323', 'Cover Story', '2000', 5.8, 74), (2628, 'tt1582560', 'Namo Venkatesha', '2010', 5.4, 328), (2629, 'tt1754202', 'Michael', 'I 2011', 5.8, 86), (2630, 'tt0107199', 'In Custody', '1994', 7.1, 308), (2631, 'tt0061378', 'Baharon Ke Sapne', '1967', 7.7, 64), (2632, 'tt2216852', 'Main Aur Mr. Riight', '2014', 5.6, 285), (2633, 'tt0305173', 'Aamdani Atthanni Kharcha Rupaiya', '2001', 4.6, 945), (2634, 'tt0359614', 'Loha', '1987', 6.2, 162), (2635, 'tt0156658', 'Junoon', '1992', 5.7, 392), (2636, 'tt0182251', 'Jab Pyaar Kisise Hota Hai', '1998', 5.9, 1989), (2637, 'tt0476762', 'Big Brother', '2007', 3.9, 673), (2638, 'tt0187227', 'Laadla', '1994', 5.8, 927), (2639, 'tt0399608', 'Santosham', '2002', 7.2, 474), (2640, 'tt1228707', 'Jimmy', '2008', 1.6, 167), (2641, 'tt0056998', 'Dil Hi To Hai', '1963', 7.3, 95), (2642, 'tt0886505', 'Kaafila', '2007', 3.6, 165), (2643, 'tt0435259', 'Padmashree Laloo Prasad Yadav', '2005', 2.5, 162), (2644, 'tt0430030', 'Dobara', '2004', 3.2, 115), (2645, 'tt0157003', 'Saatwan Aasman', '1992', 5.5, 55), (2646, 'tt3948240', 'Second Hand Husband', '2015', 4.1, 134), (2647, 'tt0389726', 'Aanch', '2003', 6.0, 91), (2648, 'tt0887753', 'Kya Love Story Hai', '2007', 3.9, 308), (2649, 'tt0052411', 'Yahudi', '1958', 7.0, 173), (2650, 'tt0045529', 'Baaz', '1953', 7.0, 74), (2651, 'tt0988655', 'A Flat', '2010', 4.7, 179), (2652, 'tt2232524', 'Mere Dost Picture Abhi Baaki Hai', '2012', 3.5, 169), (2653, 'tt0079526', 'Manzil', '1979', 7.0, 220), (2654, 'tt0292113', 'Mitr: My Friend', '2002', 6.7, 230), (2655, 'tt2184125', 'Chaalis Chauraasi', '2012', 6.2, 430), (2656, 'tt0111537', 'Udhaar Ki Zindagi', '1994', 6.1, 116), (2657, 'tt2917382', 'Veera Madakari', '2009', 6.2, 193), (2658, 'tt0461323', 'Pyare Mohan', '2006', 3.3, 703), (2659, 'tt0248125', 'Kaamchor', '1982', 6.5, 107), (2660, 'tt0453188', 'Priyasakhi', '2005', 4.9, 128), (2661, 'tt0135641', 'Sapoot', '1996', 5.7, 894), (2662, 'tt0267246', 'Aatish: Feel the Fire', '1994', 5.9, 628), (2663, 'tt0247992', 'Dulhan Hum Le Jayenge', '2000', 5.0, 2525), (2664, 'tt0461062', 'Holiday', '2006', 4.9, 131), (2665, 'tt1214948', 'Bhram: An Illusion', '2008', 5.7, 97), (2666, 'tt2023645', 'Phhir', '2011', 5.3, 161), (2667, 'tt0359457', 'Imaandaar', '1987', 5.6, 52), (2668, 'tt0158331', 'Victoria No. 203', '1972', 7.1, 155), (2669, 'tt3031896', 'Lakeer Ka Fakeer', '2013', 5.8, 114), (2670, 'tt0086892', 'Andar Baahar', '1984', 5.4, 120), (2671, 'tt0230141', 'Dil Deke Dekho', '1959', 7.0, 114), (2672, 'tt0069871', 'Chhupa Rustam', '1973', 7.2, 65), (2673, 'tt1127220', 'Mr Bhatti on Chutti', '2012', 4.0, 77), (2674, 'tt0831816', 'Shiva', '2006', 4.1, 309), (2675, 'tt0111067', 'Sangdil Sanam', '1994', 3.7, 265), (2676, 'tt0139048', 'Bahu Begum', '1967', 6.8, 53), (2677, 'tt0841134', 'Naughty Boy', '2006', 3.8, 53), (2678, 'tt1695793', 'Love U... Mr. Kalakaar!', '2011', 5.7, 387), (2679, 'tt0090202', 'Trikal Past, Present, Future', '1985', 7.1, 152), (2680, 'tt1740549', 'Thavasi', '2001', 4.7, 63), (2681, 'tt0157882', 'Kamla Ki Maut', '1989', 7.1, 124), (2682, 'tt0233808', 'Godmother', '1999', 6.4, 188), (2683, 'tt0064086', 'Bhuvan Shome', '1969', 7.4, 280), (2684, 'tt0481897', 'Pyasa Shaitan', '1984', 5.2, 52), (2685, 'tt0106287', 'Anmol', '1993', 5.7, 87), (2686, 'tt0249588', 'Hum To Mohabbat Karega', '2000', 2.8, 316), (2687, 'tt0471646', 'Darwaza Bandh Rakho', '2006', 5.3, 301), (2688, 'tt0137362', 'Anuradha', '1960', 6.8, 136), (2689, 'tt0476057', 'Film Star', '2005', 5.4, 55), (2690, 'tt0122162', 'Madhosh', '1974', 6.3, 235), (2691, 'tt0433605', 'Naach', '2004', 4.4, 534), (2692, 'tt0116024', 'Daraar', '1996', 5.4, 416), (2693, 'tt0327004', 'Kuch Tum Kaho Kuch Hum Kahein', '2002', 5.7, 450), (2694, 'tt0273452', 'Bandhan', '1998', 4.9, 1372), (2695, 'tt1114725', 'Heroes', '2008', 5.7, 1811), (2696, 'tt0058045', 'Dulha Dulhan', '1964', 7.2, 69), (2697, 'tt1956433', 'Ata Pata Lapatta', '2012', 5.6, 78), (2698, 'tt0056367', 'Prem Patra', '1962', 7.6, 51), (2699, 'tt2962230', 'Kaagaz Ke Fools', '2015', 3.9, 102), (2700, 'tt1603862', 'Jashnn: The Music Within', '2009', 3.9, 211), (2701, 'tt0286772', 'Kudrat', '1998', 4.2, 105), (2702, 'tt0262866', 'Vakil Babu', '1982', 7.6, 57), (2703, 'tt0385564', 'Border Hindustan Ka', '2003', 5.3, 74), (2704, 'tt0351714', 'Sharafat', '1970', 7.3, 56), (2705, 'tt0187176', 'Jaani Dost', '1983', 6.0, 59), (2706, 'tt4340180', 'Direct Ishq', '2016', 4.0, 61), (2707, 'tt0154578', 'Heera Panna', '1973', 6.6, 214), (2708, 'tt2043923', 'Mod', '2011', 6.5, 392), (2709, 'tt0419974', 'Morning Raga', '2004', 6.8, 287), (2710, 'tt0338959', 'Ek Se Badhkar Ek', '2004', 4.7, 129), (2711, 'tt0275489', 'Mahal', '1969', 7.5, 71), (2712, 'tt1073097', 'Apna Asmaan', '2007', 6.2, 138), (2713, 'tt0147855', 'Aashirwad', '1968', 7.4, 161), (2714, 'tt0250339', 'Dil Tera Diwana', '1962', 7.1, 61), (2715, 'tt0120551', 'Ziddi', '1997', 5.7, 1323), (2716, 'tt0260303', 'Purani Haveli', '1989', 5.3, 182), (2717, 'tt0267424', 'Dhanwaan', '1993', 3.5, 215), (2718, 'tt0250415', 'Har Dil Jo Pyar Karega...', '2000', 5.2, 3602), (2719, 'tt0246905', 'Saajan Chale Sasural', '1996', 5.9, 990), (2720, 'tt0898943', 'Raqeeb', '2007', 4.4, 251), (2721, 'tt0383875', 'Andhaa Yudh', '1987', 7.3, 54), (2722, 'tt0150718', 'Gharaonda', '1977', 6.9, 155), (2723, 'tt0178203', 'Aas Paas', '1981', 6.7, 86), (2724, 'tt0092033', 'Swati', '1986', 6.8, 80), (2725, 'tt0287065', 'Udaan', '1997', 4.3, 114), (2726, 'tt0251139', 'Indrudu Chandrudu', '1989', 7.3, 239), (2727, 'tt0349115', 'Baaz: A Bird in Danger', '2003', 3.2, 289), (2728, 'tt0106724', 'Dil Hi To Hai', '1992', 5.5, 82), (2729, 'tt0453748', 'Kuchh Meetha Ho Jaye', '2005', 3.6, 261), (2730, 'tt0078761', 'Amar Deep', '1979', 6.9, 51), (2731, 'tt0100539', 'Sailaab', '1990', 5.5, 160), (2732, 'tt0155997', 'Rafoo Chakkar', '1975', 7.0, 212), (2733, 'tt2386342', 'Anjaneya', '2003', 3.9, 225), (2734, 'tt0154788', 'Lootmaar', '1980', 6.3, 55), (2735, 'tt0081624', 'Thodisi Bewafaii', '1980', 6.8, 77), (2736, 'tt0297983', 'Ittefaq', '2001', 4.7, 62), (2737, 'tt0246493', 'Chori Mera Kaam', '1975', 7.0, 103), (2738, 'tt0410149', 'Kali Salwaar', '2002', 6.1, 63), (2739, 'tt0343048', 'Surakshaa', '1995', 4.2, 144), (2740, 'tt5573110', 'Yea Toh Two Much Ho Gayaa', '2016', 4.3, 86), (2741, 'tt0377845', 'Ek Chadar Maili Si', '1986', 7.1, 100), (2742, 'tt2265191', 'Bumboo', '2012', 3.6, 139), (2743, 'tt0259322', 'Gehrayee', '1980', 6.8, 115), (2744, 'tt0454504', 'Maine Gandhi Ko Nahin Mara', '2005', 7.4, 665), (2745, 'tt0366276', 'Calcutta Mail', '2003', 5.8, 432), (2746, 'tt0116748', 'Karobaar: The Business of Love', '2000', 4.4, 192), (2747, 'tt0138360', 'Chitralekha', '1964', 7.5, 66), (2748, 'tt2416294', 'Rabba Main Kya Karoon', '2013', 4.4, 266), (2749, 'tt1218038', 'The President Is Coming', '2009', 6.9, 621), (2750, 'tt0293589', 'Shaheed', '1965', 7.9, 293), (2751, 'tt0069218', 'Sanjog', '1972', 6.3, 102), (2752, 'tt0156160', 'Vaishali', '1988', 7.7, 321), (2753, 'tt0989605', 'Aatma', '2006', 4.3, 72), (2754, 'tt0079382', 'Jurmana', '1979', 6.9, 139), (2755, 'tt3357416', 'What the Fish', '2013', 5.1, 143), (2756, 'tt0800981', 'Anthony Kaun Hai?', '2006', 5.5, 554), (2757, 'tt0175908', 'Mere Hamdam Mere Dost', '1968', 7.1, 58), (2758, 'tt1204913', 'Karma: Crime. Passion. Reincarnation', '2008', 5.9, 99), (2759, 'tt1312135', 'Oh, My God!!', '2008', 5.4, 250), (2760, 'tt0089952', 'Saaheb', '1985', 7.2, 391), (2761, 'tt1159917', 'Bombay to Bangkok', '2008', 4.7, 388), (2762, 'tt4479178', 'A New Love Ishtory', '2013', 6.4, 63), (2763, 'tt0087868', 'Paar', '1984', 7.3, 251), (2764, 'tt0296621', \"Everybody Says I'm Fine!\", '2001', 6.1, 313), (2765, 'tt0082644', 'Laawaris', '1981', 7.2, 1896), (2766, 'tt0162480', 'Mr. & Mrs. Khiladi', '1997', 6.7, 5355), (2767, 'tt3729994', 'The Perfect Girl', '2015', 6.3, 88), (2768, 'tt0156606', 'Himalay Ki Godmein', '1965', 7.0, 67), (2769, 'tt0057409', 'Phir Wohi Dil Laya Hoon', '1963', 7.1, 62), (2770, 'tt0339878', 'Waah! Tera Kya Kehna', '2002', 3.7, 188), (2771, 'tt0417106', 'Samba', '2004', 5.1, 451), (2772, 'tt0362902', 'Nayee Padosan', '2003', 5.5, 320), (2773, 'tt0156581', 'Griha Pravesh', '1979', 7.8, 83), (2774, 'tt0262717', 'Katilon Ke Kaatil', '1981', 6.8, 69), (2775, 'tt0241914', 'Salim Langde Pe Mat Ro', '1989', 7.6, 323), (2776, 'tt3814100', 'M.A.D: Mad About Dance', '2014', 6.2, 91), (2777, 'tt1620907', 'Na Ghar Ke Na Ghaat Ke', '2010', 6.2, 250), (2778, 'tt0250774', 'Tera Jadoo Chal Gayaa', '2000', 3.1, 520), (2779, 'tt0105897', 'Zindagi Ek Juaa', '1992', 5.4, 246), (2780, 'tt0272543', 'Censor', '2001', 3.8, 104), (2781, 'tt0053767', 'Dil Apna Aur Preet Parai', '1960', 6.7, 112), (2782, 'tt0246240', 'Sneham Kosam', '1999', 6.5, 216), (2783, 'tt2317103', 'Sons of Ram', '2012', 6.7, 61), (2784, 'tt0466546', 'The Film', '2005', 6.5, 78), (2785, 'tt0075653', 'Alaap', '1977', 7.0, 272), (2786, 'tt0229950', 'Aya Sawan Jhoom Ke', '1969', 6.9, 82), (2787, 'tt0330783', 'Pran Jaaye Par Shaan Na Jaaye', '2003', 6.3, 291), (2788, 'tt0363721', \"Joggers' Park\", '2003', 6.2, 324), (2789, 'tt1191118', 'Hello Darling', '2010', 3.5, 97), (2790, 'tt0398170', 'Madhosh', '1994', 7.3, 54), (2791, 'tt0415144', 'Manasarovar', '2004', 7.5, 59), (2792, 'tt2969320', 'Kaamannana Makkalu', '2008', 7.1, 57), (2793, 'tt0055767', 'Baat Ek Raat Ki', '1962', 7.2, 98), (2794, 'tt0374117', 'Peechha Karro', '1986', 6.7, 113), (2795, 'tt0066888', 'Caravan', '1971', 6.9, 221), (2796, 'tt0088335', 'Utsav', '1984', 7.0, 297), (2797, 'tt0220597', 'Khoobsurat', '1999', 5.3, 637), (2798, 'tt0094958', 'Dayavan', '1988', 6.6, 458), (2799, 'tt1285245', 'Lahore', '2010', 6.4, 491), (2800, 'tt0071205', 'Benaam', '1974', 6.9, 204), (2801, 'tt2318401', 'Bandook', '2013', 6.0, 112), (2802, 'tt0072972', 'Faraar', '1975', 7.0, 264), (2803, 'tt0158122', 'Professor Pyarelal', '1981', 7.1, 84), (2804, 'tt0379299', 'Geet Gaaya Pattharon Ne', '1964', 6.8, 75), (2805, 'tt0156290', 'Anubhav', '1971', 7.1, 101), (2806, 'tt0158375', 'Yakeen', '1969', 7.2, 70), (2807, 'tt4161520', 'Dashavatar', '2008', 5.9, 63), (2808, 'tt0247502', 'Manchali', '1973', 6.9, 109), (2809, 'tt0213677', 'Ghaath', '2000', 4.9, 116), (2810, 'tt0214704', 'Gandhinagar 2nd Street', '1986', 7.7, 579), (2811, 'tt2155776', 'Yeh Jo Mohabbat Hai', '2012', 5.3, 69), (2812, 'tt0157965', 'Man Pasand', '1980', 6.5, 89), (2813, 'tt0271604', 'Kuch Khatti Kuch Meethi', '2001', 4.7, 557), (2814, 'tt0308158', 'Marhi Da Deeva', '1989', 7.6, 56), (2815, 'tt0172622', 'Zahreelay', '1990', 5.2, 76), (2816, 'tt1810522', 'Aalwar', '2007', 3.3, 692), (2817, 'tt1235833', 'Bombay Summer', '2009', 6.5, 77), (2818, 'tt0363784', 'Madam X', '1994', 3.8, 58), (2819, 'tt2609468', 'Sona Spa', '2013', 3.7, 176), (2820, 'tt0158585', 'Dharmputra', '1961', 7.2, 57), (2821, 'tt0050322', 'Do Ankhen Barah Haath', '1957', 8.4, 1496), (2822, 'tt0105762', 'Virodhi', '1992', 4.5, 52), (2823, 'tt0058921', 'Arzoo', '1965', 7.1, 194), (2824, 'tt0256951', 'Nadiya Ke Paar', '1982', 8.1, 609), (2825, 'tt6582458', 'Baaraat Company', '2017', 7.0, 170), (2826, 'tt0050757', 'Nau Do Gyarah', '1957', 7.0, 118), (2827, 'tt0449000', 'Chicken Tikka Masala', '2005', 5.8, 507), (2828, 'tt1479676', 'Cooking with Stella', '2009', 5.5, 265), (2829, 'tt2034011', 'Hum Tum Shabana', '2011', 3.9, 262), (2830, 'tt0048395', 'Munimji', '1955', 7.5, 74), (2831, 'tt1305840', 'Aloo Chaat', '2009', 5.9, 831), (2832, 'tt0088687', 'Aitbaar', '1985', 7.0, 121), (2833, 'tt3005850', 'Sooper Se Ooper', '2013', 4.4, 157), (2834, 'tt0150683', 'Gaman', '1978', 7.1, 119), (2835, 'tt0295294', 'Halo', '1996', 8.0, 412), (2836, 'tt0154265', 'Bullet', '1976', 6.9, 61), (2837, 'tt2071418', 'Bezawada', '2011', 3.5, 179), (2838, 'tt0137947', 'Mela', '1948', 7.3, 69), (2839, 'tt4581032', 'Luv U Alia', '2015', 5.4, 71), (2840, 'tt1702543', 'Lucky', 'IV 2011', 7.1, 233), (2841, 'tt1221147', 'Tera Kya Hoga Johnny', '2009', 6.1, 187), (2842, 'tt1458553', 'Fajr al islam', '1971', 6.0, 60), (2843, 'tt0074926', 'Mrigayaa', '1977', 7.5, 93), (2844, 'tt1941657', 'Soundtrack', 'I 2011', 7.2, 767), (2845, 'tt3056632', 'Manjunath', '2014', 7.2, 456), (2846, 'tt0061366', 'Around the World', '1967', 6.1, 76), (2847, 'tt1369669', 'Jai Veeru: Friends Forever', '2009', 3.6, 228), (2848, 'tt0330217', 'Dil Ka Rishta', '2003', 4.8, 1410), (2849, 'tt0156982', 'Saathi', '1991', 6.7, 131), (2850, 'tt0105500', 'Suryavanshi', '1992', 4.7, 694), (2851, 'tt0123102', 'Hero', 'II 1983', 6.8, 666), (2852, 'tt0262527', 'Jhooth Bole Kauwa Kaate', '1998', 5.7, 675), (2853, 'tt0114317', 'Sabse Bada Khiladi', '1995', 6.6, 4002), (2854, 'tt0135161', 'Bhai', '1997', 5.1, 272), (2855, 'tt1451763', 'Villu', '2009', 3.5, 2735), (2856, 'tt0280832', 'Khal-Naaikaa', '1993', 3.8, 116), (2857, 'tt1368102', 'Khela', '2008', 7.0, 154), (2858, 'tt0827714', 'Jaane Hoga Kya', '2006', 2.9, 151), (2859, 'tt0050820', 'Pardesi', '1957', 7.1, 73), (2860, 'tt0455415', 'Daas', '2005', 3.6, 79), (2861, 'tt1122610', 'Thulasi', '2007', 5.4, 309), (2862, 'tt0158356', 'Warrant', '1975', 7.0, 81), (2863, 'tt0280889', 'Maanthrikam', '1995', 6.3, 179), (2864, 'tt0154653', 'Ishq Ishq Ishq', '1974', 7.3, 51), (2865, 'tt2907156', 'Paranthe Wali Gali', '2014', 5.0, 80), (2866, 'tt0449870', 'Chehraa', '2005', 4.2, 105), (2867, 'tt0175448', 'Apna Desh', '1972', 7.1, 86), (2868, 'tt2612156', 'The Angrez', '2006', 7.2, 358), (2869, 'tt0404843', 'C.I.D.', '1990', 4.8, 51), (2870, 'tt0069152', 'Raaste Kaa Patthar', '1972', 5.9, 64), (2871, 'tt0445056', 'Sheesha', '2005', 3.5, 229), (2872, 'tt0841048', 'When Kiran Met Karen', '2008', 5.2, 74), (2873, 'tt0284479', 'Shararat', '2002', 4.7, 408), (2874, 'tt0423087', 'Rok Sako To Rok Lo', '2004', 6.5, 1815), (2875, 'tt0055276', 'Parakh', '1960', 7.5, 95), (2876, 'tt0072109', 'Sagina', '1974', 7.2, 65), (2877, 'tt0121401', 'Indira Priyanka', '1996', 7.0, 178), (2878, 'tt0286416', 'Aakrosh: Cyclone of Anger', '1998', 4.8, 68), (2879, 'tt0129422', 'Tarana', '1951', 7.6, 99), (2880, 'tt0116737', 'Kabhi Na Kabhi', '1998', 4.3, 118), (2881, 'tt4332782', 'Main Hoon Khiladiyon Ka Khiladi', '2004', 9.4, 61), (2882, 'tt0451639', 'Bachke Rehna Re Baba', '2005', 2.9, 189), (2883, 'tt0071307', '36 Ghante', '1974', 6.9, 100), (2884, 'tt1288644', 'Mere Khwabon Mein Jo Aaye', '2009', 5.7, 75), (2885, 'tt1255873', 'Hari Puttar: A Comedy of Terrors', '2008', 2.0, 246), (2886, 'tt0119760', 'Naseeb', '1997', 5.3, 231), (2887, 'tt0249795', 'Maya Bazaar', '1957', 9.2, 3013), (2888, 'tt3324810', 'Chhota Bheem and the Throne of Bali', '2013', 6.9, 78), (2889, 'tt0119596', 'Maharaja', '1998', 3.8, 306), (2890, 'tt0113455', 'Jallaad', '1995', 5.8, 108), (2891, 'tt0146645', 'Dosti', '1964', 8.4, 1276), (2892, 'tt0061744', 'Hamraaz', '1967', 7.2, 267), (2893, 'tt6278732', 'Motu Patlu: King of Kings', '2016', 7.5, 85), (2894, 'tt0172704', 'Lal Baadshah', '1999', 3.3, 508), (2895, 'tt0104098', 'Dharavi', '1992', 7.2, 255), (2896, 'tt0077179', 'Arvind Desai Ki Ajeeb Dastaan', '1978', 7.3, 86), (2897, 'tt0063533', 'Sadhu Aur Shaitaan', '1968', 6.8, 102), (2898, 'tt1521223', 'Iti Mrinalini: An Unfinished Letter...', '2010', 6.9, 447), (2899, 'tt1666184', 'Turning 30!!!', '2011', 5.2, 350), (2900, 'tt1575719', 'Yuvaraju', '2000', 5.9, 610), (2901, 'tt0363413', 'Agni Pankh', '2004', 5.4, 83), (2902, 'tt0405121', 'Love in Nepal', '2004', 4.0, 104), (2903, 'tt1252614', 'Sankat City', '2009', 6.3, 560), (2904, 'tt1964886', 'Love Express', '2011', 5.4, 112), (2905, 'tt0359926', 'Sahebzaade', '1992', 4.3, 65), (2906, 'tt0495022', 'Ankahee', '2006', 4.3, 325), (2907, 'tt1948640', 'The Waiting Room', 'IV 2010', 6.0, 85), (2908, 'tt0173301', 'Taaqat', '1995', 5.1, 79), (2909, 'tt2104937', 'Miley - Naa Miley - Hum', '2011', 3.9, 129), (2910, 'tt1606297', 'Naaga', '2003', 4.4, 258), (2911, 'tt1673391', 'Hello Hum Lallann Bol Rahe Hain', '2010', 6.3, 102), (2912, 'tt0338566', 'Yeh Kya Ho Raha Hai?', '2002', 4.8, 214), (2913, 'tt0248289', 'Party', '1984', 7.1, 185), (2914, 'tt0312664', 'Escape from Taliban', '2003', 4.5, 151), (2915, 'tt0187150', 'Himmatwala', '1983', 4.4, 212), (2916, 'tt0286672', 'Haqeeqat', '1995', 4.5, 264), (2917, 'tt0105209', 'Radha Ka Sangam', '1992', 5.3, 53), (2918, 'tt1667460', '10ml LOVE', '2010', 6.3, 150), (2919, 'tt0439754', 'Pyaar Mein Twist', '2005', 5.9, 154), (2920, 'tt2251552', 'Challo Driver', '2012', 6.0, 152), (2921, 'tt1043876', 'Woodstock Villa', '2008', 3.6, 357), (2922, 'tt0989633', 'Delhii Heights', '2007', 4.5, 168), (2923, 'tt4121522', 'Shuruaat Ka Interval', '2014', 7.6, 57), (2924, 'tt0338721', 'Annarth', '2002', 3.6, 120), (2925, 'tt0299027', 'Maya', 'I 2001', 7.0, 275), (2926, 'tt0093182', 'Hero Hiralal', '1988', 6.2, 230), (2927, 'tt0157545', 'Do Ladke Dono Kadke', '1979', 6.5, 86), (2928, 'tt0408126', 'Pudhiya Geethai', '2003', 4.2, 730), (2929, 'tt1804604', 'Mee Sindhutai Sapkal', '2010', 7.0, 112), (2930, 'tt0309005', 'Tum Haseen Main Jawan', '1970', 7.1, 60), (2931, 'tt0353351', 'Dhund: The Fog', '2003', 2.1, 227), (2932, 'tt0151285', 'Lahu Ke Do Rang', '1979', 6.7, 60), (2933, 'tt1464549', 'Life Goes On', 'I 2009', 6.3, 66), (2934, 'tt0323767', 'Ranga', '1982', 6.1, 77), (2935, 'tt0275217', 'Banarasi Babu', '1973', 6.9, 59), (2936, 'tt0311401', 'Kissaa Kursee Kaa', '1978', 7.7, 95), (2937, 'tt2012576', 'Bubble Gum', 'II 2011', 7.3, 415), (2938, 'tt0092507', 'Abhimanyu', '1991', 7.1, 250), (2939, 'tt5151622', 'Ranbanka', '2015', 4.7, 112), (2940, 'tt0121659', 'Professor Ki Padosan', '1994', 6.6, 54), (2941, 'tt3453964', 'Jugni', '2016', 6.1, 124), (2942, 'tt2337292', 'Chor chor super chor', '2013', 6.7, 215), (2943, 'tt0287014', 'Tere Pyaar Mein', '2000', 5.9, 125), (2944, 'tt0175756', 'Humjoli', '1970', 6.0, 64), (2945, 'tt0290839', 'Sainik', '1993', 6.7, 990), (2946, 'tt0359440', 'Hukumat', '1987', 6.2, 105), (2947, 'tt0410591', 'Shart: The Challenge', '2004', 4.7, 132), (2948, 'tt1826051', 'Yeh Dooriyan', '2011', 5.0, 64), (2949, 'tt0244567', 'Jhoothi', '1985', 6.8, 102), (2950, 'tt0120545', 'Yugpurush: A Man Who Comes Just Once in a Way', '1998', 6.4, 185), (2951, 'tt0234211', 'Massey Sahib', '1987', 7.6, 75), (2952, 'tt0361746', 'Inaam Dus Hazaar', '1987', 6.5, 98), (2953, 'tt0266333', 'Biwi O Biwi', '1981', 6.8, 160), (2954, 'tt0442831', 'Rui Ka Bojh', '1997', 7.5, 154), (2955, 'tt0119295', 'Himalay Putra', '1997', 4.7, 108), (2956, 'tt0262446', 'Gopichand Jasoos', '1982', 6.3, 92), (2957, 'tt0813539', 'Mr Prime Minister', '2005', 5.1, 51), (2958, 'tt0388315', 'Out of Control', '2003', 3.3, 202), (2959, 'tt2957126', 'Kiccha', '2003', 7.4, 88), (2960, 'tt0158690', 'Joshila', '1973', 6.5, 81), (2961, 'tt2953786', 'Hubballi', '2006', 7.0, 61), (2962, 'tt0215714', 'Dhuan', '1981', 7.2, 53), (2963, 'tt0089678', 'New Delhi Times', '1986', 7.4, 131), (2964, 'tt0401532', 'Jaago', '2004', 6.7, 131), (2965, 'tt0453582', '7 1/2 Phere: More Than a Wedding', '2005', 5.3, 216), (2966, 'tt2767712', 'Badlapur Boys', '2014', 4.8, 107), (2967, 'tt2218228', 'The Bright Day', '2015', 5.8, 114), (2968, 'tt0114230', 'Raja', '1995', 5.3, 620), (2969, 'tt0102701', 'Prahaar: The Final Attack', '1991', 7.9, 1696), (2970, 'tt0085912', 'Mashaal', '1984', 7.5, 535), (2971, 'tt0272118', 'Haathi Mere Saathi', '1971', 7.0, 922), (2972, 'tt0440590', 'Loha', '1997', 5.9, 462), (2973, 'tt0076167', 'Hum Kisise Kum Naheen', '1977', 7.2, 401), (2974, 'tt0114175', 'Prem', '1995', 4.2, 176), (2975, 'tt7399620', 'Game Over', 'IV 2017', 6.9, 65), (2976, 'tt5021968', 'Chinar Daastaan-E-Ishq', '2015', 7.1, 54), (2977, 'tt1809399', 'Utt Pataang', '2011', 6.0, 283), (2978, 'tt0110449', 'Mammo', '1994', 7.7, 324), (2979, 'tt7604032', 'Aadamkhor', '2018', 8.1, 133), (2980, 'tt0875692', 'Aggar: Passion Betrayal Terror', '2007', 4.3, 252), (2981, 'tt0233202', 'Amir Garib', '1974', 6.8, 64), (2982, 'tt0446158', 'Mera Damad', '1995', 3.2, 768), (2983, 'tt0137685', 'Ghashiram Kotwal', '1976', 6.8, 59), (2984, 'tt0187096', 'Govindha Govindha', '1993', 6.8, 282), (2985, 'tt0242831', 'Saat Rang Ke Sapne', '1998', 6.0, 70), (2986, 'tt0259541', 'Rihaee', '1988', 7.1, 60), (2987, 'tt0268519', 'Pagla Kahin Ka', '1970', 7.2, 67), (2988, 'tt0354786', 'Oops!', '2003', 3.5, 84), (2989, 'tt1847736', 'U R My Jaan', '2011', 6.0, 88), (2990, 'tt2220828', 'Identity Card', '2014', 5.9, 62), (2991, 'tt0213524', 'Bluff Master', '1963', 6.3, 89), (2992, 'tt0154857', 'Mawaali', '1983', 5.4, 53), (2993, 'tt1314274', 'Hulla', '2008', 6.4, 199), (2994, 'tt1579625', 'Mahathma', '2009', 6.0, 144), (2995, 'tt0841153', 'Rain: The Terror Within...', '2005', 3.4, 56), (2996, 'tt1020899', \"Say Salaam India: 'Let's Bring the Cup Home'\", '2007', 6.5, 84), (2997, 'tt0156651', 'Joroo Ka Ghulam', '1972', 7.0, 65), (2998, 'tt0440463', 'Hava Aney Dey', '2004', 6.2, 101), (2999, 'tt0466580', 'Siskiyaan', '2005', 5.1, 66), (3000, 'tt0361796', 'Khatarnaak', '1990', 5.5, 62), (3001, 'tt0260144', 'Lok Parlok', '1979', 6.0, 62), (3002, 'tt3303416', 'Ya Rab', '2014', 6.2, 270), (3003, 'tt2949874', 'Varadanayaka', '2013', 6.2, 132), (3004, 'tt0215907', 'Kitaab', '1977', 7.3, 162), (3005, 'tt0272735', 'Mujhe Meri Biwi Se Bachaao', '2001', 4.3, 81), (3006, 'tt0302524', 'Ehsaas: The Feeling', '2001', 5.5, 72), (3007, 'tt0255120', 'Dil Pe Mat Le Yaar!!', '2000', 4.9, 183), (3008, 'tt0036077', 'Kismet', '1943', 7.5, 71), (3009, 'tt0830938', 'Overnight', '2007', 5.1, 353), (3010, 'tt1504687', 'I Am Singh', '2011', 5.2, 60), (3011, 'tt0158451', 'Aaj Ki Awaz', '1984', 5.6, 55), (3012, 'tt0262281', 'Bhookailas', '1958', 7.3, 125), (3013, 'tt0445033', 'Mass', '2004', 6.3, 893), (3014, 'tt0152509', 'Sohni Mahiwal', '1984', 6.1, 197), (3015, 'tt0312883', 'Kya Yehi Pyaar Hai', '2002', 3.8, 387), (3016, 'tt0125556', 'Tohfa', '1984', 5.7, 86), (3017, 'tt0247944', 'Badal', '2000', 4.9, 1043), (3018, 'tt2922076', 'Veera Parampare', '2010', 7.0, 57), (3019, 'tt0244674', 'Naukri', '1978', 7.0, 61), (3020, 'tt0363634', 'Ghar Ka Chiraag', '1989', 5.9, 53), (3021, 'tt0178181', 'Aamne - Saamne', '1967', 6.3, 64), (3022, 'tt0140677', 'Vetri Vizha', '1989', 7.1, 236), (3023, 'tt0145851', 'Jagriti', '1956', 7.1, 102), (3024, 'tt0301240', 'Moksha: Salvation', '2001', 6.4, 346), (3025, 'tt0419828', 'Hyderabad Blues 2', '2004', 6.1, 221), (3026, 'tt0375021', 'Rules: Pyaar Ka Superhit Formula', '2003', 6.4, 284), (3027, 'tt0069751', 'Bandhe Haath', '1973', 6.2, 77), (3028, 'tt0105161', 'Pratikar', '1991', 5.4, 82), (3029, 'tt0246913', 'Saraswathi Sabatham', '1966', 7.6, 91), (3030, 'tt3095248', 'Kaafiron Ki Namaaz', '2013', 7.0, 235), (3031, 'tt0375733', 'Encounter: The Killing', '2002', 6.7, 108), (3032, 'tt0949524', 'Faryad moorcheha', '2006', 6.2, 759), (3033, 'tt2137171', 'Oass', '2012', 6.7, 195), (3034, 'tt0158259', 'Taqdeer', '1983', 5.8, 71), (3035, 'tt0117099', 'Mr. Bechara', '1996', 5.5, 277), (3036, 'tt1474271', 'Love Khichdi', '2009', 5.8, 288), (3037, 'tt0246261', 'Surakksha', '1979', 7.2, 69), (3038, 'tt0075411', 'Wan Pipel', '1976', 6.9, 180), (3039, 'tt2327389', 'Life Ki Toh Lag Gayi', '2012', 5.8, 192), (3040, 'tt0473684', 'Pehchaan: The Face of Truth', '2005', 6.1, 51), (3041, 'tt1210356', 'Raat Gayi, Baat Gayi?', '2009', 6.3, 503), (3042, 'tt1612039', 'Souryam', '2008', 6.0, 126), (3043, 'tt0283911', 'Bas Itna Sa Khwaab Hai...', '2001', 3.9, 536), (3044, 'tt1454567', 'Team: The Force', '2009', 4.3, 60), (3045, 'tt0027256', 'Achhut Kanya', '1936', 7.2, 83), (3046, 'tt2706340', 'Ishk Actually', '2013', 2.6, 129), (3047, 'tt2951596', 'Partha', '2003', 7.2, 56), (3048, 'tt0272001', 'Bindhaast', '1999', 7.1, 118), (3049, 'tt0254247', 'Daisy', '1988', 7.3, 65), (3050, 'tt0157418', 'Bombay 405 Miles', '1980', 7.2, 67), (3051, 'tt2354223', 'Mumbai Cha Raja', '2012', 6.6, 112), (3052, 'tt0286504', 'Beti No. 1', '2000', 3.7, 177), (3053, 'tt0186799', 'Dharm Adhikari', '1986', 6.1, 76), (3054, 'tt0145856', 'Jhanak Jhanak Payal Baaje', '1955', 7.2, 85), (3055, 'tt4920116', 'Halla Gulla', '2015', 5.7, 64), (3056, 'tt1634329', 'Fruit & Nut', '2009', 3.1, 56), (3057, 'tt0317751', 'Janwar', '1965', 7.0, 139), (3058, 'tt6722870', 'Kadamban', '2017', 5.3, 316), (3059, 'tt0843335', 'Desamuduru', '2007', 6.1, 1162), (3060, 'tt0193449', 'Rang', '1993', 5.0, 147), (3061, 'tt0331421', 'Akhiyon Se Goli Maare', '2002', 5.0, 626), (3062, 'tt0099953', 'Kroadh', '1990', 5.2, 129), (3063, 'tt0318405', 'Love in Simla', '1960', 7.3, 79), (3064, 'tt0271023', 'Kab? Kyoon? Aur Kahan?', '1970', 7.2, 90), (3065, 'tt0087736', 'Mohan Joshi Hazir Ho!', '1984', 7.1, 123), (3066, 'tt0410954', 'Sheen', '2004', 6.2, 54), (3067, 'tt0071714', 'Kasauti', '1974', 6.3, 89), (3068, 'tt0982875', 'Gauri: The Unborn', '2007', 4.5, 98), (3069, 'tt0064371', 'Gladiatorerna', '1969', 6.8, 407), (3070, 'tt0068620', 'Garam Masala', '1972', 6.1, 57), (3071, 'tt1867070', 'Teen Thay Bhai', '2011', 4.3, 290), (3072, 'tt0249818', 'Mrugaraaju', '2001', 4.4, 219), (3073, 'tt1606162', 'Aakhari Decision', '2010', 6.0, 62), (3074, 'tt0988036', 'Fear', 'I 2007', 4.0, 119), (3075, 'tt1071798', 'Frozen', '2007', 7.0, 186), (3076, 'tt0071083', '27 Down', '1974', 7.6, 87), (3077, 'tt0217608', 'Laawaris', '1999', 4.3, 121), (3078, 'tt0026274', 'Devdas', '1936', 6.4, 67), (3079, 'tt0084744', 'Swami Dada', '1982', 6.4, 72), (3080, 'tt0107821', 'Phool Aur Angaar', '1993', 6.8, 96), (3081, 'tt0130348', 'Vinashak - Destroyer', '1998', 5.2, 196), (3082, 'tt0367214', 'Waaris', '1988', 6.7, 105), (3083, 'tt0098491', 'Toofan', 'I 1989', 4.8, 521), (3084, 'tt5755594', 'Hello Mumbai: Salaam Mumbai', '2016', 4.2, 632), (3085, 'tt0493437', 'Apna Sapna Money Money', '2006', 5.3, 1599), (3086, 'tt0317680', 'Hum Hain Kamaal Ke', '1993', 5.7, 150), (3087, 'tt0417735', 'The Hangman', '2005', 7.0, 58), (3088, 'tt1979209', 'Chala Mussaddi - Office Office', '2011', 6.0, 299), (3089, 'tt0485204', 'Duet', '1994', 6.6, 231), (3090, 'tt0081147', 'Meera', '1979', 7.3, 77), (3091, 'tt0099437', 'Disha', '1990', 7.2, 134), (3092, 'tt0978570', 'Bhageeratha', '2005', 5.6, 190), (3093, 'tt1999857', 'Cycle Kick', '2011', 6.9, 91), (3094, 'tt0944185', 'Andhrudu', '2005', 6.6, 120), (3095, 'tt0043078', 'The 20 Questions Murder Mystery', '1950', 6.3, 83), (3096, 'tt1388424', 'Three: Love, Lies, Betrayal', '2009', 5.8, 62), (3097, 'tt2959626', 'Nalla', '2004', 7.2, 60), (3098, 'tt0225566', 'Drohi', '1992', 6.1, 110), (3099, 'tt1500682', 'Aadmi Ki Aurat Aur Anya Kahaniya', '2009', 7.8, 51), (3100, 'tt1583218', 'Allari Ramudu', '2002', 4.5, 248), (3101, 'tt1079967', 'Hastey Hastey Follow Your Heart', '2008', 2.2, 124), (3102, 'tt0249358', 'Ankahee', '1985', 7.7, 87), (3103, 'tt0155530', 'Bandhan', 'I 1969', 7.0, 64), (3104, 'tt0172487', 'The Gentleman', '1994', 5.8, 157), (3105, 'tt1112110', 'Hollywood', '2003', 7.1, 152), (3106, 'tt2188811', 'Luv U Soniyo', '2012', 5.5, 60), (3107, 'tt0050734', 'Musafir', '1957', 7.6, 69), (3108, 'tt1688092', 'Yagnam', '2004', 6.2, 73), (3109, 'tt0308417', 'Is Raat Ki Subah Nahin', '1996', 7.1, 202), (3110, 'tt0249866', 'Paromitar Ek Din', '2000', 7.2, 219), (3111, 'tt0309800', 'Chota Jadugar', '2003', 3.9, 87), (3112, 'tt0357208', 'Stumped', '2003', 3.8, 94), (3113, 'tt5759540', 'Missing on a Weekend', '2016', 4.6, 53), (3114, 'tt0893585', 'Detective Naani', '2009', 6.4, 67), (3115, 'tt2613458', 'Khoya', '2015', 8.2, 62), (3116, 'tt1579691', 'Rowdy Inspector', '1992', 7.5, 92), (3117, 'tt5052972', 'Yaara Silly Silly', '2015', 5.7, 203), (3118, 'tt0314186', 'Humko Tumse Pyaar Hai', '2006', 5.1, 461), (3119, 'tt0308347', 'Haan Maine Bhi Pyaar Kiya', '2002', 4.1, 1109), (3120, 'tt1947973', 'Bin Bulaye Baraati', '2011', 3.8, 217), (3121, 'tt0094720', 'Bees Saal Baad', '1988', 5.2, 121), (3122, 'tt0177490', 'Aghaat', '1985', 7.2, 102), (3123, 'tt2355921', 'The Wish Fish', '2012', 3.4, 60), (3124, 'tt0331453', 'Chhal', '2002', 6.2, 164), (3125, 'tt0122233', 'Saaz', '1997', 7.2, 97), (3126, 'tt0050956', 'Sharada', '1957', 6.5, 77), (3127, 'tt1124386', 'Pankh', '2010', 4.2, 108), (3128, 'tt1268906', 'Samudram', '1999', 6.7, 83), (3129, 'tt1605778', 'Toh Baat Pakki!', '2010', 5.3, 324), (3130, 'tt0823261', 'Zindaggi Rocks', '2006', 4.2, 226), (3131, 'tt1095038', 'Victoria No. 203: Diamonds Are Forever', '2007', 3.7, 137), (3132, 'tt2389486', 'Mai', '2013', 6.9, 85), (3133, 'tt2190256', 'Daal Mein Kuch Kaala Hai', '2012', 4.1, 68), (3134, 'tt7191668', 'The Rally', '2017', 8.5, 214), (3135, 'tt3666258', 'Coffee Bloom', '2015', 6.5, 96), (3136, 'tt0156833', 'Naya Din Nai Raat', '1974', 7.1, 121), (3137, 'tt0047271', 'Naukari', '1954', 7.8, 55), (3138, 'tt3693936', 'Once Upon a Time in Bihar', '2015', 7.2, 168), (3139, 'tt0048181', 'House No. 44', '1955', 7.4, 61), (3140, 'tt8338754', \"Vaibhav Sethia: Don't\", '2018', 6.9, 116), (3141, 'tt0280396', 'Adharm', '1992', 5.2, 52), (3142, 'tt0260107', 'Jeevan Dhaara', '1982', 7.3, 54), (3143, 'tt1152845', 'Showbiz', '2007', 4.0, 102), (3144, 'tt0431776', 'Andarivaadu', '2005', 4.4, 290), (3145, 'tt0177483', 'Aadmi', '1993', 6.5, 63), (3146, 'tt1941638', 'Sadda Adda', '2011', 7.2, 1213), (3147, 'tt0470381', 'Jaan Tere Naam', '1992', 6.4, 111), (3148, 'tt0156745', 'Loafer', '1973', 7.1, 103), (3149, 'tt0267617', 'Jigar', '1992', 4.8, 670), (3150, 'tt0155499', 'Antarnaad', '1991', 5.7, 102), (3151, 'tt2063749', 'Pazhani', '2008', 3.3, 72), (3152, 'tt0442799', 'Kis Kis Ki Kismat', '2004', 2.6, 120), (3153, 'tt0232585', 'Sapnon Ka Saudagar', '1968', 7.4, 53), (3154, 'tt0141819', 'Shaheed', '1948', 6.9, 54), (3155, 'tt0085785', 'Khandhar', '1984', 7.2, 185), (3156, 'tt0046164', 'Parineeta', '1953', 7.5, 95), (3157, 'tt1594964', 'Chase', 'I 2010', 4.8, 84), (3158, 'tt0217351', 'Dahek: A Burning Passion', '1999', 5.8, 154), (3159, 'tt0299078', 'Palkon Ki Chhaon Mein', '1977', 7.7, 55), (3160, 'tt0286942', 'Sardari Begum', '1996', 6.9, 151), (3161, 'tt0049549', 'New Delhi', '1956', 7.5, 56), (3162, 'tt0286664', 'Gudgudee', '1997', 4.6, 305), (3163, 'tt0317796', 'Kalaignan', '1993', 6.4, 186), (3164, 'tt0057332', 'Mujhe Jeene Do', '1963', 7.9, 55), (3165, 'tt0949423', 'Hotel Very Welcome', '2007', 6.5, 524), (3166, 'tt0178211', 'Ab Ayega Mazaa', '1984', 6.8, 62), (3167, 'tt1499935', 'Shadow', 'I 2009', 5.0, 52), (3168, 'tt0178207', 'Aashiq Hoon Baharon Ka', '1977', 5.6, 75), (3169, 'tt3319248', 'A Film by Aravind', '2005', 7.1, 126), (3170, 'tt0459554', 'Subhash Chandra Bose', '2005', 3.0, 207), (3171, 'tt0215466', 'Achanak', '1973', 7.0, 166), (3172, 'tt0141441', 'Jogan', '1950', 7.4, 53), (3173, 'tt0070009', 'Duvidha', '1973', 7.2, 201), (3174, 'tt0151090', 'Jaan', '1996', 4.0, 447), (3175, 'tt0232081', 'Mera Gaon Mera Desh', '1971', 7.2, 237), (3176, 'tt0234207', 'Marte Dam Tak', '1987', 6.0, 69), (3177, 'tt0186857', 'Banjaran', '1991', 5.3, 54), (3178, 'tt0091601', 'Nagina', '1986', 6.6, 611), (3179, 'tt1990976', '7 Welcome to London', '2012', 5.0, 102), (3180, 'tt0470611', 'Yatra', '2006', 6.1, 126), (3181, 'tt0244549', 'Inspector Balram', '1991', 6.9, 242), (3182, 'tt2222716', 'Baat Bann Gayi', '2013', 5.3, 60), (3183, 'tt1016177', 'Go', '2007', 2.7, 131), (3184, 'tt0361515', 'Do Jasoos', '1975', 6.2, 76), (3185, 'tt0099467', 'Drishti', '1990', 7.3, 63), (3186, 'tt0187576', 'Vazhve Mayam', '1982', 6.9, 191), (3187, 'tt0286934', 'Safari', '1999', 4.6, 210), (3188, 'tt0447890', 'Chaahat Ek Nasha...', '2005', 4.0, 66), (3189, 'tt0237132', 'Dadar Kirti', '1980', 7.2, 143), (3190, 'tt0272651', 'Hari-Bhari', '2000', 7.1, 95), (3191, 'tt0241753', 'Naseem', '1995', 7.6, 83), (3192, 'tt0283580', 'Satyameva Jayate', '2000', 5.3, 65), (3193, 'tt3672618', 'Dekh Tamasha Dekh', '2014', 6.1, 270), (3194, 'tt2938702', 'Attahaasa', '2013', 7.1, 110), (3195, 'tt1674757', 'Veettilekkulla Vazhi', '2011', 6.8, 148), (3196, 'tt0257475', 'Chhailla Babu', '1977', 7.0, 60), (3197, 'tt0103763', 'Balwaan', '1992', 4.4, 136), (3198, 'tt0259877', 'Bandh Darwaza', '1990', 5.1, 237), (3199, 'tt0111625', 'Vijaypath', '1994', 5.0, 532), (3200, 'tt0097926', 'Mujrim', 'I 1989', 5.8, 102), (3201, 'tt0217632', 'Love You Hamesha', '2001', 5.0, 54), (3202, 'tt0139447', 'Majhli Didi', '1967', 7.7, 69), (3203, 'tt1649760', 'The Film Emotional Atyachar', '2010', 6.0, 274), (3204, 'tt0310865', 'Chann Pardesee', '1980', 8.2, 53), (3205, 'tt0324977', 'Alavuddinum Athbutha Vilakkum', '1979', 6.8, 144), (3206, 'tt1825665', 'Bach ke Zara', '2008', 2.9, 52), (3207, 'tt0924271', 'Raghavendra', '2003', 5.0, 217), (3208, 'tt0110105', 'Ikke Pe Ikka', '1994', 4.5, 594), (3209, 'tt4944418', 'Man On Mission Jaanbaaz', '2005', 8.3, 116), (3210, 'tt0251067', 'Ek Hota Vidushak', '1992', 7.9, 90), (3211, 'tt2624852', 'Gangoobai', '2013', 7.5, 271), (3212, 'tt0875693', 'Anjaam', '2007', 6.2, 59), (3213, 'tt0379504', 'Shriman Shrimati', '1982', 6.5, 112), (3214, 'tt1132595', 'Maan Gaye Mughall-E-Azam', '2008', 3.1, 457), (3215, 'tt5914630', 'Chudail Story', '2016', 2.9, 92), (3216, 'tt0233587', 'Do Kaliyaan', '1968', 6.2, 55), (3217, 'tt0307682', 'Yeh Dil Aashiqanaa', '2002', 4.4, 212), (3218, 'tt0187192', 'Julie', '1975', 6.1, 242), (3219, 'tt5544662', 'Sweetiee Weds NRI', '2017', 3.7, 183), (3220, 'tt0105315', 'Sapne Saajan Ke', '1992', 3.4, 105), (3221, 'tt3153580', 'Barefoot to Goa', '2015', 5.8, 131), (3222, 'tt2066899', 'Jugaad', '2009', 5.2, 52), (3223, 'tt0278761', 'Tarpan The Absolution', '1995', 7.4, 56), (3224, 'tt0155408', 'Zara Si Zindagi', '1983', 7.7, 77), (3225, 'tt2191164', 'Station', 'I 2014', 7.6, 88), (3226, 'tt1955036', 'Vakratunda Mahakaaya', '2013', 7.5, 51), (3227, 'tt2343417', 'Chhodo Kal Ki Baatein', '2012', 7.1, 138), (3228, 'tt1515210', 'Vihir', '2009', 7.2, 235), (3229, 'tt0874854', 'Anjaane: The Unkown', '2005', 4.1, 58), (3230, 'tt1220718', 'Via Darjeeling', '2008', 5.4, 151), (3231, 'tt1887817', 'Monica', '2011', 5.6, 64), (3232, 'tt0429541', 'Afsana Dilwalon Ka', '2001', 3.6, 1017), (3233, 'tt1801521', 'Khap', '2011', 5.8, 91), (3234, 'tt0857385', 'Kudiyon Ka Hai Zamaana', '2006', 3.6, 63), (3235, 'tt0459466', 'Mera Dil Leke Dekho', '2006', 7.1, 52), (3236, 'tt1132606', 'Ugly Aur Pagli', '2008', 3.8, 652), (3237, 'tt0186794', 'Thayillamal Nannilai', '1979', 6.7, 51), (3238, 'tt2084863', \"Jo Dooba So Paar: It's Love in Bihar!\", '2011', 5.8, 67), (3239, 'tt0215515', 'Anand Math', '1952', 7.5, 73), (3240, 'tt0275381', 'Grahan', '2001', 5.2, 69), (3241, 'tt0091105', 'Genesis', '1986', 7.3, 89), (3242, 'tt0137341', 'Ab Dilli Dur Nahin', '1957', 7.6, 61), (3243, 'tt0157844', 'Jagir', '1984', 6.5, 127), (3244, 'tt0172188', 'Bol Radha Bol', '1992', 6.1, 639), (3245, 'tt0173283', 'Swarg', '1990', 6.7, 859), (3246, 'tt0432090', 'Bunny', '2005', 5.8, 721), (3247, 'tt1725798', 'Be-Careful', '2011', 3.2, 77), (3248, 'tt1610418', 'Prem Kaa Game', '2010', 3.6, 191), (3249, 'tt3030720', 'Dilwale:The Brave Heart', '2001', 9.3, 83), (3250, 'tt0447659', 'Lezioni di volo', '2007', 6.3, 302), (3251, 'tt0088986', 'Damul', '1985', 7.6, 94), (3252, 'tt1183944', 'Sadiyaan: Boundaries Divide... Love Unites', '2010', 4.5, 63), (3253, 'tt0362204', 'Supari', '2003', 4.5, 110), (3254, 'tt0096355', 'Uttar Dakshin', '1987', 4.8, 86), (3255, 'tt1587424', 'Radio: Love on Air', '2009', 3.1, 282), (3256, 'tt7496256', 'Gospel Movie: Who Is My Lord', '2017', 9.1, 81), (3257, 'tt4511416', 'Badmashiyaan', '2015', 4.7, 118), (3258, 'tt0819777', 'Jigyaasa', '2006', 5.1, 255), (3259, 'tt3737442', 'Oraalppokkam', '2014', 8.1, 78), (3260, 'tt0979913', 'Life Mein Kabhie Kabhiee', '2007', 6.2, 236), (3261, 'tt0071317', 'Chorus', '1975', 7.5, 53), (3262, 'tt0290940', 'Zaalim', '1994', 5.8, 512), (3263, 'tt0231646', 'Ghar Ghar Ki Kahani', '1988', 5.0, 155), (3264, 'tt0318956', 'Tum Se Achcha Kaun Hai', '2002', 4.7, 136), (3265, 'tt0267703', 'Maidan-E-Jung', '1995', 5.4, 461), (3266, 'tt0213532', 'Budtameez', '1966', 7.4, 54), (3267, 'tt0402606', 'Suno Sasurjee', '2004', 2.6, 126), (3268, 'tt0396782', 'Paisa Vasool', '2004', 4.3, 113), (3269, 'tt0158071', 'Pasand Apni Apni', '1983', 6.6, 51), (3270, 'tt1630613', 'Soch Lo', '2010', 6.6, 82), (3271, 'tt1606319', 'Prematho Raa', '2001', 5.9, 83), (3272, 'tt1869759', 'The Train', '2011', 4.5, 113), (3273, 'tt1649404', 'Sankham', '2009', 5.3, 110), (3274, 'tt5049906', 'Subah Subah', '1983', 8.1, 63), (3275, 'tt0390079', 'Hari Om', '2004', 7.2, 108), (3276, 'tt1390840', 'Walkaway', '2010', 7.6, 798), (3277, 'tt0303187', 'Veer Savarkar', '2001', 7.7, 69), (3278, 'tt1135931', 'Chintu Ji', '2009', 5.8, 163), (3279, 'tt0060317', 'Do Dooni Char', '1968', 7.2, 55), (3280, 'tt2253708', 'Future to Bright Hai Ji', '2012', 6.3, 81), (3281, 'tt2951534', 'Chandu', '2002', 7.3, 62), (3282, 'tt0258647', 'Heeralal Pannalal', '1978', 6.6, 66), (3283, 'tt1431729', 'Mohandas', '2009', 6.6, 114), (3284, 'tt0092599', 'Awara Baap', '1985', 4.9, 66), (3285, 'tt2084939', 'Sivappathikaaram', '2006', 5.5, 56), (3286, 'tt0268489', 'Naughty Boy', '1962', 5.7, 348), (3287, 'tt0929630', 'Veerabhadra', '2005', 2.8, 160), (3288, 'tt2248905', 'Madly Bangali', '2009', 6.9, 229), (3289, 'tt0104941', 'Muqabla', '1993', 4.5, 132), (3290, 'tt0148691', 'Sachaa Jhutha', '1970', 6.8, 191), (3291, 'tt0478868', 'Koi Mere Dil Mein Hai', '2005', 4.8, 66), (3292, 'tt0116726', 'Jung', '1996', 4.1, 197), (3293, 'tt0217660', 'Mohabbat', '1997', 5.4, 422), (3294, 'tt2798620', 'Chhota Bheem and the Curse of Damyaan', '2012', 6.2, 79), (3295, 'tt0309926', 'Noorie', '1979', 6.1, 118), (3296, 'tt0349058', '88 Antop Hill', '2003', 4.6, 59), (3297, 'tt0173189', 'Shanti Kranti', '1991', 6.8, 78), (3298, 'tt0319336', 'Eduruleni Manishi', '2001', 4.4, 100), (3299, 'tt1821385', 'Curry Munchers', '2011', 5.2, 53), (3300, 'tt0130797', 'Jaali Note', '1960', 7.5, 59), (3301, 'tt0097268', 'Ek Din Achanak', '1989', 6.9, 166), (3302, 'tt3073326', 'Goopi Gawaiya Bagha Bajaiya', '2013', 7.9, 52), (3303, 'tt0250043', 'Thodasa Roomani Ho Jaayen', '1990', 7.1, 163), (3304, 'tt2112978', 'Jo Hum Chahein', '2011', 5.4, 91), (3305, 'tt1773042', \"Shahrukh Bola 'Khoobsurat Hai Tu'... And She Believed in It\", '2010', 6.2, 1036), (3306, 'tt1108831', 'Chhodon Naa Yaar', '2007', 4.6, 81), (3307, 'tt0080435', 'Bhavni Bhavai', '1980', 7.5, 93), (3308, 'tt3261022', 'Kajarya', '2013', 6.6, 111), (3309, 'tt0414503', 'Shaadi Ka Laddoo', '2004', 5.3, 87), (3310, 'tt0364698', 'Thee', '1981', 6.8, 108), (3311, 'tt0398404', 'God Only Knows!', '2007', 5.8, 52), (3312, 'tt2924844', 'Nandhi', '2002', 7.4, 63), (3313, 'tt0066514', 'Uski Roti', '1970', 6.8, 158), (3314, 'tt0269821', 'Ravoyi Chandamama', '1999', 4.9, 173), (3315, 'tt0087958', 'Pyar Jhukta Nahin', '1985', 6.5, 130), (3316, 'tt0242509', 'Haatim Tai', '1990', 5.6, 237), (3317, 'tt0114225', 'Raghuveer', '1995', 4.2, 57), (3318, 'tt0260222', 'Naseeb Apna Apna', '1986', 5.5, 102), (3319, 'tt1111869', '50 Lakh', '2007', 6.7, 69), (3320, 'tt2914898', 'Gooli', '2008', 6.9, 63), (3321, 'tt0430480', 'Popcorn Khao! Mast Ho Jao', '2004', 4.1, 78), (3322, 'tt0453842', 'Sau Jhooth Ek Sach', '2004', 7.3, 63), (3323, 'tt0827202', 'Sanshodhan', '1996', 7.6, 51), (3324, 'tt2950606', 'Kiccha Huccha', '2010', 6.2, 56), (3325, 'tt0378350', 'Meri Biwi Ki Shaadi', '1979', 6.6, 82), (3326, 'tt4309284', 'The Silent Heroes', '2015', 6.7, 55), (3327, 'tt0476847', 'Raakh: A Poem Masked in Blood', '2007', 3.6, 60), (3328, 'tt4971292', 'Uyirile Kalanthathu', '2000', 7.8, 56), (3329, 'tt0488976', 'C U at 9', '2005', 4.2, 57), (3330, 'tt0785069', 'Umar', '2006', 5.7, 59), (3331, 'tt0039654', 'Neel Kamal', '1947', 7.5, 51), (3332, 'tt1814619', 'Aashiqui.in', '2011', 3.7, 58), (3333, 'tt0038491', 'Dr. Kotnis Ki Amar Kahani', '1946', 7.2, 61), (3334, 'tt0278274', 'Azaad', '2000', 6.2, 188), (3335, 'tt0452236', 'Ginny Aur Johny', '1976', 7.4, 70), (3336, 'tt1563732', '5 Ghantey Mien 5 Crore', '2012', 5.2, 57), (3337, 'tt0230294', 'Hip Hip Hurray', '1984', 7.5, 68), (3338, 'tt0214853', 'Killer', '1992', 6.4, 109), (3339, 'tt0319351', 'Enakkul Oruvan', '1984', 6.9, 110), (3340, 'tt0246567', 'Diksha', '1991', 7.0, 141), (3341, 'tt1097006', 'Kaisay Kahein...', '2007', 6.4, 51), (3342, 'tt0227496', 'Shreemaan Aashique', '1993', 4.7, 92), (3343, 'tt0423313', 'Tahalka', '1992', 5.6, 236), (3344, 'tt0286488', 'Barood', '1998', 5.5, 552), (3345, 'tt0177487', 'Aag Hi Aag', '1987', 5.3, 80), (3346, 'tt0448121', 'Sins', '2005', 4.2, 348), (3347, 'tt0359215', 'Elaan-E-Jung', '1989', 5.6, 64), (3348, 'tt2429930', 'Khwaabb', '2014', 5.6, 60), (3349, 'tt0381090', 'Chupke Se', '2003', 5.5, 107), (3350, 'tt4946222', 'Man on Mission Fauladi', '2004', 9.6, 96), (3351, 'tt1087852', 'The Great Indian Butterfly', '2007', 6.2, 176), (3352, 'tt0070498', 'Padatik', '1973', 7.1, 113), (3353, 'tt1236441', 'Phir Kabhi', '2008', 7.0, 67), (3354, 'tt0345594', 'Love at Times Square', '2003', 3.6, 168), (3355, 'tt1399602', 'Straight', 'II 2009', 5.9, 336), (3356, 'tt0157917', 'Lakhon Ki Baat', '1984', 6.8, 107), (3357, 'tt0246255', 'Stoovertpuram Police Station', '1991', 6.7, 52), (3358, 'tt0193102', 'Dil Ka Kya Kasoor', '1992', 6.0, 144), (3359, 'tt0272062', 'Daag: The Fire', '1999', 4.8, 566), (3360, 'tt0233979', 'Zapatlela', '1993', 7.5, 431), (3361, 'tt0278315', 'Arjun Pandit', '1999', 5.3, 653), (3362, 'tt2838554', 'Khokababu', '2012', 5.7, 131), (3363, 'tt0409068', 'Muskaan', '2004', 4.5, 163), (3364, 'tt0328810', 'Aamaar Bhuvan', '2002', 8.0, 51), (3365, 'tt0415820', 'Ghar Grihasti', '2004', 5.6, 118), (3366, 'tt0093555', 'Mohre', '1987', 5.5, 62), (3367, 'tt1358852', 'Madholal Keep Walking', '2009', 6.7, 92), (3368, 'tt1024852', 'Meerabai Not Out', '2008', 4.0, 83), (3369, 'tt1740017', 'Payback', '2010', 5.5, 58), (3370, 'tt0142521', 'Lal Darja', '1997', 7.0, 51), (3371, 'tt0358914', 'Aavida Maa Aavide', '1998', 6.4, 134), (3372, 'tt1584917', 'Bolo Raam', '2009', 5.2, 73), (3373, 'tt1503640', 'Damadamm!', '2011', 4.2, 380), (3374, 'tt1942905', 'Kucch Luv Jaisaa', '2011', 4.4, 111), (3375, 'tt0449982', 'Hum Dum', '2005', 7.0, 89), (3376, 'tt2373632', 'Kalloori Vaasal', '1996', 5.5, 102), (3377, 'tt1285147', 'Visakha Express', '2008', 4.9, 57), (3378, 'tt2997780', 'Edegarike', '2012', 7.3, 182), (3379, 'tt0420854', 'Ranga S.S.L.C', '2004', 6.3, 119), (3380, 'tt0125485', 'Sazaye Maut', '1981', 7.4, 62), (3381, 'tt2950478', 'Mr. Theertha', '2010', 7.0, 51), (3382, 'tt1630282', 'Sahi Dhandhe Galat Bande', '2011', 4.8, 256), (3383, 'tt0353675', 'Love Birds', '1997', 5.1, 104), (3384, 'tt0374342', 'Yaad Rakhegi Duniya', '1992', 7.6, 79), (3385, 'tt0102513', 'Narasimha', '1991', 5.7, 324), (3386, 'tt0246766', 'Maa', 'I 1992', 6.2, 80), (3387, 'tt0165625', 'Ankhiyon Ke Jharokhon Se', '1978', 7.1, 317), (3388, 'tt0321067', 'Ab Ke Baras', '2002', 3.4, 180), (3389, 'tt0267252', 'Ajay', '1996', 3.9, 130), (3390, 'tt0082314', 'Ek Duuje Ke Liye', '1981', 7.5, 918), (3391, 'tt0929754', 'Ganga', '2006', 5.0, 53), (3392, 'tt0246204', 'Rudranetra', '1989', 7.0, 55), (3393, 'tt1886663', 'Satrangee Parachute', '2011', 5.5, 57), (3394, 'tt0095857', 'Pestonjee', '1988', 7.1, 129), (3395, 'tt0158833', 'Once We Were Strangers', '1997', 6.7, 221), (3396, 'tt0155928', 'Oru Kaidhiyin Diary', '1985', 7.0, 180), (3397, 'tt0293304', 'Kairee', '1999', 7.7, 99), (3398, 'tt1501301', 'Morning Walk', '2009', 4.6, 53), (3399, 'tt0484279', 'Miss India: The Mystery', '2003', 3.6, 57), (3400, 'tt0268533', 'Phir Subha Hogi', '1958', 7.7, 60), (3401, 'tt3160450', 'Ooops a Desi', '2013', 7.0, 175), (3402, 'tt0271197', 'Snip!', '2000', 6.3, 121), (3403, 'tt0296274', 'The Prince of Light', '2000', 7.0, 74), (3404, 'tt0395519', 'Doodh Ka Karz', '1990', 5.0, 92), (3405, 'tt0254437', 'Izzat', '1968', 7.2, 58), (3406, 'tt0112916', 'Dushmani: A Violent Love Story', '1995', 5.2, 152), (3407, 'tt0350195', 'Teri Meherbaniyan', '1985', 5.3, 180), (3408, 'tt0118924', 'Dance of the Wind', '1997', 6.6, 114), (3409, 'tt0382188', 'Mumbai Matinee', '2003', 5.3, 212), (3410, 'tt2963042', 'Mahakali Ka Insaaf', '2001', 9.6, 60), (3411, 'tt0380337', 'Ek Din 24 Ghante', '2003', 4.0, 74), (3412, 'tt1502960', 'The Taste of Relation', '2009', 7.2, 664), (3413, 'tt0156361', 'Bhootnath', '1963', 6.4, 61), (3414, 'tt0398390', 'White Rainbow', '2004', 7.2, 104), (3415, 'tt0353848', 'Ottran', '2003', 5.7, 53), (3416, 'tt0438153', \"Let's Enjoy\", '2004', 6.6, 108), (3417, 'tt0157729', 'Hamari Bahu Alka', '1982', 6.8, 53), (3418, 'tt3477498', 'Masala Republic', '2014', 5.4, 87), (3419, 'tt0158481', 'Bagh Bahadur', '1989', 7.9, 55), (3420, 'tt0114618', 'Target', '1995', 7.8, 71), (3421, 'tt0267524', 'Ghatotkachudu', '1995', 7.2, 82), (3422, 'tt0089610', 'Mohabbat', '1985', 5.9, 73), (3423, 'tt0269758', 'Raja Ki Ayegi Baraat', '1997', 3.8, 301), (3424, 'tt0317214', 'Captain Prabhakaran', '1991', 7.1, 162), (3425, 'tt0187351', 'Nigahen: Nagina Part II', '1989', 5.2, 231), (3426, 'tt1582597', 'Sathyam', '2008', 5.2, 109), (3427, 'tt1948047', 'Impatient Vivek', '2011', 2.3, 55), (3428, 'tt1682931', 'Life! Camera Action...', '2012', 6.0, 114), (3429, 'tt1668078', 'Daayen Ya Baayen', '2010', 6.1, 134), (3430, 'tt1679290', 'Shuttlecock Boys', '2011', 5.9, 106), (3431, 'tt2591508', 'Surkhaab', '2014', 6.5, 56), (3432, 'tt1224454', 'Sirf....: Life Looks Greener on the Other Side', '2008', 6.0, 87), (3433, 'tt2075376', 'Laadli Laila', '2009', 5.9, 52), (3434, 'tt1391894', 'Siddharth: The Prisoner', '2009', 6.6, 104), (3435, 'tt0461258', 'Kal: Yesterday and Tomorrow', '2005', 6.4, 109), (3436, 'tt2917786', 'Thirupathi', '2006', 6.6, 53), (3437, 'tt1087526', 'Tandoori Love', '2008', 5.2, 181), (3438, 'tt1074201', \"It's Breaking News\", '2007', 5.5, 61), (3439, 'tt0068929', 'Maya Darpan', '1972', 6.9, 53), (3440, 'tt0319621', 'Kaadhal Parisu', '1987', 6.1, 79), (3441, 'tt0843327', 'Boss', '2006', 4.4, 239), (3442, 'tt0441303', 'Hei ma lai ah sing', '2005', 3.8, 280), (3443, 'tt0257416', 'Ashwamedham', '1992', 7.2, 52), (3444, 'tt1833008', 'Kvelaferi kargad iqneba', '2009', 6.0, 136), (3445, 'tt1583363', 'Seema Simham', '2002', 5.1, 64), (3446, 'tt1825655', 'Angel', 'I 2011', 6.6, 76), (3447, 'tt0055035', 'Junglee', '1961', 7.3, 397), (3448, 'tt0363887', 'Phool Bane Angaarey', '1991', 4.9, 102), (3449, 'tt0298327', 'Fakira', '1976', 6.5, 76), (3450, 'tt0307116', 'Maa Tujhhe Salaam', '2002', 3.5, 1030), (3451, 'tt2356426', 'Le Halua Le', '2012', 6.0, 85), (3452, 'tt8085616', 'Shohrat the Trap', '2018', 6.5, 100), (3453, 'tt1773774', 'The Untitled Kartik Krishnan Project', '2010', 6.4, 52), (3454, 'tt0245830', 'Chettaniki Kallu Levu', '1981', 7.8, 71), (3455, 'tt0497606', 'Dansh', '2005', 7.2, 61), (3456, 'tt2971560', 'Ram Raaj', '2008', 7.5, 149), (3457, 'tt0098976', 'Aaj Ka Arjun', '1990', 4.7, 509), (3458, 'tt0290632', 'Goa Dalli CID 999', '1968', 8.5, 51), (3459, 'tt1587388', 'Yeh Hai Malegaon Ka Superman', '2009', 7.5, 63), (3460, 'tt3445640', 'Military Officer', '1998', 9.4, 61), (3461, 'tt4944394', 'Man on Mission Taqatwar', '2005', 9.5, 58), (3462, 'tt0318607', 'Raasaiyya', '1995', 5.2, 167), (3463, 'tt0432288', 'Finding Preet', '2006', 5.0, 51), (3464, 'tt0106799', 'Ek Hi Raasta', '1993', 3.6, 149), (3465, 'tt0273804', 'Mehndi', '1998', 3.5, 241), (3466, 'tt0363011', 'Raja Aur Rangeeli', '1996', 9.0, 55), (3467, 'tt0322816', 'Thoongathey Tambi Thoongathey', '1983', 7.0, 89), (3468, 'tt0439464', 'Bewafa Sanam', '1995', 6.4, 77), (3469, 'tt0165795', 'Geet Gaata Chal', '1975', 7.7, 79), (3470, 'tt0090611', 'Allah-Rakha', '1986', 6.2, 96), (3471, 'tt0106270', 'Anari', '1993', 4.7, 301), (3472, 'tt0852989', 'Come December', '2006', 5.7, 57), (3473, 'tt0375882', 'Kala Jigar', '1939', 3.3, 174), (3474, 'tt0375890', 'Kanoon', '1994', 3.2, 103)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = 'What are names of films where year is 2018?'\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "# Update the path to the database file\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "\n",
        "# Perform pooling for query sentence\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embedding for query sentence\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "\n",
        "# Find the table name and column name in the query sentence by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "table_name = None\n",
        "column_name = None\n",
        "for table_name in table_names:\n",
        "    table_names_encoded = tokenizer(\n",
        "        [table_name], padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        table_names_output = model(**table_names_encoded)\n",
        "    table_names_embeddings = mean_pooling(\n",
        "        table_names_output, table_names_encoded['attention_mask'])\n",
        "    table_names_embeddings = F.normalize(\n",
        "        table_names_embeddings, p=2, dim=1)\n",
        "    cosine_similarity = torch.nn.functional.cosine_similarity(\n",
        "        query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "    if cosine_similarity > 0.5:\n",
        "        break\n",
        "\n",
        "# Find the column name in the query sentence by computing the cosine similarity between the query sentence embedding and the column names embeddings\n",
        "if table_name is not None:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names = [column_info[1] for column_info in cursor.fetchall()]\n",
        "    column_names_encoded = tokenizer(\n",
        "        column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        column_names_output = model(**column_names_encoded)\n",
        "    column_names_embeddings = mean_pooling(\n",
        "        column_names_output, column_names_encoded['attention_mask'])\n",
        "    column_names_embeddings = F.normalize(\n",
        "        column_names_embeddings, p=2, dim=1)\n",
        "    cosine_similarities_columns = torch.nn.functional.cosine_similarity(\n",
        "        query_sentence_embedding, column_names_embeddings, dim=1)\n",
        "    most_similar_column_names_indices = cosine_similarities_columns.argsort(\n",
        "        descending=True)\n",
        "    most_similar_column_names = [column_names[i]\n",
        "                                for i in most_similar_column_names_indices]\n",
        "    column_name = most_similar_column_names[0]\n",
        "\n",
        "# Generate the SQL query\n",
        "if table_name is not None and column_name is None:\n",
        "    # If only the table name is found, select all columns from the table\n",
        "    sql_query = f\"SELECT * FROM {table_name}\"\n",
        "elif table_name is not None and column_name is not None:\n",
        "    # If both the table name and column name are found, select only the specified column from the table\n",
        "    sql_query = f\"SELECT {column_name} FROM {table_name}\"\n",
        "else:\n",
        "    # If no table name or column name is found, return an empty result\n",
        "    sql_query = \"SELECT * FROM WHERE 1=2\"\n",
        "\n",
        "# Add a WHERE condition to the SQL query if needed\n",
        "where_condition = None\n",
        "for word in query_sentence.split():\n",
        "    if word.lower() in column_names:\n",
        "        where_condition = word.lower()\n",
        "        break\n",
        "if where_condition is not None:\n",
        "    sql_query += f\" WHERE {where_condition}=?\"\n",
        "\n",
        "print(sql_query)\n",
        "\n",
        "# # Execute the SQL query\n",
        "# if where_condition is not None:\n",
        "#     cursor.execute(sql_query, [2018])\n",
        "# else:\n",
        "#     cursor.execute(sql_query)\n",
        "# results = cursor.fetchall()\n",
        "\n",
        "# # Print the results\n",
        "# for result in results:\n",
        "#     print(result)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbNqNpuZ7wt9",
        "outputId": "d7d793ce-e434-4fad-af64-4124cded893c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT MID FROM M_Cast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def find_where_condition(question):\n",
        "  doc = nlp(question)\n",
        "  \n",
        "  where_condition = []\n",
        "  \n",
        "  for token in doc:\n",
        "    if token.dep_ == 'prep':\n",
        "      # Check if the token is a preposition that is part of a WHERE condition\n",
        "      if token.head.text.lower() in ['where']:\n",
        "        # Add the preposition and the object of the preposition to the WHERE condition\n",
        "        where_condition.append(token.text)\n",
        "        where_condition.append(token.head.text)\n",
        "  \n",
        "  return ' '.join(where_condition)\n",
        "\n",
        "question = \"What is the title of the movie directed by Christopher Nolan?\"\n",
        "where_condition = find_where_condition(question)\n",
        "print(where_condition)  # Output: \"directed by Christopher Nolan\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j_35tfv9s-I",
        "outputId": "cafa5a72-78f9-434f-8b15-50ff31a0c01b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = 'What are names of films?'\n",
        "\n",
        "# Function to find WHERE clause present in the query sentence semantically\n",
        "def find_where_clause(query_sentence):\n",
        "    # Tokenize the query sentence\n",
        "    query_sentence_encoded = tokenizer(\n",
        "        [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Compute token embeddings for the query sentence\n",
        "    with torch.no_grad():\n",
        "        query_sentence_output = model(**query_sentence_encoded)\n",
        "\n",
        "    # Perform mean pooling on the output of the language model\n",
        "    query_sentence_embedding = mean_pooling(\n",
        "        query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "\n",
        "    # Normalize the embeddings\n",
        "    query_sentence_embedding = F.normalize(\n",
        "        query_sentence_embedding, p=2, dim=1)\n",
        "\n",
        "    # Find the most similar word to the word 'where' in the query sentence by computing the cosine similarity between the query sentence embedding and the word embeddings of the word 'where'\n",
        "    where_word_embedding = tokenizer(\n",
        "        ['where'], padding=True, truncation=True, return_tensors='pt')['input_ids'][0]\n",
        "    where_word_embedding = model.embeddings.word_embeddings(\n",
        "        where_word_embedding).unsqueeze(0)\n",
        "    where_word_embedding = F.normalize(where_word_embedding, p=2, dim=1)\n",
        "    cosine_similarities = torch.nn.functional.cosine_similarity(\n",
        "        query_sentence_embedding, where_word_embedding, dim=1)\n",
        "\n",
        "    # If the cosine similarity score is greater than 0.5, then the word 'where' is present in the query sentence semantically\n",
        "    if cosine_similarities[0] > 0.5:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Print whether the word 'where' is present in the query sentence semantically or not\n",
        "print(find_where_clause(query_sentence))\n",
        "\n",
        "\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "# Update the path to the database file\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name by using the index obtained above\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Find the column names of the highest matching table by querying the database\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [\n",
        "    column_info[1] for column_info in cursor.fetchall()]\n",
        "\n",
        "# Tokenize the column names of the highest matching table\n",
        "highest_matching_table_column_names_encoded = tokenizer(\n",
        "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute the token embeddings for the column names of the highest matching table\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(\n",
        "        **highest_matching_table_column_names_encoded)\n",
        "\n",
        "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(\n",
        "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize the embeddings for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = F.normalize(\n",
        "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "\n",
        "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
        "    most_similar_highest_matching_table_column_name_index]\n",
        "\n",
        "\n",
        "# Generate an SQL SELECT query based on the most similar table names and column names\n",
        "query = f\"SELECT {most_similar_highest_matching_table_column_name} FROM {most_similar_table_names[0]}\"\n",
        "\n",
        "# Iterate through the list of possible queries and execute each one\n",
        "\n",
        "# Print the generated SQL query\n",
        "print(query)\n",
        "\n",
        "try:\n",
        "    cursor.execute(query)\n",
        "    results = cursor.fetchall()\n",
        "    print(f'Query: {query}')\n",
        "    print(f'Results: {results}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "\n",
        "\n",
        "# close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "MjBPp70aAtuU",
        "outputId": "cc906af4-a11e-4efa-9f8f-0e334205734a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-46ac15e6c3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Print whether the word 'where' is present in the query sentence semantically or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_where_clause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-46ac15e6c3e6>\u001b[0m in \u001b[0;36mfind_where_clause\u001b[0;34m(query_sentence)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# If the cosine similarity score is greater than 0.5, then the word 'where' is present in the query sentence semantically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcosine_similarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = input(\"Enter English Question: \")\n",
        "\n",
        "# Function to find WHERE clause present in the query sentence semantically\n",
        "def find_where_clause(query_sentence):\n",
        "    # Tokenize the query sentence\n",
        "    query_sentence_encoded = tokenizer(\n",
        "        [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Compute token embeddings for the query sentence\n",
        "    with torch.no_grad():\n",
        "        query_sentence_output = model(**query_sentence_encoded)\n",
        "\n",
        "    # Perform mean pooling on the output of the language model\n",
        "    query_sentence_embedding = mean_pooling(\n",
        "        query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "\n",
        "    # Normalize the embeddings\n",
        "    query_sentence_embedding = F.normalize(\n",
        "        query_sentence_embedding, p=2, dim=1)\n",
        "\n",
        "    # Find the most similar word to the word 'where' in the query sentence by computing the cosine similarity between the query sentence embedding and the word embeddings of the word 'where'\n",
        "    where_word_embedding = tokenizer(\n",
        "        ['where'], padding=True, truncation=True, return_tensors='pt')['input_ids'][0]\n",
        "    where_word_embedding = model.embeddings.word_embeddings(\n",
        "        where_word_embedding).unsqueeze(0)\n",
        "    where_word_embedding = F.normalize(where_word_embedding, p=2, dim=1)\n",
        "    cosine_similarities = torch.nn.functional.cosine_similarity(\n",
        "        query_sentence_embedding, where_word_embedding, dim=1)\n",
        "\n",
        "    # If the cosine similarity score is greater than 0.5, then the word 'where' is present in the query sentence semantically\n",
        "    # if cosine_similarities[0] > 0.5:\n",
        "    #     return True\n",
        "    # else:\n",
        "    #     return False\n",
        "    return cosine_similarities[0]\n",
        "\n",
        "\n",
        "# Print whether the word 'where' is present in the query sentence semantically or not\n",
        "print(find_where_clause(query_sentence))\n",
        "\n",
        "\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "# Update the path to the database file\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name by using the index obtained above\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Find the column names of the highest matching table by querying the database\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [\n",
        "    column_info[1] for column_info in cursor.fetchall()]\n",
        "\n",
        "# Tokenize the column names of the highest matching table\n",
        "highest_matching_table_column_names_encoded = tokenizer(\n",
        "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute the token embeddings for the column names of the highest matching table\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(\n",
        "        **highest_matching_table_column_names_encoded)\n",
        "\n",
        "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(\n",
        "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize the embeddings for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = F.normalize(\n",
        "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "\n",
        "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
        "    most_similar_highest_matching_table_column_name_index]\n",
        "\n",
        "\n",
        "# Generate an SQL SELECT query based on the most similar table names and column names\n",
        "query = f\"SELECT {most_similar_highest_matching_table_column_name} FROM {most_similar_table_names[0]}\"\n",
        "\n",
        "# Iterate through the list of possible queries and execute each one\n",
        "\n",
        "# Print the generated SQL query\n",
        "print(query)\n",
        "\n",
        "try:\n",
        "    cursor.execute(query)\n",
        "    results = cursor.fetchall()\n",
        "    print(f'Query: {query}')\n",
        "    print(f'Results: {results}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "\n",
        "\n",
        "# close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckmxp4FcBAln",
        "outputId": "8494a9c3-4d3a-47b2-daaa-97a4c28e7cbe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English Question: list names of films?\n",
            "tensor([-6.2479e-01,  9.1112e-01,  4.5930e-01, -6.6885e-01,  5.7831e-01,\n",
            "        -2.3079e-01,  6.9298e-01,  3.8513e-01,  1.5325e-01,  2.0177e-01,\n",
            "        -1.3969e-01, -8.7173e-01, -6.4584e-01, -9.1482e-01,  5.7355e-01,\n",
            "         4.7524e-01, -7.3181e-01, -4.8331e-01,  1.5521e-01,  2.6555e-01,\n",
            "         3.8588e-01, -7.9231e-01, -5.5317e-02, -3.2534e-01,  4.6411e-01,\n",
            "         7.0094e-01, -7.4526e-01,  5.7568e-01,  2.7745e-01,  4.1117e-01,\n",
            "        -4.0875e-01, -8.5912e-01,  6.7275e-01,  4.7819e-01, -7.9872e-01,\n",
            "        -3.9493e-02,  2.0297e-01,  1.4098e-01, -3.6509e-02, -3.3213e-01,\n",
            "         5.5716e-01,  4.7439e-01,  5.7596e-01, -4.9438e-01,  5.0285e-01,\n",
            "         6.7864e-01,  3.1534e-01, -3.6764e-01,  6.6232e-01,  7.8794e-01,\n",
            "         2.7150e-01, -7.3286e-02,  3.7914e-01,  7.6099e-01, -7.1334e-01,\n",
            "        -4.4301e-01,  4.8762e-02,  6.0840e-01, -1.6086e-01, -7.5905e-01,\n",
            "         4.0591e-01, -2.2273e-02,  5.5880e-01, -5.7640e-01, -1.5227e-01,\n",
            "         2.6524e-01,  5.4712e-01,  3.4068e-01, -3.2833e-01,  1.2673e-01,\n",
            "         1.8774e-01,  2.7240e-01, -6.6284e-01, -3.4409e-01, -8.3190e-01,\n",
            "        -6.8933e-01, -8.3304e-01, -2.7281e-01,  2.2917e-01,  8.7334e-01,\n",
            "         4.4356e-01,  8.8324e-01,  8.9221e-01,  6.1860e-01, -3.0205e-01,\n",
            "         9.0002e-01,  8.9020e-01, -5.2921e-01, -4.0574e-01,  7.5351e-01,\n",
            "         1.3628e-01, -8.3743e-01, -4.7049e-01,  8.6839e-01,  6.8779e-01,\n",
            "         9.4491e-01, -2.8998e-01, -9.2786e-01,  2.7722e-01,  5.8767e-01,\n",
            "        -3.4112e-01, -4.6628e-01,  1.8114e-01, -4.7356e-01,  2.7976e-01,\n",
            "        -7.3874e-01, -5.7371e-03,  2.8485e-01,  8.0346e-01, -7.1226e-02,\n",
            "         1.0551e-01, -1.3839e-01, -1.2401e-03,  2.6180e-01,  2.1045e-01,\n",
            "         7.5670e-01,  9.5101e-01,  8.9191e-01, -4.5026e-01, -9.7766e-01,\n",
            "        -6.9037e-01,  7.9768e-01, -6.2003e-01,  2.8482e-01, -1.3050e-01,\n",
            "         8.6502e-02,  2.6485e-01, -4.0365e-25, -3.5438e-01, -6.2324e-01,\n",
            "        -5.1070e-01, -3.2888e-01,  2.6667e-01,  3.8890e-01, -1.9370e-01,\n",
            "        -5.9041e-01,  1.3488e-01, -8.9271e-01,  1.3881e-01,  4.5839e-01,\n",
            "        -5.3733e-01, -4.5654e-01,  9.2735e-01,  6.1664e-01, -7.8784e-02,\n",
            "         9.0504e-01,  5.1305e-01, -3.0466e-02, -3.3070e-01, -5.6022e-01,\n",
            "         5.8437e-01,  9.5843e-01, -4.7992e-01, -8.3567e-01, -6.0282e-01,\n",
            "         9.8292e-01, -2.6018e-02, -5.5966e-01,  4.2831e-01,  4.6264e-01,\n",
            "        -5.0552e-01, -2.5523e-02,  7.4079e-01,  4.7946e-01,  8.1962e-01,\n",
            "         9.6596e-01, -6.6591e-01,  8.6143e-01, -5.5721e-01,  6.4382e-01,\n",
            "         9.5040e-01,  8.0481e-02,  9.2161e-01,  8.8625e-01,  5.8631e-01,\n",
            "         4.3733e-02,  2.0173e-01,  4.2543e-01, -8.4031e-02, -6.7198e-02,\n",
            "        -1.4610e-01, -8.9047e-01,  1.4520e-01,  3.5489e-01, -8.8132e-02,\n",
            "        -7.1473e-01, -9.9893e-01, -1.7447e-01,  1.1312e-01, -1.8076e-01,\n",
            "         3.3795e-01,  6.1228e-01,  9.8044e-01, -6.9274e-01, -1.1982e-03,\n",
            "         8.5222e-01, -4.8918e-01,  1.9898e-01,  2.1795e-01,  1.6455e-01,\n",
            "         3.5191e-01, -4.0889e-01, -3.4173e-01, -6.3069e-01,  2.7883e-01,\n",
            "        -3.0263e-01,  3.6800e-01,  8.1695e-01,  9.6256e-01, -6.9780e-01,\n",
            "         3.0515e-01,  7.0855e-01,  3.3907e-01,  2.6741e-01, -2.9146e-01,\n",
            "        -8.9004e-01,  3.1585e-01,  4.9429e-01, -7.1054e-01, -8.1357e-02,\n",
            "         2.4283e-01,  8.6640e-01, -9.1874e-01,  6.6796e-26, -3.3617e-01,\n",
            "         8.0402e-01, -9.8507e-01,  5.5868e-01,  2.6291e-01, -6.9923e-01,\n",
            "         7.5036e-02,  3.3999e-01, -3.7612e-02,  9.0808e-01, -6.4922e-01,\n",
            "        -2.9898e-01,  4.2787e-01, -4.4857e-01, -9.9145e-01, -5.4514e-01,\n",
            "        -6.8998e-01, -2.0840e-02,  4.9283e-01,  5.7941e-01,  7.1865e-01,\n",
            "        -2.2972e-02,  4.9084e-01, -1.2262e-01,  4.1438e-01, -7.3506e-01,\n",
            "        -6.9219e-01, -8.2531e-01,  5.4583e-01, -4.6735e-01,  2.5154e-02,\n",
            "         4.0373e-01,  9.8019e-01, -2.0843e-01, -4.5814e-01,  7.8730e-01,\n",
            "        -4.4334e-01, -1.4608e-01,  4.8254e-01,  2.6706e-01,  1.1612e-01,\n",
            "        -3.1881e-01,  2.9432e-02,  8.9045e-01,  3.8330e-01, -5.7700e-01,\n",
            "        -2.6837e-01,  4.0904e-01,  8.6788e-01, -2.3404e-01, -1.3470e-01,\n",
            "         2.5416e-01,  9.8651e-01,  8.6346e-01,  1.0012e-01, -2.2378e-01,\n",
            "        -3.4555e-01,  3.4624e-02,  3.6831e-01, -3.8376e-01,  1.3444e-01,\n",
            "         6.4550e-01,  8.9245e-01, -4.5635e-01, -9.5160e-02,  4.6376e-01,\n",
            "         1.5743e-01, -8.4133e-02, -1.0387e-01,  5.5330e-01, -4.0453e-01,\n",
            "         6.5063e-01,  5.7479e-01, -8.5537e-01,  7.3240e-01,  5.0992e-01,\n",
            "        -3.7968e-01, -1.0162e-02, -7.7846e-01,  5.6398e-01, -9.1565e-01,\n",
            "         6.7054e-01, -7.7212e-01,  2.8006e-01, -4.3215e-01,  9.8941e-01,\n",
            "        -1.3766e-01,  4.0193e-01,  4.3697e-01,  3.6365e-01, -2.8328e-01,\n",
            "        -2.7475e-01, -4.7439e-01,  9.4651e-01, -6.6702e-02,  8.2287e-01,\n",
            "         8.7742e-01, -5.0656e-01, -2.0202e-01,  6.8348e-01, -9.3719e-01,\n",
            "         3.4176e-01,  1.3550e-01,  9.8552e-01,  8.7008e-01,  3.5789e-01,\n",
            "         1.7912e-01,  6.7838e-02,  3.3003e-01,  5.6890e-01,  9.0630e-01,\n",
            "         5.1979e-01,  8.9979e-01, -9.2574e-02,  2.7532e-01, -2.0415e-01,\n",
            "        -8.0152e-01, -2.4580e-01,  5.3788e-01, -3.5188e-01,  9.6717e-01,\n",
            "        -2.1975e-01, -8.3920e-01, -8.2296e-01, -9.2682e-01, -8.0100e-03,\n",
            "        -7.4828e-01, -5.8463e-02,  4.7422e-01,  3.6087e-01,  2.7440e-01,\n",
            "        -2.0501e-01,  4.6442e-01,  5.9245e-02, -3.6568e-01,  6.0338e-01,\n",
            "         2.7019e-01,  1.6282e-01, -2.5098e-01, -9.8782e-01,  4.7689e-01,\n",
            "         5.9121e-01, -9.1037e-02, -9.2397e-01, -2.1541e-02, -7.8860e-01,\n",
            "        -6.1844e-01,  3.4894e-01,  3.5962e-03,  7.7555e-01,  3.1402e-01,\n",
            "        -4.2025e-01, -9.4389e-01,  5.1281e-01,  5.6159e-01,  2.0078e-01,\n",
            "        -4.1014e-01, -4.0565e-01, -3.5365e-01,  2.4628e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Table name: Movie, cosine similarity score: 0.4740450382232666\n",
            "Table name: Genre, cosine similarity score: 0.3992653489112854\n",
            "Table name: M_Genre, cosine similarity score: 0.3120344281196594\n",
            "Table name: M_Director, cosine similarity score: 0.279435932636261\n",
            "Table name: M_Cast, cosine similarity score: 0.25687310099601746\n",
            "Table name: M_Producer, cosine similarity score: 0.19183848798274994\n",
            "Table name: Country, cosine similarity score: 0.1532454937696457\n",
            "Table name: M_Country, cosine similarity score: 0.1460212916135788\n",
            "Table name: Language, cosine similarity score: 0.1452818214893341\n",
            "Table name: Location, cosine similarity score: 0.1363571137189865\n",
            "Table name: M_Location, cosine similarity score: 0.0899093747138977\n",
            "Table name: Person, cosine similarity score: 0.08245006948709488\n",
            "Table name: M_Language, cosine similarity score: 0.08129163086414337\n",
            "SELECT title FROM Movie\n",
            "Query: SELECT title FROM Movie\n",
            "Results: [('Mowgli',), (\"Ocean's Eight\",), ('Tomb Raider',), ('The Avengers',), ('Tumbbad',), ('Kedarnath',), ('Captain America: Civil War',), ('Andhadhun',), ('Lion',), ('Rajma Chawal',), ('Geostorm',), ('Slumdog Millionaire',), ('2012',), ('Mastizaade',), ('Close Encounters of the Third Kind',), ('Manto',), ('Life of Pi',), ('A Good Day to Die Hard',), ('Pixels',), ('Stree',), ('Thugs of Hindostan',), ('Badhaai Ho',), ('Hotel Mumbai',), ('The Darjeeling Limited',), ('Jalebi',), ('Bend It Like Beckham',), ('Dangal',), ('Manmarziyaan',), ('The Day the Earth Stood Still',), ('Dragonball Evolution',), ('Pataakha',), ('I Origins',), ('Harold & Kumar Go to White Castle',), ('3 Idiots',), ('Taare Zameen Par',), ('Bajirao Mastani',), ('PK',), ('The Hundred-Foot Journey',), ('Gangs of Wasseypur',), ('Seven Years in Tibet',), ('Octopussy',), ('Chung Hing sam lam',), ('Sanju',), ('Victoria & Abdul',), ('Karwaan',), ('Mitron',), ('Padman',), ('Tigers',), ('Ghajini',), ('The Best Exotic Marigold Hotel',), ('Around the World in 80 Days',), ('The Ghost and the Darkness',), ('Padmaavat',), ('Raazi',), ('Bareilly Ki Barfi',), ('Lagaan: Once Upon a Time in India',), ('Lust Stories',), ('My Name Is Khan',), ('Love Sonia',), ('Batti Gul Meter Chalu',), ('Loveyatri',), ('Don 2',), ('Rang De Basanti',), ('Love Per Square Foot',), ('Ugly',), ('Bahubali: The Beginning',), ('Anand',), ('Baahubali 2: The Conclusion',), ('Sui Dhaaga: Made in India',), ('FryDay',), ('Se, jie',), ('Gold',), ('Bhavesh Joshi Superhero',), ('Sonu Ke Titu Ki Sweety',), ('Satyameva Jayate',), ('Race 3',), ('The Accidental Husband',), ('Million Dollar Arm',), ('Nasha',), ('Dabba',), ('Vikram Vedha',), ('Trapped',), ('Parmanu: The Story of Pokhran',), ('Barfi!',), ('Auntie Mame',), ('Beyond the Clouds',), ('Tamasha',), ('Mukkabaaz',), ('Bride & Prejudice',), ('Dhadak',), ('Mom',), ('A Passage to India',), ('Happy Phirr Bhag Jayegi',), ('Kabhi Khushi Kabhie Gham...',), ('Masaan',), (\"Viceroy's House\",), ('Laila Majnu',), ('Gurgaon',), ('Dear Zindagi',), ('Newton',), ('Aiyaary',), ('Nayakan',), ('Baazaar',), ('Raid',), ('Dil Dhadakne Do',), ('October',), ('Talvar',), ('Shahid',), ('Raees',), ('Kahaani',), ('One Less God',), ('Gunday',), ('Beavis and Butt-Head Do America',), ('Bajrangi Bhaijaan',), ('Parched',), ('Jagga Jasoos',), ('Agneepath',), ('Mary Kom',), ('Udta Punjab',), ('Shaadi Mein Zaroor Aana',), ('Street Fighter: The Legend of Chun-Li',), ('Black',), ('Soorma',), ('Raman Raghav 2.0',), ('Krrish',), ('Airlift',), ('Devdas',), ('Ittefaq',), ('Ae Dil Hai Mushkil',), ('Blackmail',), ('Pink',), ('Go Goa Gone',), ('Drishyam',), ('Dostana',), ('Anbe Sivam',), ('Veere Di Wedding',), ('Dil Juunglee',), ('Dhoom:3',), ('Secret Superstar',), ('A Wednesday',), ('Goliyon Ki Rasleela Ram-Leela',), ('B.A. Pass',), ('Sultan',), ('Toilet - Ek Prem Katha',), ('Garbage',), ('Dilwale Dulhania Le Jayenge',), ('Dev.D',), ('Mulk',), ('Efter brylluppet',), ('Tiger Zinda Hai',), ('Kal Ho Naa Ho',), ('Kapoor & Sons',), ('Halkaa',), ('Kaabil',), ('Shivaay',), ('Dilwale',), ('Monsoon Wedding',), ('Raabta',), ('Talaash',), ('Badlapur',), ('The Other Side of the Door',), ('Rockstar',), ('Om Shanti Om',), ('Kuch Kuch Hota Hai',), ('Jab We Met',), ('Te3n',), ('Jab Harry Met Sejal',), ('Haider',), ('The Lovers',), ('Tubelight',), ('Welcome to New York',), ('Veer-Zaara',), ('Delhi Belly',), ('Piku',), ('Jodhaa Akbar',), ('A Gentleman',), ('Ra.One',), ('Mersal',), ('Hichki',), ('Zindagi Na Milegi Dobara',), ('Rab Ne Bana Di Jodi',), ('Dishoom',), ('Paan Singh Tomar',), ('Qarib Qarib Singlle',), ('Kai po che!',), ('Yeh Jawaani Hai Deewani',), ('Sanam Teri Kasam',), ('Myeong-ryang',), ('Swades: We, the People',), ('Kaalakaandi',), ('Naam Shabana',), ('Jab Tak Hai Jaan',), ('102 Not Out',), ('The Challenger',), ('Baaghi 2',), ('Namaste England',), ('Sholay',), ('Omerta',), ('Nanu Ki Jaanu',), ('The Little Princess',), ('Mard Ko Dard Nahin Hota',), ('Pari',), ('Chak De! India',), ('Queen',), ('Chennai Express',), ('Black Friday',), ('Love Aaj Kal',), ('Detective Byomkesh Bakshy!',), ('Dhoom:2',), ('The Namesake',), ('Loev',), ('Fitoor',), ('Half Girlfriend',), ('Pihu',), ('Haraamkhor',), ('Shubh Mangal Saavdhan',), ('The Guru',), ('Baar Baar Dekho',), ('Hate Story IV',), ('Munna Bhai M.B.B.S.',), ('Baby',), ('Bing feng: Chong sheng zhi men',), ('The House Next Door',), ('Mohabbatein',), ('Phantom',), ('Chef',), ('Love Games',), ('Brij Mohan Amar Rahe',), ('Ankhon Dekhi',), ('Bhaiaji Superhit',), ('Chal Bhaag',), ('Salaam Bombay!',), ('Kabali',), ('Hindi Medium',), ('Sivaji',), ('Khosla Ka Ghosla!',), ('Maqbool',), ('Special Chabbis',), ('Fashion',), ('Krrish 3',), ('Paltan',), ('Oye Lucky! Lucky Oye!',), ('Fan',), ('One Night Stand',), ('Yamla Pagla Deewana Phir Se...',), ('Pyaar Ka Punchnama',), ('Madaari',), ('OMG: Oh My God!',), ('Phamous',), ('Befikre',), ('Trishna',), ('Wake Up Sid',), ('Singham',), ('Don',), ('Margarita with a Straw',), ('Kites',), ('Dum Laga Ke Haisha',), ('Khoobsurat',), ('Horror Story',), ('Aashiqui 2',), ('Guzaarish',), ('Kaho Naa... Pyaar Hai',), ('Happy New Year',), ('Outsourced',), ('Rahasya',), ('7 Khoon Maaf',), ('Gulaal',), ('Missing',), ('A Death in the Gunj',), ('Hate Story',), ('Genius',), ('Bombay Talkies',), ('Student of the Year',), ('Golmaal Again',), ('Dil Chahta Hai',), ('Ship of Theseus',), ('No One Killed Jessica',), ('Pyaar Ka Punchnama 2',), ('Badrinath Ki Dulhania',), ('Rangoon',), ('Jism 2',), ('Mohenjo Daro',), ('Nawabzaade',), ('Dhoom',), ('Ek Tha Tiger',), ('Bombay Velvet',), ('Main Hoon Na',), ('Aligarh',), ('Vicky Donor',), ('Rustom',), ('Bang Bang',), ('Company',), ('Akira',), ('Jaane Tu... Ya Jaane Na',), ('Simran',), ('Katti Batti',), ('Roy',), ('Omkara',), ('City Lights',), ('Ekk Deewana Tha',), ('Bhaag Milkha Bhaag',), ('Ek Hasina Thi',), ('Guru',), ('Mubarakan',), ('Kurbaan',), ('Once Again',), ('Judwaa 2',), ('Kuchh Bheege Alfaaz',), ('Koi... Mil Gaya',), ('Baaghi',), ('Hera Pheri',), ('The Attacks of 26/11',), ('Indian',), ('Johnny Gaddaar',), ('Hotel Salvation',), ('Kaminey',), ('Ajji',), ('Dil Se..',), ('2 States',), ('The Other End of the Line',), ('Madras Cafe',), ('Kabhi Alvida Naa Kehna',), ('Ladies vs. Ricky Bahl',), ('Ragini MMS 2',), ('Hey Ram',), ('Neerja',), ('No Smoking',), ('Raanjhanaa',), ('Chashme Baddoor',), ('Cocktail',), ('Mujhse Dosti Karoge!',), ('Fanaa',), ('Race 2',), ('Udaan',), ('The Ghazi Attack',), ('M.S. Dhoni: The Untold Story',), ('Chandni Chowk to China',), ('Dhobi Ghat',), ('Lucknow Central',), ('Mirzya',), ('Josh',), ('Kahaani 2',), ('Sir',), ('Anniyan',), ('Xun Long Jue',), ('Tere Mere Sapne',), ('1920 London',), ('La mujer de mi hermano',), ('Angrezi Mein Kehte Hain',), ('Playback',), ('Hunterrr',), ('Hate Story 3',), ('Force 2',), ('Wazir',), ('Highway',), ('Andaz Apna Apna',), ('Band Baaja Baaraat',), ('Dil To Pagal Hai',), ('Lamhe',), ('Lipstick Under My Burkha',), ('Brothers',), ('Happy Bhag Jayegi',), ('Tees Maar Khan',), ('Meri Pyaari Bindu',), ('Madly',), ('Do Dooni Chaar',), ('Baazigar',), ('Rocky Handsome',), ('Gabbar is Back',), ('Singh Is Bliing',), ('English Vinglish',), ('Phobia',), ('Mangal Pandey: The Rising',), ('Shanghai',), ('Dark wind',), ('Guest iin London',), ('Rocky',), ('Baadshaho',), ('Kaante',), ('Sold',), ('Rock On!!',), ('We Are Family',), ('Ek Villain',), ('Raja Natwarlal',), ('Tanu Weds Manu',), ('Bioscopewala',), ('Maine Pyar Kiya',), ('3 Storeys',), ('Sarkar 3',), ('Al-risâlah',), ('Dobaara: See Your Evil',), ('Satya',), ('Love and Shukla',), ('Tikli and Laxmi Bomb',), ('Bhoomi',), ('Awarapan',), ('Jolly LLB 2',), ('Game 6',), ('Umrika',), ('Tanu Weds Manu Returns',), ('Delhi-6',), ('Paheli',), ('That Girl in Yellow Boots',), ('Kaagaz Ke Phool',), ('Any Body Can Dance 2',), ('Bewakoofiyaan',), ('The Song of Scorpions',), ('Mera Naam Joker',), ('Mumbai Meri Jaan',), ('Tumhari Sulu',), ('Saawariya',), ('Saheb Biwi Aur Gangster 3',), ('Alone',), ('Desi Boyz',), ('Raajneeti',), ('Hisss',), ('Shaitan',), ('Jai Gangaajal',), ('Eega',), ('The Dirty Picture',), ('I Hate Luv Storys',), ('Court',), ('Gol Maal',), ('Dolly Ki Doli',), ('Sangam',), ('Teri Bhabhi Hai Pagle',), ('Life in a Metro',), ('Khal Nayak',), ('Unfreedom',), ('Vishwaroopam 2',), ('Grand Masti',), ('A Flying Jatt',), ('Mission Kashmir',), ('Kambakkht Ishq',), ('Aurangzeb',), ('Namastey London',), ('Singham Returns',), ('X: Past Is Present',), ('Heroine',), ('Dhanak',), ('Murder',), ('Mardaani',), ('B.A. Pass 2',), ('Titli',), ('Yaadein...',), ('Fukrey Returns',), ('Freaky Ali',), ('Shorgul',), ('Thuppakki',), ('Bandit Queen',), (\"Neal 'N' Nikki\",), ('Sarkar',), ('Lakshya',), ('The Stoneman Murders',), ('Monsoon Shootout',), ('Hum Dil De Chuke Sanam',), ('NH10',), ('Helicopter Eela',), ('Joker',), ('Anjaana Anjaani',), ('Irada',), ('Boom',), ('John Day',), ('Shamitabh',), ('Murder 2',), ('Vodka Diaries',), ('Happy Ending',), ('OK Jaanu',), ('Pizza',), ('Patel Ki Punjabi Shaadi',), ('Mohalla Assi',), ('Shaurya: It Takes Courage to Make Right... Right',), ('Action Jackson',), ('Kyaa Kool Hain Hum 3',), ('Akaash Vani',), ('Vaastav: The Reality',), ('Housefull 3',), ('Noor',), ('Asoka',), ('Kick',), ('Yuva',), ('Heropanti',), ('Humpty Sharma Ki Dulhania',), ('Daddy',), ('High Jack',), ('Water',), ('Bollywood/Hollywood',), ('Tu Hai Mera Sunday',), ('Lupt',), ('Lootera',), ('Heyy Babyy',), (\"Kuldip Patwal: I Didn't Do It!\",), ('Behen Hogi Teri',), ('Race',), ('Hope Aur Hum',), ('Silvat',), ('Chalte Chalte',), ('Daas Dev',), ('Once Upon a Time in Mumbaai',), ('Kaun?',), ('Hostel',), ('Baasha',), ('Miss Teacher',), ('Fever',), ('Fiza',), ('Nil Battey Sannata',), ('Paanch',), ('Beiimaan Love',), ('Bhool Bhulaiyaa',), ('Hasee Toh Phasee',), ('Babumoshai Bandookbaaz',), ('The Hero of Color City',), ('Finding Fanny',), ('Jaane Bhi Do Yaaro',), ('Commando 2',), ('Adisaya Piravi',), ('Phillauri',), ('Roja',), ('Darr',), ('Ghanchakkar',), ('Hamari Adhuri Kahani',), ('Mahabharat',), ('Billu',), ('Jo Jeeta Wohi Sikandar',), ('Dhan Dhana Dhan Goal',), ('Knock Out',), ('Lage Raho Munna Bhai',), ('Zed Plus',), ('Hum Tumhare Hain Sanam',), ('North West Frontier',), ('Welcome',), ('The Blue Umbrella',), ('Aakrosh',), ('Munna Michael',), ('Socha Na Tha',), ('Rocket Singh: Salesman of the Year',), ('Manorama Six Feet Under',), ('Satyagraha',), ('Kaal',), ('Fire',), ('Shootout at Wadala',), ('Pardes',), ('Mujhse Shaadi Karogi',), ('Rangeela',), ('Agent Vinod',), ('Main Tera Hero',), ('Vishwaroopam',), ('Shaandaar',), ('Ki & Ka',), ('Chittagong',), ('Ankur Arora Murder Case',), ('Chup Chup Ke',), ('Aitraaz',), ('Bullett Raja',), ('Commando',), ('Bodyguard',), ('LSD: Love, Sex Aur Dhokha',), ('Mere Brother Ki Dulhan',), ('Bachna Ae Haseeno',), ('Welcome to Karachi',), ('Luv Shuv Tey Chicken Khurana',), ('Luv Ka the End',), ('Athadu',), ('Table No.21',), ('Nayak: The Real Hero',), ('Chupke Chupke',), ('Holiday',), ('Great Grand Masti',), ('Jaane Kahan Se Aayi Hai',), ('Bluffmaster!',), ('Cheeni Kum',), ('Players',), ('Aankhen',), ('Agneepath',), ('Vivah',), ('Dabangg',), ('Ek Main Aur Ekk Tu',), ('Warrior Savitri',), ('Aryan: Unbreakable',), ('1942: A Love Story',), ('Bol Bachchan',), ('ABCD Any Body Can Dance',), ('Tere Bin Laden',), ('Thank You',), ('D-Day',), ('Teen Patti',), ('Himmatwala',), ('Kill Dil',), ('Mirror Game',), ('Paa',), ('Baghban',), ('Ribbon',), ('Action Replayy',), ('Pyaar Impossible!',), ('Karan Arjun',), ('Sadma',), ('Salaam Namaste',), ('Hate Story 2',), ('Kochadaiiyaan',), ('Wanted',), ('A Sublime Love Story: Barsaat',), ('Badmaa$h Company',), ('Mr. India',), ('Angry Indian Goddesses',), ('Rudhramadevi',), ('Dhruva',), ('Sarrainodu',), ('Housefull',), ('Thaandavam',), ('Phir Hera Pheri',), ('De Dana Dan',), ('Saajan',), ('Gunda',), ('Mother India',), ('Nautanki Saala!',), ('Maroon',), ('Prem Ratan Dhan Payo',), ('Baadshah',), ('Rehnaa Hai Terre Dil Mein',), ('My Brother... Nikhil',), ('Force',), ('Sarkar Raj',), ('Kartoos',), ('Sarfarosh',), ('Blue',), ('Samsara',), ('Tezz',), ('Rock On 2',), ('Salaam-E-Ishq',), ('Kismat Konnection',), ('1921',), ('Bairavaa',), (\"Midnight's Children\",), ('Maachis',), ('Chalo Dilli',), ('Welcome to Sajjanpur',), ('Nishabd',), ('Shree',), ('Deewangee',), ('Insan',), ('Kabhi Haan Kabhi Naa',), ('Yevadu',), ('Veergati',), (\"'D'\",), ('LoveShhuda',), ('1920: Evil Returns',), ('Manjhi: The Mountain Man',), ('Hum Aapke Hain Koun...!',), ('Siddharth',), ('Union Leader',), ('Rang Rasiya',), ('Main Aur Charles',), ('16 December',), ('Qurbani',), ('Sonali Cable',), ('Rakhwala',), ('Anupama',), ('De surprise',), ('Zinda',), ('Ramaiya Vastavaiya',), ('Milan',), ('Abhimaan',), ('Geet',), ('Zanjeer',), ('Aunty No. 1',), ('Gopi Kishan',), ('Fun2shh... Dudes in the 10th Century',), ('Hum Aapke Dil Mein Rehte Hain',), ('Zamane Se Kya Darna',), ('Raat',), ('Hatya',), ('Sangram',), ('Kasam',), ('Teri Meri Kahaani',), ('Bombay',), ('Dhamaal',), ('Dishkiyaoon',), ('MP3: Mera Pehla Pehla Pyaar',), ('Ready',), ('Hulchul',), ('Farishtay',), ('Kati Patang',), ('Tum Mere Ho',), ('I Am Kalam',), ('Baabul',), ('Kidnap',), ('Junooniyat',), ('Dulhe Raja',), ('Amaanat',), ('Maine Dil Tujhko Diya',), ('Aisha',), ('Inteha',), ('Firaaq',), ('Army',), ('Humshakals',), ('Hum Tum',), ('Aandhi',), ('Dus',), ('Buddha in a Traffic Jam',), ('Drona',), ('Gangster',), ('Border',), ('Gumrah',), ('Love Story 2050',), ('Baankey Ki Crazy Baraat',), ('Zameen',), ('Sadak',), ('Main Khiladi Tu Anari',), ('Dhoondte Reh Jaoge',), ('Eena Meena Deeka',), ('Love Story',), ('Ankhen',), ('Karz',), ('Dora',), ('Hum Saath-Saath Hain: We Stand United',), ('Ek Phool Do Mali',), ('Little Terrors',), ('Jeena Sirf Merre Liye',), ('Kaashi in Search of Ganga',), ('Pyaasa',), ('R... Rajkumar',), ('Sanam Teri Kasam',), ('Izzat Ki Roti',), ('Traffic',), ('Meri Biwi Ka Jawab Nahin',), ('Luck by Chance',), ('Waiting',), ('Shalimar',), ('Jab Jab Phool Khile',), ('Aamir',), ('Deewaar',), ('Tere Naam',), ('Jugnu',), ('Matrubhoomi: A Nation Without Women',), ('Khatron Ke Khiladi',), ('Tum Mile',), ('Raju Ban Gaya Gentleman',), ('Ranna',), ('Singh Is Kinng',), ('Teraa Surroor',), ('Hamara Dil Aapke Paas Hai',), ('Imtihaan',), ('Meri Nimmo',), ('Rascals',), ('Aa Gale Lag Jaa',), ('Aalavandhan',), ('Kis Kisko Pyaar Karoon',), (\"Liar's Dice\",), ('Jayantabhai Ki Luv Story',), ('Khilona Bana Khalnayak',), ('Khoon Bhari Maang',), ('Ishaqzaade',), ('Khudgarz',), ('Gumnaam',), ('Break Ke Baad',), ('Bazaar',), ('Fukrey',), ('Samadhi',), ('Kurbaan',), ('Duplicate',), ('Thupparivaalan',), ('Aladin',), ('Ram Gopal Varma Ki Aag',), ('C.I.D.',), ('Umrao Jaan',), ('Six X',), ('Matru ki Bijlee ka Mandola',), ('Global Baba',), ('Dushman Duniya Ka',), ('Kurukshetra',), ('Dedh Ishqiya',), ('Fool N Final',), ('Kadhal Desam',), ('Shakti',), ('Filth and Wisdom',), ('Humraaz',), ('Bank Chor',), ('Dushman',), ('Main Krishna Hoon',), ('U Me Aur Hum',), ('Dil Toh Baccha Hai Ji',), ('Vidhaata',), ('Roti Kapada Aur Makaan',), ('Shola Aur Shabnam',), ('Rukh',), ('Purani Jeans',), ('Baazi',), ('Aaja Nachle',), ('Chaudhvin Ka Chand',), ('Chameli',), ('Aadhi Bhagavan',), ('Ananda Ashram',), ('Bhopal: A Prayer for Rain',), ('Dance Dance',), ('Anaarkali of Aarah',), ('Khamoshi: The Musical',), ('Bol',), ('Guide',), ('Ek Paheli Leela',), ('Children of War',), ('Karzzzz',), ('Don',), ('Gali Guleiyan',), ('Raavan',), ('Haré Rama Haré Krishna',), ('Girlfriend',), ('China Gate',), ('Chocolate: Deep Dark Secrets',), ('Dear Dad',), ('Dasvidaniya',), ('Sangharsh',), ('Pataal Bhairavi',), ('Jai Santoshi Maa',), ('Jis Desh Mein Ganga Rehta Hain',), ('Arth',), ('Aarakshan',), ('Shool',), ('Saat Uchakkey',), ('Parzania',), ('Khushi',), ('Naseeb',), ('Bhindi Baazaar',), ('Bulandi',), ('Shaadi Ke Side Effects',), ('Chor Machaye Shor',), ('Sarbjit',), ('Raja Hindustani',), ('Silsila',), ('Shabd',), ('Yodha',), (\"Jaan-E-Mann: Let's Fall in Love... Again\",), ('Phir Teri Kahani Yaad Aayee',), ('Blood Money',), ('Right Yaaa Wrong',), ('Waqt Hamara Hai',), ('Seeta Aur Geeta',), ('Dil Ne Jise Apna Kaha',), ('Kunwara',), ('The Silence',), ('Chameli Ki Shaadi',), ('Garam Masala',), ('Angoor',), ('Mahal',), ('Johny Mera Naam',), ('Page 3',), ('Tum Bin...: Love Will Find a Way',), ('Phir Bhi Dil Hai Hindustani',), ('Pusher',), ('3G - A Killer Connection',), ('Achanak',), ('Roop Ki Rani Choron Ka Raja',), ('Raaz',), ('Saathiya',), ('Bhabhi',), ('Maa',), ('Prem Deewane',), ('Saheb Biwi Aur Gangster',), ('Kalyug',), ('Awara Paagal Deewana',), ('Indu Sarkar',), ('Geraftaar',), ('Filmistaan',), ('Love',), ('Bekhudi',), ('Dalaal',), ('Ghayal Once Again',), ('Login',), ('Good Boy, Bad Boy',), ('Mohra',), ('Machine',), ('Rise of the Zombie',), ('Sargam',), ('Haqeeqat',), ('Gang',), ('Dil Dosti Etc',), ('Heer Ranjha',), ('Strangers',), ('Gundaraj',), ('Aan Milo Sajna',), ('Andaaz',), ('Tejasvini',), ('Stanley Ka Dabba',), ('Yaar Gaddar',), ('Style',), ('Dongri Ka Raja',), ('Sahasam',), ('Main Aurr Mrs Khanna',), ('LOC: Kargil',), ('Sanam Re',), ('Gyakusatsu kikan',), ('Majboor',), ('Major Saab',), ('Satte Pe Satta',), ('Khatta Meetha',), ('Ek Thi Daayan',), ('Yeh Saali Zindagi',), ('Lekar Hum Deewana Dil',), ('Mere Baap Pehle Aap',), ('Pyaar Tune Kya Kiya...',), ('Aastha: In the Prison of Spring',), ('Issaq',), ('Fareb',), ('Ek Chalis Ki Last Local',), ('Rowdy Rathore',), ('Iqbal',), ('Phas Gaye Re Obama',), ('Samraat',), ('Zamaana Deewana',), ('Ilaaka',), ('Dus Kahaniyaan',), ('Adhurs',), ('Housefull 2',), ('Earth',), ('Son of Sardaar',), ('Amar Akbar Anthony',), ('Satta',), ('Anita & Me',), ('Rann',), ('Yeh Lamhe Judaai Ke',), ('Kyaa Kool Hai Hum',), ('King Uncle',), ('Inteqam: The Perfect Game',), ('Jai Ho',), ('Begum Jaan',), ('Shirdi Ke Sai Baba',), ('International Khiladi',), ('Mahaan',), ('Ta Ra Rum Pum',), ('Henna',), ('New York',), ('Mere Yaar Ki Shaadi Hai',), ('Phool Aur Patthar',), (\"What's Your Raashee?\",), ('Yun Hota Toh Kya Hota',), ('Hawaa Hawaai',), ('Qahar',), ('Dushman',), ('Aarzoo',), ('London Dreams',), ('Deewana Mastana',), ('Footpath',), ('Khoj',), ('Aaj Ki Taaza Khabar',), ('Shirdi Sai Baba',), ('...Yahaan',), ('Oh Darling Yeh Hai India',), ('Jism',), ('Deewana',), ('Veer',), ('Golmaal: Fun Unlimited',), ('Ajab Prem Ki Ghazab Kahani',), ('Ghulam',), ('Kranti',), ('Super Nani',), ('Aadmi Khilona Hai',), ('31st October',), ('Dum Maaro Dum',), ('Irandam Ulagam',), ('Ghar',), ('Dabangg 2',), ('Duvvada Jagannadham',), ('Hungama',), ('Firangi',), ('Hum',), ('Hawa',), ('Jewel Thief',), ('Veerey Ki Wedding',), ('Ram Aur Shyam',), ('Chhota Bheem Himalayan Adventure',), ('Rakhta Charitra 2',), ('Chhoti Si Baat',), ('Inkaar',), ('Club 60',), ('Hulchul',), ('Ghazab',), ('Hum Kisi Se Kum Nahin',), ('Vijeta',), ('Koyla',), ('Anjaam',), ('Bhoothnath Returns',), ('Daai zek lou',), ('Jaal: The Trap',), ('Rajkumar',), ('Dil Tera Aashiq',), ('Ittefaq',), ('Bhupathi',), ('Kudrat',), ('Welcome Back',), ('Hawaizaada',), ('Mausam',), ('Meeruthiya Gangsters',), ('Main Tulsi Tere Aangan Ki',), ('Khhotte Sikkay',), ('Yeh Hai Bakrapur',), ('Shootout at Lokhandwala',), ('Laal Rang',), ('Deewane',), ('Ankur',), ('Do Anjaane',), ('Anjaan',), ('Ishq',), ('Aaj Ka Goonda Raaj',), ('Karz: The Burden of Truth',), ('Khajoor Pe Atke',), ('Jilla',), ('Tales of the KamaSutra 2: Monsoon',), ('Julie 2',), ('Bhoothnath',), ('Anokha Bandhan',), ('Souten',), ('Virasat',), ('Vettaiyaadu Vilaiyaadu',), ('Qayamat Se Qayamat Tak',), ('Sin-ui hansu',), ('Ishqiya',), ('Game',), ('Doosara Aadmi',), ('Ranchi Diaries',), ('London Paris New York',), ('Agyaat',), ('Namak Haraam',), ('Lal Patthar',), ('Hera Pheri',), ('Adventures of Tarzan',), ('Raincoat',), ('Chandra Mukhi',), ('Netaji Subhas Chandra Bose: The Forgotten Hero',), ('Izzatdaar',), ('Banarasi Babu',), ('Woh Lamhe',), ('13B: Fear Has a New Address',), ('Shree 420',), ('Kuch Naa Kaho',), ('Mounam Pesiyadhe',), ('Ijaazat',), ('Yeh Dil',), ('Dilwaala',), ('Mr Joe B. Carvalho',), ('Madhumati',), ('Journey Bombay to Goa: Laughter Unlimited',), ('Sri Siddhartha Gautama',), ('Teesri Manzil',), ('Devar',), ('Hazaaron Khwaishein Aisi',), ('Chance Pe Dance',), ('Aiyyaa',), ('Raag Desh',), ('Divya Shakti',), ('Mehboob Ki Mehndi',), ('Dil Kya Kare',), ('Dulha Mil Gaya',), ('Barsaat',), ('Waqt',), ('Wajood',), ('Kalicharan',), ('Wajah Tum Ho',), ('Dhadkan',), ('Hadh Kar Di Aapne',), ('Hum Tum Aur Ghost',), ('Teen Aur Aadha',), ('Khelein Hum Jee Jaan Sey',), ('Prem Pratigyaa',), ('The Deceivers',), ('Kaalo',), ('Refugee',), ('Samay: When Time Strikes',), ('Jhootha Hi Sahi',), ('Woh Kaun Thi?',), ('Tu Chor Main Sipahi',), ('Yamla Pagla Deewana',), ('City of Life',), ('Hey Bro',), ('Saala Khadoos',), ('Gadar: Ek Prem Katha',), ('Golmaal 3',), ('Khuda Gawah',), ('Nanak Shah Fakir',), ('Ungli',), ('Police Force: An Inside Story',), ('Pretty Polly',), ('I Am',), ('Tumsa Nahin Dekha',), ('Jangal Mein Mangal',), ('Jagte Raho',), ('Bombay to Goa',), ('Superman',), ('Gunga Jumna',), ('Vaada Raha... I Promise',), ('The Dark Side of Life: Mumbai City',), ('The Legend of Bhagat Singh',), ('Miss Lovely',), ('Judwaa',), ('Daava',), ('Gippi',), ('Total Siyapaa',), ('99',), ('Rakhta Charitra',), ('Alag: He Is Different.... He Is Alone...',), ('Saaya',), ('Kal Kissne Dekha',), ('Tamanna',), ('Dum',), ('Mickey Virus',), ('Samrat & Co.',), ('All Is Well',), ('Mela',), ('Khiladi 786',), ('Gauru: Journey of Courage',), ('I, Me aur Main',), ('Ek Ladka Ek Ladki',), ('Deadline: Sirf 24 Ghante',), ('Sahib Bibi Aur Ghulam',), ('Veerana',), ('Jazbaa',), ('Azhar',), ('Tauba Tauba',), ('Krodh',), ('Chori Chori',), ('Keemat: They Are Back',), ('Yalgaar',), ('Zubaan',), ('Baarish Aur Chowmein',), ('Golmaal Returns',), ('Chalo Ishq Ladaaye',), ('Pehchaan',), ('Do Raaste',), ('Avtaar',), ('Taqdeerwala',), ('Mera Saaya',), ('Phir Milenge',), ('1971',), ('Jai Vikraanta',), ('Khakee',), ('Autohead',), ('Jattu Engineer',), ('Saagar',), ('Dirty Politics',), ('God Tussi Great Ho',), ('Sanam',), ('Dil Vil Pyar Vyar',), ('Golimar',), ('Chalti Ka Naam Gaadi',), ('Shagird',), ('Boss',), ('Hero No. 1',), ('Partner',), ('Mission Istaanbul: Darr Ke Aagey Jeet Hai!',), ('Budhia Singh: Born to Run',), ('Khamoshiyan',), ('Aksar 2',), ('Murder 3',), ('Parinda',), ('Karthik Calling Karthik',), ('Xcuse Me',), ('Kashmir Ki Kali',), ('Khandan',), ('Paap Ki Duniya',), ('Shor in the City',), ('Fida',), (\"Trip to Bhangarh: Asia's Most Haunted Place\",), ('Lafangey Parindey',), ('Kisaan',), ('Insaniyat',), ('Bangistan',), ('Ek Doctor Ki Maut',), ('Cash',), ('Khuddar',), ('Not a Love Story',), ('Angaaray',), ('Sanam Teri Kasam',), ('Fugly',), ('Jodi',), ('Yeh Vaada Raha',), ('Bombay Boys',), ('Bhagam Bhag',), ('Eklavya',), ('Main Prem Ki Diwani Hoon',), (\"It's Entertainment\",), ('Shuddh Desi Romance',), ('Jackpot',), ('Tadipaar',), ('Jurm',), ('Mausam',), ('Ishq Forever',), ('Dacait',), ('100 Days',), ('Snegithiye',), ('David',), ('Kasoor',), ('Bees Saal Baad',), ('Always Kabhi Kabhi',), ('Suraj',), ('Running Shaadi',), ('Phata Poster Nikhla Hero',), ('Mr. X',), ('Chillar Party',), ('Mere Dad Ki Maruti',), ('Apaharan',), ('Bhoot Returns',), ('The Train: Some Lines Should Never Be Crossed...',), ('Athidhi',), ('Ravan Raaj: A True Story',), ('Jodi No.1',), ('Ghayal',), ('Yamla Pagla Deewana 2',), ('Amazon Obhijaan',), ('Johnny',), ('Rudraksh',), ('Patthar Ke Phool',), ('The Killer',), ('Ragini MMS',), ('Tashan',), ('Gangaajal',), ('Khubsoorat',), ('Aks',), ('Jaal',), ('Ek Ajnabee',), ('Milenge Milenge',), ('Raaj Tilak',), ('Duniya',), ('Guddu',), ('Sultanat',), ('Sunday',), ('Shiva',), ('Pratiggya',), ('Khamosh',), ('Money Hai Toh Honey Hai',), ('Hattrick',), ('Jannat 2',), ('Puli',), ('Chachi 420',), ('Ram Jaane',), ('Veeram',), ('Kyun! Ho Gaya Na...',), ('Vikram',), ('Corporate',), ('Dil Aashna Hai ...The Heart Knows',), ('Gunaah',), ('Pyari Behna',), ('Mr. and Mrs. Iyer',), ('Phörpa',), ('Kabul Express',), ('Jeete Hain Shaan Se',), ('Brahmachari',), ('Gulaab Gang',), ('Guddi',), ('Jaanwar',), ('Raaz Reboot',), ('Tera Intezaar',), ('Arjun: The Warrior Prince',), ('Yeh Raaste Hain Pyaar Ke',), ('Rahul',), ('Boot Polish',), ('Wedding Anniversary',), ('Shatranj',), ('Shortkut - The Con Is On',), ('Days of Tafree',), ('Bhouri',), ('Mann',), ('Julie',), ('Santa Banta Pvt Ltd',), ('Rudaali',), ('Madurey',), ('Mumbai Delhi Mumbai',), ('Pehla Nasha',), ('Jolly LLB',), ('Maine Pyaar Kyun Kiya',), ('Patiala House',), ('Taal',), ('Jhankaar Beats',), ('Kaanchi',), ('Chal Mere Bhai',), ('Desh Premee',), ('Department',), ('Machhli Jal Ki Rani Hai',), ('Pinjar: Beyond Boundaries...',), ('Thanedaar',), ('Hide & Seek',), ('Kondaveeti Raja',), ('Zakhmi Dil',), ('7 Aum Arivu',), ('S/O Satyamurthy',), ('Ab Tumhare Hawale Watan Saathiyo',), ('Akele Hum Akele Tum',), ('Balu Mahi',), ('Jail',), ('Student No. 1',), ('Anji',), ('Shaapit: The Cursed',), ('Hawas',), ('Pyaar Koi Khel Nahin',), ('Kadhalar Dinam',), ('Mahaanta: The Film',), ('Zakhm',), ('36 China Town',), ('Amal',), ('Chandni Bar',), ('Janbaaz',), ('Ram Balram',), ('Chhota Chetan',), ('Amit Sahni Ki List',), ('Ankush',), ('Halla Bol',), ('Poster Boys',), ('Raju Chacha',), ('Masterji',), ('Farz',), ('Jungle',), ('Baazi',), ('I See You',), ('Shab',), ('Ferrari Ki Sawaari',), ('Anth',), ('Watan Ke Rakhwale',), ('Darna Mana Hai',), ('Do Knot Disturb',), ('Kareeb',), ('Raakh',), ('Naina',), ('Humse Badhkar Kaun: The Entertainer',), ('Platform',), ('Rishtey',), ('Tere Bin Laden: Dead Or Alive',), ('Parampara',), ('Luck',), ('Maya',), ('Chaalbaaz',), ('Tum Bin 2',), ('Yaariyan',), ('Baazi',), ('Makdee',), ('Chingaari',), ('Paap',), ('The Xpose',), ('Bheja Fry',), ('Taxi No. 9 2 11: Nau Do Gyarah',), ('Yeh Zindagi Ka Safar',), ('Dil Pardesi Ho Gayaa',), ('Darwaza',), ('Baton Baton Mein',), ('The Burning Train',), ('Bewafaa',), ('Dil Hai Ki Manta Nahin',), ('Soni',), ('Prem Rog',), ('Peepli Live',), ('Pyar Ka Mandir',), ('The Warrior',), ('Aatma',), ('Revolver Rani',), ('Ek Chhotisi Love Story',), ('Rajput',), ('Super Model',), ('Amrapali',), ('Aap Ki Kasam',), ('Disco Dancer',), ('Chaahat',), ('Kammatti Paadam',), ('Uvaa',), ('Mumbai 125 KM 3D',), ('Dhokha',), ('Teesra Kaun?',), ('Teree Sang: A Kidult Love Story',), ('Barsaat Ki Raat',), ('Yakeen',), ('Main, Meri Patni... Aur Woh!',), ('Karam',), ('Dillagi',), ('Indrajeet',), ('Dharmatma',), ('Mamta',), ('Dor',), ('Khilona',), ('Rough Book',), ('III Smoking Barrels',), ('Chatrapathi',), ('Dil Bole Hadippa!',), ('Jannat: In Search of Heaven...',), ('Apoorva Sagodharargal',), ('Krodhi',), ('Yeh Hai Jalwa',), ('Thammudu',), ('Jurm',), ('Hotel',), ('Desh Drohi',), ('Footsteps in the Dark',), ('Shastra',), ('The Shaukeens',), ('The Great New Wonderful',), ('Darna Zaroori Hai',), ('Dharti',), ('Dil Tera Diwana',), ('Mangalashtak Once More',), ('Mahakaal',), ('Hote Hote Pyar Hogaya',), ('Nanhe Jaisalmer: A Dream Come True',), ('Guddu Ki Gun',), ('Naam',), ('Baabarr',), ('Jawani Diwani',), ('Pitaah',), ('Naajayaz',), ('Prem Geet',), ('Darr @ the Mall',), ('Laali Ki Shaadi Mein Laaddoo Deewana',), ('Criminal',), ('Khoj',), ('Gemini',), ('Parvarish',), ('Gharwali Baharwali',), ('Hum Hain Rahi Pyar Ke',), ('Ajooba',), ('Sehar',), ('Gallit Gondhal, Dillit Mujra',), ('Naam Gum Jaayega',), ('Daayraa',), ('Bezubaan Ishq',), ('Daulat Ki Jung',), ('Zokkomon',), ('Isi Life Mein...!',), ('Amrit',), ('Gaddaar',), ('23rd March 1931: Shaheed',), ('Aagey Se Right',), ('Krishna Cottage',), ('Ram-Avtar',), ('Salaakhen',), ('Pyaar Ishq Aur Mohabbat',), ('Fight Club: Members Only',), ('Jodi Breakers',), ('Jis Desh Men Ganga Behti Hai',), ('Lakshmi',), ('Aa Gaya Hero',), ('Shaan',), ('Gori Tere Pyaar Mein!',), ('Veerappan',), ('Chakravyuh',), ('Dhaai Akshar Prem Ke',), ('Chori Chori',), ('Plan',), ('Periyanna',), ('Dil Hai Betaab',), ('Masoom',), ('Amanush',), ('Yeh Mohabbat Hai',), ('Arpan',), ('Subedar Joginder Singh',), ('Bhaag Johnny',), ('Heer Raanjha',), ('Fuddu',), ('Naya Daur',), ('Papa Kahte Hain',), ('Chhupa Rustam: A Musical Thriller',), ('Samarasimha Reddy',), ('Valley of Flowers',), ('Roti',), ('Vedi',), ('Khuda Kasam',), ('Mr. Azaad',), ('Jaanam Samjha Karo',), ('Satya 2',), ('Policegiri',), ('Awaara',), ('Dilwale',), ('Chaar Sahibzaade',), ('Raaz 3: The Third Dimension',), ('Raja Babu',), ('Zila Ghaziabad',), ('Yeshwant',), ('Aakrosh',), ('Joru Ka Ghulam',), ('Koshish',), ('Satyamev Jayate',), ('Pukar',), ('Hey Ram Hamne Gandhi Ko maar Diya',), ('Ashaant',), ('Sansar',), ('Once Upon a Time in Mumbai Dobaara!',), ('Sharaabi',), ('Anjali',), ('Youngistaan',), ('Phir Se...',), ('Aashiq',), ('The Pool',), ('Agni Sakshi',), ('Vaalee',), ('Khan kluay',), ('Apartment: Rent at Your Own Risk',), ('Thamizhan',), ('Aag',), ('Hello',), ('Coolie',), ('Dharam Kanta',), ('Ab Tak Chhappan 2',), ('Zindagi 50 50',), (\"Deewaar: Let's Bring Our Heroes Home\",), ('My Friend Ganesha',), ('I.D.',), ('Vansh',), ('Kohram',), ('Tezaab',), ('Maatr',), ('No Problem',), ('Aankhen',), ('Dil Hai Tumhaara',), ('Kyaa Super Kool Hain Hum',), ('Karamati Coat',), ('Naqaab',), ('Mumbai Se Aaya Mera Dost',), ('Maharathi',), ('Train Station',), ('Professor',), ('Roadside Romeo',), ('The Wishing Tree',), ('Gang of Ghosts',), ('Aa Dekhen Zara',), ('Singam 2',), ('LIE',), ('404: Error Not Found',), ('Tahkhana',), ('Masoom',), ('Aashiqui',), ('Waqt Ki Awaz',), ('The Last Lear',), ('Jajantaram Mamantaram',), ('Mirch Masala',), ('Saraswatichandra',), ('MSG: The Messenger of God',), ('Pyar Ka Devta',), ('Saaransh',), ('Ghulami',), ('Punjab 1984',), ('ROAR: Tigers of the Sundarbans',), ('Kabhie Kabhie',), ('Besharam',), ('Yuvvraaj',), ('Hanuman',), ('Maanikya',), ('6-5=2',), ('Crash Test Aglaé',), ('Ardh Satya',), ('Sheshnaag',), ('Kyo Kii... Main Jhuth Nahin Bolta',), ('Bas Ek Pal',), ('Guddu Rangeela',), ('Jia aur Jia',), ('Winner',), ('Lajja',), ('Zeher',), ('Hero',), ('Yeh Dillagi',), ('Bunty Aur Babli',), ('Hum Tum Pe Marte Hain',), ('Azaad',), ('Hawayein',), ('Agni Varsha',), ('Bemisal',), ('Jal',), ('The Domino Effect',), ('Aatank Hi Aatank',), ('Loafer',), ('Masti',), ('Zubeidaa',), ('Michael Madana Kamarajan',), ('Raja Jani',), ('Thikka',), ('Hero Hindustani',), ('Honeymoon Travels Pvt. Ltd.',), ('Shuddhi',), ('Yateem',), ('Barah Aana',), ('Pehla Pehla Pyar',), ('Dev',), ('Bahurani',), ('Hathyar: Face to Face with Reality',), ('Majunu',), ('Khiladi 420',), ('My Birthday Song',), ('Vaah! Life Ho Toh Aisi!',), ('Betaab',), ('Aaina',), ('Gair',), ('Mantostaan',), ('Monsoon',), ('Saath Saath',), ('Cape Karma',), ('Paying Guests',), ('1920',), ('Calendar Girls',), ('Musafir',), ('Hatya: The Murder',), ('Ishkq in Paris',), ('Mujhe Kucch Kehna Hai',), ('Ramana',), ('Do Gaz Zameen Ke Neeche',), ('Khel',), ('Haasil',), ('Lekin...',), ('Yaan',), ('8 x 10 Tasveer',), ('Zid',), ('Shakthi: The Power',), ('F.A.L.T.U',), ('Khiladi',), ('Kitne Door... Kitne Paas',), ('Tom, Dick, and Harry',), ('Sullan',), ('Shivamani',), ('Insaaf: The Justice',), ('Bajatey Raho',), ('Love in Tokyo',), ('Deedar',), ('Deedar',), ('Vaada',), ('Nazrana',), ('Madhoshi',), ('Vijay',), ('Agar Tum Na Hote',), ('Khud-Daar',), ('Shortcut Romeo',), ('Insaf Ka Tarazu',), ('Nastik',), ('Sandwich',), ('Muthu',), ('Double Dhamaal',), ('Deewane Huye Paagal',), ('Ab Tak Chhappan',), ('Mandi',), ('Khoya Khoya Chand',), ('Andaz',), ('Deewana Mujh Sa Nahin',), ('Mere Mehboob',), ('Woh 7 Din',), ('Ajanabee',), ('Red Swastik',), ('Inteqam',), ('Enemmy',), ('Garm Hava',), ('Dilliwaali Zaalim Girlfriend',), ('Dear Maya',), ('Hum Ho Gaye Aap Ke',), ('Ramji Londonwaley',), ('Dil Maange More!!!',), ('Om Jai Jagadish',), ('Tum Milo Toh Sahi',), ('Khatta Meetha',), ('Naaraaz',), ('My Friend Pinto',), ('Hijack',), ('China Town',), ('Naksha',), ('Damini',), ('Rakshak',), ('Waqt: The Race Against Time',), ('Bhoot',), ('Dil',), ('Raaz: The Mystery Continues',), ('Kyon Ki...',), ('Get Educated: Paathshaala',), ('Tridev',), ('Saajan Ka Ghar',), ('Bajrangbali',), ('Barkhaa',), ('Fox',), ('Dharam Sankat Mein',), ('Vishwatma',), ('One 2 Ka 4',), ('Purana Mandir',), ('Dev Bhoomi',), ('Pardesi Babu',), ('Charas',), ('Shoot on Sight',), ('Aaghaaz',), ('Aap Aye Bahaar Ayee',), ('Sindoor',), ('Wah Taj',), ('Pinjra',), ('Cheetah',), ('Road',), ('Afsana Pyar Ka',), ('Janasheen',), ('Auzaar',), ('Aap Ki Khatir',), ('Jhoom Barabar Jhoom',), ('Bobby',), ('Coolie No. 1',), ('Taarzan: The Wonder Car',), ('Laaga Chunari Mein Daag: Journey of a Woman',), ('Saudagar',), ('Pyaar Kiya To Darna Kya',), ('Shakespeare-Wallah',), ('Phoonk',), ('Andaz',), ('Aashayein',), ('Lootere',), ('Kamaal Dhamaal Malamaal',), ('Priyamanavale',), ('MSG the Warrior: Lion Heart',), ('Kucch To Hai',), ('Nishant',), ('Chemmeen',), ('Blackmail',), ('Manoranjan',), ('Swayamvar',), ('Neel Kamal',), ('Khamoshi',), ('Kal Aaj Aur Kal',), ('Aa Gale Lag Jaa',), ('Chashme Buddoor',), ('Alam Ara',), ('Taj Mahal: An Eternal Love Story',), ('Parwana',), ('Humko Deewana Kar Gaye',), ('Parineeta',), ('Yes Boss',), ('Nagin',), ('Poorna',), ('Not Today',), ('Bobby Jasoos',), ('Nandha',), ('Jeena Isi Ka Naam Hai',), ('Lahoo Ke Do Rang',), ('Samundar',), ('Sparsh',), ('Love Ke Liye Kuch Bhi Karega',), ('Kabuliwala',), ('Tere Ghar Ke Samne',), ('Sameer',), ('Patthar Ke Sanam',), ('Ajnabee',), ('Bbuddah... Hoga Terra Baap',), ('Zanjeer',), ('Traffic Signal',), ('T for Taj Mahal',), ('Y.M.I. Yeh Mera India',), ('Dharam Karam',), ('The Violin Player',), ('Rokkk',), ('Purab Aur Pachhim',), ('Mr Ya Miss',), ('Main Aisa Hi Hoon',), ('Mujhse Fraaandship Karoge',), ('Kya Kehna',), ('Gupt: The Hidden Truth',), ('Alibaba Aur 40 Chor',), ('Khoon Pasina',), ('Dahan',), ('Teesri Kasam',), ('Talash',), ('Kalaakaar',), ('Saare Jahaan Se Mehnga...',), ('Ek Tha Raja',), ('Kishen Kanhaiya',), ('Hyderabad Nawabs',), ('Ram Lakhan',), ('Deha',), ('Pukar',), ('Devdas',), ('Tevar',), ('Mere Jeevan Saathi',), ('Sardar',), ('Chor Machaaye Shor',), ('Jab Pyar Kisise Hota Hai',), ('Gardish',), ('Sniff!!!',), ('No Entry',), ('Atithi Tum Kab Jaoge?',), ('Banjo',), ('Jung',), ('Prince',), ('Kadhal Sadugudu',), ('Jheel Ke Us Paar',), ('Naya Zamana',), ('Desi Kattey',), ('Lamhaa: The Untold Story of Kashmir',), ('Hum Hain Bemisaal',), ('Takkar',), ('Yudh',), ('Moh Maya Money',), ('Allari Pidugu',), ('Benaam Badsha',), ('Sur: The Melody of Life',), ('Singh Saab the Great',), ('Daawat-e-Ishq',), ('Aradhana',), ('Ram Teri Ganga Maili',), ('Aakhir Kyon?',), ('Shagird',), ('Chura Liyaa Hai Tumne',), ('Bachchan',), ('Sir',), ('Red Rose',), ('Tarkieb',), ('Aah',), ('Sainikudu',), ('Hyderabad Blues',), ('Kalyug',), ('Abdullah',), ('Waisa Bhi Hota Hai Part II',), ('Pyar Ka Mausam',), ('Bewaffa Se Waffa',), ('Rama Rama Kya Hai Dramaaa',), ('Prince',), ('All the Best: Fun Begins',), ('Jaagruti',), ('Andaz',), ('I Love New Year',), ('Vishnuvardhana',), ('Phullu',), ('Krazzy 4',), ('PremGranth',), ('Masti Express',), ('Nirdosh',), ('Upkar',), ('Shor',), ('Haseena Maan Jaayegi',), ('Deewaanapan',), ('Saathi',), ('Punnagai Mannan',), ('Mutamestri',), ('Time Out',), ('Anurodh',), ('Officer',), ('Kaccha Limboo',), ('Chal Chala Chal',), ('Prem Pujari',), ('Junoon',), ('Daddy Cool: Join the Fun',), ('Champion',), ('Leader',), ('Anuraag',), ('Bittoo Boss',), ('Dangerous Ishhq',), ('100% Love',), ('Anari',), ('Chandni',), ('Nazar',), ('Naram Garam',), ('Kaajal',), ('Mr. White Mr. Black',), ('Kya Dilli Kya Lahore',), ('Krishna Aur Kans',), ('Gollu aur Pappu',), ('Lakeer - Forbidden Lines',), ('3 Deewarein',), ('Return of Jewel Thief',), ('Pyaar Ke Side Effects',), ('Black Mail',), ('Bawarchi',), ('Hogi Pyaar Ki Jeet',), ('Barsaat',), ('Daddy',), ('Dancer',), ('Gambler',), ('Rog',), ('Wedding Pullav',), ('Mithya',), ('Naukar Biwi Ka',), ('Vaastu Shastra',), ('One by Two',), ('Himmat',), ('Dost',), ('Holi',), ('Sharafat Gayi Tel Lene',), (\"My Wife's Murder\",), ('Dastak',), ('Commando',), ('Police Public',), ('Kempe Gowda',), ('Badhaai Ho Badhaai',), ('Qayamat: City Under Threat',), ('Viruddh... Family Comes First',), ('7 Hours to Go',), ('Deewana Main Deewana',), ('Khel',), ('Phoring',), ('Aasha',), ('Rajnigandha',), ('Suhaag',), ('Trimurti',), ('Muqaddar Ka Sikandar',), ('Biwi No. 1',), ('Aakhree Raasta',), ('Kisna: The Warrior Poet',), ('Tere Mere Sapne',), ('Ram Shastra',), ('Arjun',), ('Katha',), ('Koi Aap Sa: But Lovers Have to Be Friends',), ('Satyakam',), (\"Where's the Party Yaar?\",), ('Elaan',), ('Thoda Pyaar Thoda Magic',), ('Kranti',), ('Anwar',), ('Shahenshah',), ('The Don',), ('The Death Sentence: Mrityu Dand',), ('Gangotri',), ('Bumm Bumm Bole',), ('Gair Kaanooni',), ('The Train',), ('Aar-Paar',), ('Kyaa Dil Ne Kahaa',), ('Mumbai Salsa',), ('Kala Pani',), ('Navra Mazha Navsacha',), ('Koi Mere Dil Se Poochhe',), ('Chitchor',), ('Itlu Sravani Subramanyam',), ('Trinetra',), ('Dracula 2012',), ('Aryan',), ('Zor: Never Underestimate the Force',), ('Angaar',), ('Yeh Hai Mumbai Meri Jaan',), ('Gandhi, My Father',), ('Pratighaat',), ('Bandini',), ('The Hero: Love Story of a Spy',), ('Aan: Men at Work',), ('Dastak',), ('Mast',), ('Vadh',), ('Aadalat',), ('Ilzaam',), ('Rockford',), ('Aasoo Bane Angaarey',), ('Astitva',), ('Ghulam-E-Musthafa',), ('Ghar Ghar Ki Kahani',), ('Prem Aggan',), ('Hathyar',), ('Aag Ka Gola',), ('Bhrashtachar',), ('Prague',), ('Mumbai Mirror',), ('Kala Pani',), ('Mard',), ('Sirf Tum',), ('Aashik Aawara',), ('Na Tum Jaano Na Hum',), ('Haseena',), ('Island City',), (\"Mr. & Mrs. '55\",), ('Apnapan',), ('I Proud to Be an Indian',), ('Akayla',), ('Swarag Se Sunder',), ('Maska',), ('Howrah Bridge',), ('Aabra Ka Daabra',), ('Lateef',), ('Heartless',), ('Pairon Talle',), ('Gambler',), ('Delhi in a Day',), ('Well Done Abba!',), ('Antardwand',), ('Teen Devian',), ('Balika Badhu',), ('Haal-e-Dil',), ('Sulemani Keeda',), ('Khullam Khulla Pyaar Karen',), ('Silsiilay',), ('Taaqatwar',), ('Mirch',), ('Bade Miyan Chote Miyan',), ('Gangaa Jamunaa Saraswathi',), ('Acid Factory',), ('Aadupuliyattam',), ('Bedardi',), ('Rangrezz',), ('Dil Diya Hai',), ('Yehi Hai Zindagi',), ('Fareb',), ('Jawani Zindabad',), ('Kohinoor',), ('Chakra',), ('Jogi the King',), ('Chori Chori Chupke Chupke',), ('Garv: Pride and Honour',), ('Shatranj Ke Khilari',), ('The Great Gambler',), ('Dariya Dil',), ('Jalpari: The Desert Mermaid',), ('Dillagi',), ('Bambai Ka Babu',), ('Anari No. 1',), ('Darling',), ('Saat Hindustani',), ('Anarkali',), ('Mr. Bond',), ('Tamanchey: Pyar Mein Dil Pe Maar De Goli',), ('Ahista Ahista',), ('New Delhi',), ('Parwana',), ('Mili',), ('Silsila Hai Pyar Ka',), ('Aathi',), ('Gali Gali Chor Hai',), ('Jeet',), ('Koyelaanchal',), ('Dhol',), ('Dream Girl',), ('Mrs. Scooter',), ('Rajkumar',), ('Click',), ('Inquilaab',), ('Jo Bole So Nihaal',), ('Zulm Ki Hukumat',), ('Lanka',), ('Guru',), ('Kaun Kitney Paani Mein',), ('Amu',), ('Pattathu Yaanai',), ('Sachein',), ('Lucky: No Time for Love',), ('Ayee Milan Ki Bela',), ('Gurudev',), ('Aetbaar',), ('1982 - A Love Marriage',), ('Meridian Lines',), ('Armaan',), ('Hum Kaun Hai?',), ('Love Breakups Zindagi',), ('Ramaa: The Saviour',), ('Shikhar',), ('Warning',), ('Bhoot Unkle',), ('Suraj Ka Satvan Ghoda',), ('Kanoon',), ('Aag',), ('Daud: Fun on the Run',), ('Malamaal Weekly',), ('Andolan',), ('Mumbai Cutting',), ('Raja Bhaiya',), ('Aan',), ('Kissi Se Na Kehna',), ('Bollywood Queen',), ('Manthan',), ('Kohraa',), ('Awaargi',), ('Bawandar',), ('Kasme Vaade',), ('Tumsa Nahin Dekha',), ('Hind Ka Napak Ko Jawab',), ('Karle Pyaar Karle',), ('Shaukeen',), ('Apne',), ('Kshatriya',), ('Shaadi Karke Phas Gaya Yaar',), ('Benny and Babloo',), ('Muskurahat',), ('Sone Pe Suhaaga',), ('Kisse Pyaar Karoon?',), ('Gumrah',), ('Jhansi Ki Rani',), ('Baharen Phir Bhi Aayengi',), ('War Chod Na Yaar',), ('Lucky Kabootar',), ('Bollywood Hero',), ('Taxi Driver',), ('Apradhi Kaun?',), ('Justice Chaudhury',), ('Bade Dilwala',), ('Gora Aur Kala',), ('Testing Movie',), ('Market',), ('Hum Paanch',), ('Satyam Shivam Sundaram: Love Sublime',), ('Soldier',), ('Chacha Bhatija',), ('3 AM: A Paranormal Experience',), ('Jaadugar',), ('Sssshhh...',), ('Hulchul',), ('Jhuk Gaya Aasman',), ('Prem Shakti',), ('Maha-Sangram',), ('Rajjo',), ('Bhoot Bungla',), ('Insaaf: The Final Justice',), ('Ghar Ek Mandir',), ('Shaktiman',), ('Road to Sangam',), ('Pyaar Diwana Hota Hai',), ('Dil Ne Phir Yaad Kiya',), ('Baiju Bawra',), ('Sarhad Paar',), ('Hum Dono',), ('Kala Bazar',), ('Charlie Kay Chakkar Mein',), ('Aar Ya Paar',), ('Dhund',), ('Sar Ankhon Par',), ('Namak Halaal',), ('Dosti: Friends Forever',), ('Sixteen',), ('Trishul',), ('Awwal Number',), ('Nehlle Pe Dehlla',), ('Blue Oranges',), ('Seema',), ('Love Love Love',), ('Zulmi',), ('Ninne Pelladatha',), ('Kanyadaan',), ('Ab... Bas!',), ('Bomb the System',), ('Hum Dono',), ('Tumse Achha Kaun Hai',), ('Saawan... The Love Season',), ('Ek Vivaah... Aisa Bhi',), ('Safar',), ('Haseena Maan Jayegi',), ('My Dear Kuttichaathan',), ('Rikki-Tikki-Tavi',), ('Rakht',), ('Bal Bramhachari',), ('Prem Kahani',), ('Be-Imaan',), ('Kora Kagaz',), ('Asambhav',), ('Narasimhudu',), ('Aadmi',), ('Who',), ('Swami',), ('Anpadh',), ('Rajaji',), ('Chatur Singh Two Star',), ('Saansein: The Last Breath',), ('Red: The Dark Side',), ('Kerry on Kutton',), ('EMI: Liya Hai To Chukana Padega',), ('Leera the Soulmate',), ('Help',), ('Tutak Tutak Tutiya',), ('Saheb Biwi Aur Gangster Returns',), ('Ghatak: Lethal',), ('Tujhe Meri Kasam',), ('Aksar',), ('Run',), ('Chain Kulii Ki Main Kulii',), ('Creature',), ('Vardi',), ('Rush',), ('Angrakshak',), ('Mehbooba',), ('Ek Nazar',), ('Aaina',), ('Maqsad',), ('Yaraana',), ('Titoo MBA',), ('Chamku',), ('Dharm',), ('Do Bigha Zamin',), ('Aashiq Banaya Aapne: Love Takes Over',), ('Ramayana: The Epic',), ('Yaarana',), ('Yeh Majhdhaar',), ('Bhumika',), ('Barsaat Ki Ek Raat',), ('Elaan',), ('Aazaan',), ('Striker',), ('Azaad',), ('Memories in March',), ('Anjaneyulu',), ('Red Alert: The War Within',), ('Sangeet',), ('Mr. Singh/Mrs. Mehta',), ('Aflatoon',), ('Baaghi',), ('Road, Movie',), ('Rajdhani Express',), ('P Se PM Tak',), ('Kaun Sachcha Kaun Jhootha',), ('C Kkompany',), ('Shirin Farhad Ki Toh Nikal Padi',), ('Tirangaa',), ('Ishq Vishk',), ('Padosan',), ('Kahin Pyaar Na Ho Jaaye',), ('Charas: A Joint Effort',), (\"Hanuman Da' Damdaar\",), ('Tarazu',), ('Tawaif',), ('Papa the Great',), ('Filhaal...',), ('Aao Pyaar Karen',), ('Kash... Aap Hamare Hote',), ('Cheluvi',), ('City of Gold - Mumbai 1982: Ek Ankahee Kahani',), ('Gour Hari Dastaan: The Freedom File',), ('Asli-Naqli',), ('Haathkadi',), ('Mr. X in Bombay',), ('Inkaar',), ('Shikari',), ('Kuku Mathur Ki Jhand Ho Gayi',), ('Aatank',), ('Chenna Kesava Reddy',), ('My Name Is Anthony Gonsalves',), ('Gayab',), ('Dil Jo Bhi Kahey...',), ('Mumbai Express',), ('Judaai',), ('Sardaar Gabbar Singh',), ('The Long Duel',), ('Reshma Aur Shera',), ('Diwana',), ('Buddha Mil Gaya',), ('Superstar',), ('Nazar Ke Samne',), ('Zabardast',), ('Zameer',), ('O Teri',), ('Kshay',), ('Kasam Paida Karne Wale Ki',), ('Chargesheet',), ('James',), ('Crd',), ('Jeevan Ek Sanghursh',), ('Aap Kaa Surroor: The Moviee - The Real Luv Story',), ('Toonpur Ka Superrhero',), ('Dostana',), ('Railway Children',), ('Train to Pakistan',), ('Mehbooba',), ('Tera Mera Saath Rahen',), ('Jaal',), ('Meenaxi: Tale of 3 Cities',), ('Besharam',), ('Sahasa Veerudu Sagara Kanya',), ('Pyaasa Sawan',), ('Daag',), ('Aap Ke Deewane',), ('Ekka Raja Rani',), ('Raghu Romeo',), ('Ajab Gazabb Love',), ('Home Delivery: Aapko... Ghar Tak',), ('Doli Saja Ke Rakhna',), ('Phoonk 2',), ('Quick Gun Murugun: Misadventures of an Indian Cowboy',), ('Mrityudaata',), ('Tango Charlie',), ('Ek Rishtaa: The Bond of Love',), ('Maharana Pratap: The First Freedom Fighter',), ('Super',), ('Chaand Kaa Tukdaa',), ('Bangaram',), ('Paandav',), ('Shaadi No. 1',), ('Buddha Mar Gaya',), ('Love Shagun',), ('Rocky',), ('Anamika: The Untold Story',), ('Parichay',), ('Amar',), ('One Two Three',), ('Kaalia',), ('Tumko Na Bhool Paayenge',), ('A Scandall',), ('Mr. Natwarlal',), ('Half Ticket',), ('Dam999',), ('Ganga Ki Saugand',), ('Pyaar Mein Kabhi Kabhi...',), ('Shukriya: Till Death Do Us Apart',), ('Jaanisaar',), ('Kunwara Baap',), ('Pati Patni Aur Woh',), ('Sri Shirdi Saibaba Mahathyam',), ('Patita',), ('Teesri Aankh: The Hidden Camera',), ('Peddlers',), ('Piya Ka Ghar',), ('Gandhi to Hitler',), ('Varudu',), ('Shakalaka Boom Boom',), ('Sunny',), ('Kinara',), ('Dulaara',), ('Biwi Ho To Aisi',), ('Khiladiyon Ka Khiladi',), ('Gafla',), ('Yaadein',), ('Albert Pinto Ko Gussa Kyon Ata Hai',), ('Namkeen',), ('Thanga Magan',), ('3 Bachelors',), ('Baseraa',), ('I Am 24',), ('Speed',), ('Ishq Hai Tumse',), ('Jalwa',), ('Rang Birangi',), ('Daag: A Poem of Love',), ('Sandai Kozhi',), ('Bichhoo',), ('Kanthaswamy',), ('Kachche Dhaage',), ('Miss Tanakpur Haazir Ho',), ('My Bollywood Bride',), ('Marry Me - Aber bitte auf Indisch',), ('Ru-Ba-Ru',), ('Kismet Love Paisa Dilli',), ('Gehri Chaal',), ('Bhairava Dweepam',), ('Aakhri Khat',), ('Sanjog',), ('Aapko Pehle Bhi Kahin Dekha Hai',), ('Prem Nagar',), ('Bombai Ka Babu',), ('Chauranga',), ('Kaksparsh',), ('Isi Ka Naam Zindagi',), ('Bhagwaan Dada',), ('Qila',), ('Anamika',), ('Ladies Tailor',), ('Kahani Kismat Ki',), ('Ghar Ho To Aisa',), ('Kuch Kuch Locha Hai',), ('Suhaag',), ('Sooryavansham',), ('Kabzaa',), ('Gopi',), ('Narasimha Nayudu',), ('Dus Numbri',), ('Split Wide Open',), ('Gaja Gamini',), ('Chitkabrey',), ('Jeevan Mrityu',), ('Kajraare',), ('Jigariyaa',), ('Shaadi Se Pehle',), ('Anubhav',), ('Dhoom Dadakka',), ('Hameshaa',), ('Mirza Ghalib',), ('Hindustan Ki Kasam',), ('Baaghi: A Rebel for Love',), ('Yaadon Ki Baaraat',), (\"Crook: It's Good to Be Bad\",), ('Do Lafzon Ki Kahani',), ('Dr. Babasaheb Ambedkar',), ('Main Madhuri Dixit Banna Chahti Hoon!',), ('Kala Bazaar',), ('An Evening in Paris',), ('Gunaah',), ('Aavishkar',), ('Banaras',), ('Risk',), ('Kismat',), ('Fired',), ('Swami',), ('Khauff',), ('Boxer',), ('Shudra the Rising',), ('Aaj',), ('Tere Mere Phere',), ('Mr. Romeo',), ('Prem Qaidi',), ('Tathastu',), ('Thanks Maa',), ('Yeh Teraa Ghar Yeh Meraa Ghar',), ('Naughty @ 40',), ('15 Park Avenue',), ('Eeshwar',), ('Ek: The Power of One',), ('Hum Hai Raahi CAR Ke',), ('Jugnu',), ('Khushboo',), ('Delhi Safari',), ('Krantiveer',), ('Main Azaad Hoon',), ('Ishqedarriyaan',), ('Thunder in the East',), ('Ramchand Pakistani',), ('Thakshak',), ('Navrang',), ('Sunghursh',), ('Return of Hanuman',), ('Dil Ki Baazi',), ('Harud',), ('Khichdi: The Movie',), ('Chamatkar',), ('Mastram',), ('Kasak',), ('Teesri Aankh',), ('Maalamaal',), ('Jeene Ki Raah',), ('Black & White',), ('Agent Vinod',), ('Bhopal Express',), ('Hifazat',), ('Victory',), ('Immaan Dharam',), ('The Memsahib',), ('Albela',), ('Kevi Rite Jaish',), (\"Pappu Can't Dance Saala\",), ('Betaabi',), ('Mere Apne',), ('Sar Utha Ke Jiyo',), ('Dastaan',), ('State Rowdy',), ('Antarmahal: Views of the Inner Chamber',), ('Raaz',), ('Contract',), ('Anuranan',), ('Chaar Din Ki Chandni',), ('Jawani Diwani: A Youthful Joyride',), ('36 Chowringhee Lane',), ('Des Pardes',), ('Farz',), ('Saudagar',), ('Kaala Patthar',), ('Talaash: The Hunt Begins...',), ('Family: Ties of Blood',), ('Paroma',), ('Fun: Can Be Dangerous Sometimes',), ('Ashanti',), ('Jaan Se Pyaara',), ('Insaaf',), ('Mukhbiir',), ('Adi Shankaracharya',), ('Maximum',), ('Joshilaay',), ('Utopia',), ('Karz Chukana Hai',), ('Mixed Doubles',), ('Khoon Ka Karz',), ('Aao Wish Karein',), ('Raja Rani',), ('Ghost',), ('Bardaasht',), ('Mirza Juuliet',), ('Maazii',), ('Raja Ko Rani Se Pyar Ho Gaya',), ('Diary of a Butterfly',), ('Dus Tola',), ('Gumnaam: The Mystery',), ('Badrinath',), ('Khaleja',), ('Pyaar To Hona Hi Tha',), ('Milan',), ('Zameer',), ('Dhoop',), ('Noukadubi',), ('Kayda Kanoon',), ('Summer 2007',), ('Aakhri Adaalat',), ('Saanjh',), ('Bulundi',), ('Sorry Bhai!',), ('Shor Se Shuruaat',), ('The Forest',), ('Pyaasa',), ('De Taali',), ('Alif',), ('Tehzeeb',), ('Abhinetri',), ('Nishchaiy',), ('Baat Ban Jaye',), ('Aaye Din Bahar Ke',), ('Vishwanath',), ('Vivekananda',), ('Undertrial',), ('Itni Si Baat',), ('Allah Ke Banday',), ('Wajahh: A Reason to Kill',), ('Wafaa',), ('Khel Khel Mein',), ('Khwahish',), ('Shapath',), ('Mere Jeevan Saathi',), ('Meri Jung',), ('Dil Kabaddi',), ('Babloo Happy Hai',), ('Bollywood Diaries',), ('Albela',), ('Jai Kishen',), ('Bade Dil Wala',), ('Do Aur Do Paanch',), ('Fatso!',), ('Kehtaa Hai Dil Baar Baar',), ('Sant Tukaram',), ('Dil Diya Dard Liya',), ('Bairaag',), ('Dil Ek Mandir',), ('Shikar',), ('Aulad',), ('Apradh',), ('Bhavnao Ko Samjho',), ('Paayum Puli',), ('The Lookalike',), ('Saugandh',), ('Karma',), ('Tere Naal Love Ho Gaya',), ('Chaar Sahibzaade 2: Rise of Banda Singh Bahadur',), ('Sujata',), ('MSG 2 the Messenger',), ('Apne Paraye',), ('Droh Kaal',), ('Jamai Raja',), ('Ek Khiladi Ek Haseena',), ('Mazdoor',), ('1:13:7 Ek Tera Saath',), (\"'Kaash'\",), ('Clerk',), ('Daman: A Victim of Marital Violence',), ('Phool',), ('Sharmeelee',), ('Umbartha',), ('Tum: A Dangerous Obsession',), ('Sahibaan',), ('Aadmi Aur Insaan',), ('Ek Hi Bhool',), ('Pratibandh',), ('Vishwasghaat',), ('Haunted - 3D',), ('Jaani Dushman: Ek Anokhi Kahani',), ('...Aur Pyaar Ho Gaya',), ('Aap Mujhe Achche Lagne Lage',), ('Sanam Bewafa',), ('Maseeha',), ('Soch',), ('Pyar Ki Kahani',), ('Shorts',), ('Dhool Ka Phool',), ('Chhote Sarkar',), ('Kushti',), ('Jaag Utha Insan',), ('Do Qaidi',), ('Gharana',), ('The Good Road',), ('Saas Bahu Aur Sensex',), ('Hum Naujawan',), ('Bheja Fry 2',), ('Do Badan',), ('Paying Guest',), ('Zalzala',), ('Beta',), ('Itihaas',), ('Jaisi Karni Waisi Bharni',), ('Andhaa Kaanoon',), ('Seetharamula Kalyanam Lankalo',), ('Phir Wohi Raat',), ('Anokhi Raat',), ('From Sydney with Love',), ('2001: Do Hazaar Ek',), ('Guru',), ('Karma, Confessions and Holi',), ('Just Married: Marriage Was Only the Beginning!',), ('Ekkees Toppon Ki Salaami',), ('Sangdil',), ('Om Dar-B-Dar',), ('Chhalia',), ('Dhaal: The Battle of Law Against Law',), ('Haath Ki Safai',), ('Lalkar The Challenge',), ('Kanoon Apna Apna',), ('Shaadi Teri Bajayenge Hum Band',), ('Phool Aur Kaante',), ('Aa Ab Laut Chalen',), ('Diljale',), ('Batwara',), ('Arjun',), ('Krishna',), ('Alag Alag',), ('Prithvi',), ('Insaniyat',), ('Listen... Amaya',), ('Qatl',), ('Dunno Y Na Jaane Kyun...',), ('Ishq Click',), ('French Immersion',), ('Hazaar Chaurasi Ki Maa',), ('Panchakshari',), ('Gattu',), ('Khilaaf',), ('Lal Salaam',), ('Return to Rajapur',), ('Shabri',), ('Khamoshh... Khauff Ki Raat',), ('Loot',), ('Sharabi',), ('Jhumroo',), ('Hariyali Aur Rasta',), ('Ek Haseena Thi Ek Deewana Tha',), ('Chalk N Duster',), ('Dharam Veer',), ('Hello Brother',), ('Jaani Dushman',), ('Abodh',), ('Pyar Kiye Jaa',), ('Vijeta',), ('Nenunnanu',), ('Hanste Zakhm',), ('Dhill',), ('Papi Gudia',), ('Premante Idera',), ('Nagarahavu',), ('?: A Question Mark',), ('Neecha Nagar',), ('Love 86',), ('Funtoosh',), ('Will You Marry Me',), ('Rebellious Flower',), ('Patang',), ('Nauker',), ('Aarya',), ('Baap Numbri Beta Dus Numbri',), ('Amar Prem',), ('Indian',), ('Hu Tu Tu',), ('Tell Me O Kkhuda',), ('Solva Saal',), ('Sikandar',), ('Anita',), ('Zamaane Ko Dikhana Hai',), ('The Blueberry Hunt',), ('Accident on Hill Road',), ('Cover Story',), ('Namo Venkatesha',), ('Michael',), ('In Custody',), ('Baharon Ke Sapne',), ('Main Aur Mr. Riight',), ('Aamdani Atthanni Kharcha Rupaiya',), ('Loha',), ('Junoon',), ('Jab Pyaar Kisise Hota Hai',), ('Big Brother',), ('Laadla',), ('Santosham',), ('Jimmy',), ('Dil Hi To Hai',), ('Kaafila',), ('Padmashree Laloo Prasad Yadav',), ('Dobara',), ('Saatwan Aasman',), ('Second Hand Husband',), ('Aanch',), ('Kya Love Story Hai',), ('Yahudi',), ('Baaz',), ('A Flat',), ('Mere Dost Picture Abhi Baaki Hai',), ('Manzil',), ('Mitr: My Friend',), ('Chaalis Chauraasi',), ('Udhaar Ki Zindagi',), ('Veera Madakari',), ('Pyare Mohan',), ('Kaamchor',), ('Priyasakhi',), ('Sapoot',), ('Aatish: Feel the Fire',), ('Dulhan Hum Le Jayenge',), ('Holiday',), ('Bhram: An Illusion',), ('Phhir',), ('Imaandaar',), ('Victoria No. 203',), ('Lakeer Ka Fakeer',), ('Andar Baahar',), ('Dil Deke Dekho',), ('Chhupa Rustam',), ('Mr Bhatti on Chutti',), ('Shiva',), ('Sangdil Sanam',), ('Bahu Begum',), ('Naughty Boy',), ('Love U... Mr. Kalakaar!',), ('Trikal Past, Present, Future',), ('Thavasi',), ('Kamla Ki Maut',), ('Godmother',), ('Bhuvan Shome',), ('Pyasa Shaitan',), ('Anmol',), ('Hum To Mohabbat Karega',), ('Darwaza Bandh Rakho',), ('Anuradha',), ('Film Star',), ('Madhosh',), ('Naach',), ('Daraar',), ('Kuch Tum Kaho Kuch Hum Kahein',), ('Bandhan',), ('Heroes',), ('Dulha Dulhan',), ('Ata Pata Lapatta',), ('Prem Patra',), ('Kaagaz Ke Fools',), ('Jashnn: The Music Within',), ('Kudrat',), ('Vakil Babu',), ('Border Hindustan Ka',), ('Sharafat',), ('Jaani Dost',), ('Direct Ishq',), ('Heera Panna',), ('Mod',), ('Morning Raga',), ('Ek Se Badhkar Ek',), ('Mahal',), ('Apna Asmaan',), ('Aashirwad',), ('Dil Tera Diwana',), ('Ziddi',), ('Purani Haveli',), ('Dhanwaan',), ('Har Dil Jo Pyar Karega...',), ('Saajan Chale Sasural',), ('Raqeeb',), ('Andhaa Yudh',), ('Gharaonda',), ('Aas Paas',), ('Swati',), ('Udaan',), ('Indrudu Chandrudu',), ('Baaz: A Bird in Danger',), ('Dil Hi To Hai',), ('Kuchh Meetha Ho Jaye',), ('Amar Deep',), ('Sailaab',), ('Rafoo Chakkar',), ('Anjaneya',), ('Lootmaar',), ('Thodisi Bewafaii',), ('Ittefaq',), ('Chori Mera Kaam',), ('Kali Salwaar',), ('Surakshaa',), ('Yea Toh Two Much Ho Gayaa',), ('Ek Chadar Maili Si',), ('Bumboo',), ('Gehrayee',), ('Maine Gandhi Ko Nahin Mara',), ('Calcutta Mail',), ('Karobaar: The Business of Love',), ('Chitralekha',), ('Rabba Main Kya Karoon',), ('The President Is Coming',), ('Shaheed',), ('Sanjog',), ('Vaishali',), ('Aatma',), ('Jurmana',), ('What the Fish',), ('Anthony Kaun Hai?',), ('Mere Hamdam Mere Dost',), ('Karma: Crime. Passion. Reincarnation',), ('Oh, My God!!',), ('Saaheb',), ('Bombay to Bangkok',), ('A New Love Ishtory',), ('Paar',), (\"Everybody Says I'm Fine!\",), ('Laawaris',), ('Mr. & Mrs. Khiladi',), ('The Perfect Girl',), ('Himalay Ki Godmein',), ('Phir Wohi Dil Laya Hoon',), ('Waah! Tera Kya Kehna',), ('Samba',), ('Nayee Padosan',), ('Griha Pravesh',), ('Katilon Ke Kaatil',), ('Salim Langde Pe Mat Ro',), ('M.A.D: Mad About Dance',), ('Na Ghar Ke Na Ghaat Ke',), ('Tera Jadoo Chal Gayaa',), ('Zindagi Ek Juaa',), ('Censor',), ('Dil Apna Aur Preet Parai',), ('Sneham Kosam',), ('Sons of Ram',), ('The Film',), ('Alaap',), ('Aya Sawan Jhoom Ke',), ('Pran Jaaye Par Shaan Na Jaaye',), (\"Joggers' Park\",), ('Hello Darling',), ('Madhosh',), ('Manasarovar',), ('Kaamannana Makkalu',), ('Baat Ek Raat Ki',), ('Peechha Karro',), ('Caravan',), ('Utsav',), ('Khoobsurat',), ('Dayavan',), ('Lahore',), ('Benaam',), ('Bandook',), ('Faraar',), ('Professor Pyarelal',), ('Geet Gaaya Pattharon Ne',), ('Anubhav',), ('Yakeen',), ('Dashavatar',), ('Manchali',), ('Ghaath',), ('Gandhinagar 2nd Street',), ('Yeh Jo Mohabbat Hai',), ('Man Pasand',), ('Kuch Khatti Kuch Meethi',), ('Marhi Da Deeva',), ('Zahreelay',), ('Aalwar',), ('Bombay Summer',), ('Madam X',), ('Sona Spa',), ('Dharmputra',), ('Do Ankhen Barah Haath',), ('Virodhi',), ('Arzoo',), ('Nadiya Ke Paar',), ('Baaraat Company',), ('Nau Do Gyarah',), ('Chicken Tikka Masala',), ('Cooking with Stella',), ('Hum Tum Shabana',), ('Munimji',), ('Aloo Chaat',), ('Aitbaar',), ('Sooper Se Ooper',), ('Gaman',), ('Halo',), ('Bullet',), ('Bezawada',), ('Mela',), ('Luv U Alia',), ('Lucky',), ('Tera Kya Hoga Johnny',), ('Fajr al islam',), ('Mrigayaa',), ('Soundtrack',), ('Manjunath',), ('Around the World',), ('Jai Veeru: Friends Forever',), ('Dil Ka Rishta',), ('Saathi',), ('Suryavanshi',), ('Hero',), ('Jhooth Bole Kauwa Kaate',), ('Sabse Bada Khiladi',), ('Bhai',), ('Villu',), ('Khal-Naaikaa',), ('Khela',), ('Jaane Hoga Kya',), ('Pardesi',), ('Daas',), ('Thulasi',), ('Warrant',), ('Maanthrikam',), ('Ishq Ishq Ishq',), ('Paranthe Wali Gali',), ('Chehraa',), ('Apna Desh',), ('The Angrez',), ('C.I.D.',), ('Raaste Kaa Patthar',), ('Sheesha',), ('When Kiran Met Karen',), ('Shararat',), ('Rok Sako To Rok Lo',), ('Parakh',), ('Sagina',), ('Indira Priyanka',), ('Aakrosh: Cyclone of Anger',), ('Tarana',), ('Kabhi Na Kabhi',), ('Main Hoon Khiladiyon Ka Khiladi',), ('Bachke Rehna Re Baba',), ('36 Ghante',), ('Mere Khwabon Mein Jo Aaye',), ('Hari Puttar: A Comedy of Terrors',), ('Naseeb',), ('Maya Bazaar',), ('Chhota Bheem and the Throne of Bali',), ('Maharaja',), ('Jallaad',), ('Dosti',), ('Hamraaz',), ('Motu Patlu: King of Kings',), ('Lal Baadshah',), ('Dharavi',), ('Arvind Desai Ki Ajeeb Dastaan',), ('Sadhu Aur Shaitaan',), ('Iti Mrinalini: An Unfinished Letter...',), ('Turning 30!!!',), ('Yuvaraju',), ('Agni Pankh',), ('Love in Nepal',), ('Sankat City',), ('Love Express',), ('Sahebzaade',), ('Ankahee',), ('The Waiting Room',), ('Taaqat',), ('Miley - Naa Miley - Hum',), ('Naaga',), ('Hello Hum Lallann Bol Rahe Hain',), ('Yeh Kya Ho Raha Hai?',), ('Party',), ('Escape from Taliban',), ('Himmatwala',), ('Haqeeqat',), ('Radha Ka Sangam',), ('10ml LOVE',), ('Pyaar Mein Twist',), ('Challo Driver',), ('Woodstock Villa',), ('Delhii Heights',), ('Shuruaat Ka Interval',), ('Annarth',), ('Maya',), ('Hero Hiralal',), ('Do Ladke Dono Kadke',), ('Pudhiya Geethai',), ('Mee Sindhutai Sapkal',), ('Tum Haseen Main Jawan',), ('Dhund: The Fog',), ('Lahu Ke Do Rang',), ('Life Goes On',), ('Ranga',), ('Banarasi Babu',), ('Kissaa Kursee Kaa',), ('Bubble Gum',), ('Abhimanyu',), ('Ranbanka',), ('Professor Ki Padosan',), ('Jugni',), ('Chor chor super chor',), ('Tere Pyaar Mein',), ('Humjoli',), ('Sainik',), ('Hukumat',), ('Shart: The Challenge',), ('Yeh Dooriyan',), ('Jhoothi',), ('Yugpurush: A Man Who Comes Just Once in a Way',), ('Massey Sahib',), ('Inaam Dus Hazaar',), ('Biwi O Biwi',), ('Rui Ka Bojh',), ('Himalay Putra',), ('Gopichand Jasoos',), ('Mr Prime Minister',), ('Out of Control',), ('Kiccha',), ('Joshila',), ('Hubballi',), ('Dhuan',), ('New Delhi Times',), ('Jaago',), ('7 1/2 Phere: More Than a Wedding',), ('Badlapur Boys',), ('The Bright Day',), ('Raja',), ('Prahaar: The Final Attack',), ('Mashaal',), ('Haathi Mere Saathi',), ('Loha',), ('Hum Kisise Kum Naheen',), ('Prem',), ('Game Over',), ('Chinar Daastaan-E-Ishq',), ('Utt Pataang',), ('Mammo',), ('Aadamkhor',), ('Aggar: Passion Betrayal Terror',), ('Amir Garib',), ('Mera Damad',), ('Ghashiram Kotwal',), ('Govindha Govindha',), ('Saat Rang Ke Sapne',), ('Rihaee',), ('Pagla Kahin Ka',), ('Oops!',), ('U R My Jaan',), ('Identity Card',), ('Bluff Master',), ('Mawaali',), ('Hulla',), ('Mahathma',), ('Rain: The Terror Within...',), (\"Say Salaam India: 'Let's Bring the Cup Home'\",), ('Joroo Ka Ghulam',), ('Hava Aney Dey',), ('Siskiyaan',), ('Khatarnaak',), ('Lok Parlok',), ('Ya Rab',), ('Varadanayaka',), ('Kitaab',), ('Mujhe Meri Biwi Se Bachaao',), ('Ehsaas: The Feeling',), ('Dil Pe Mat Le Yaar!!',), ('Kismet',), ('Overnight',), ('I Am Singh',), ('Aaj Ki Awaz',), ('Bhookailas',), ('Mass',), ('Sohni Mahiwal',), ('Kya Yehi Pyaar Hai',), ('Tohfa',), ('Badal',), ('Veera Parampare',), ('Naukri',), ('Ghar Ka Chiraag',), ('Aamne - Saamne',), ('Vetri Vizha',), ('Jagriti',), ('Moksha: Salvation',), ('Hyderabad Blues 2',), ('Rules: Pyaar Ka Superhit Formula',), ('Bandhe Haath',), ('Pratikar',), ('Saraswathi Sabatham',), ('Kaafiron Ki Namaaz',), ('Encounter: The Killing',), ('Faryad moorcheha',), ('Oass',), ('Taqdeer',), ('Mr. Bechara',), ('Love Khichdi',), ('Surakksha',), ('Wan Pipel',), ('Life Ki Toh Lag Gayi',), ('Pehchaan: The Face of Truth',), ('Raat Gayi, Baat Gayi?',), ('Souryam',), ('Bas Itna Sa Khwaab Hai...',), ('Team: The Force',), ('Achhut Kanya',), ('Ishk Actually',), ('Partha',), ('Bindhaast',), ('Daisy',), ('Bombay 405 Miles',), ('Mumbai Cha Raja',), ('Beti No. 1',), ('Dharm Adhikari',), ('Jhanak Jhanak Payal Baaje',), ('Halla Gulla',), ('Fruit & Nut',), ('Janwar',), ('Kadamban',), ('Desamuduru',), ('Rang',), ('Akhiyon Se Goli Maare',), ('Kroadh',), ('Love in Simla',), ('Kab? Kyoon? Aur Kahan?',), ('Mohan Joshi Hazir Ho!',), ('Sheen',), ('Kasauti',), ('Gauri: The Unborn',), ('Gladiatorerna',), ('Garam Masala',), ('Teen Thay Bhai',), ('Mrugaraaju',), ('Aakhari Decision',), ('Fear',), ('Frozen',), ('27 Down',), ('Laawaris',), ('Devdas',), ('Swami Dada',), ('Phool Aur Angaar',), ('Vinashak - Destroyer',), ('Waaris',), ('Toofan',), ('Hello Mumbai: Salaam Mumbai',), ('Apna Sapna Money Money',), ('Hum Hain Kamaal Ke',), ('The Hangman',), ('Chala Mussaddi - Office Office',), ('Duet',), ('Meera',), ('Disha',), ('Bhageeratha',), ('Cycle Kick',), ('Andhrudu',), ('The 20 Questions Murder Mystery',), ('Three: Love, Lies, Betrayal',), ('Nalla',), ('Drohi',), ('Aadmi Ki Aurat Aur Anya Kahaniya',), ('Allari Ramudu',), ('Hastey Hastey Follow Your Heart',), ('Ankahee',), ('Bandhan',), ('The Gentleman',), ('Hollywood',), ('Luv U Soniyo',), ('Musafir',), ('Yagnam',), ('Is Raat Ki Subah Nahin',), ('Paromitar Ek Din',), ('Chota Jadugar',), ('Stumped',), ('Missing on a Weekend',), ('Detective Naani',), ('Khoya',), ('Rowdy Inspector',), ('Yaara Silly Silly',), ('Humko Tumse Pyaar Hai',), ('Haan Maine Bhi Pyaar Kiya',), ('Bin Bulaye Baraati',), ('Bees Saal Baad',), ('Aghaat',), ('The Wish Fish',), ('Chhal',), ('Saaz',), ('Sharada',), ('Pankh',), ('Samudram',), ('Toh Baat Pakki!',), ('Zindaggi Rocks',), ('Victoria No. 203: Diamonds Are Forever',), ('Mai',), ('Daal Mein Kuch Kaala Hai',), ('The Rally',), ('Coffee Bloom',), ('Naya Din Nai Raat',), ('Naukari',), ('Once Upon a Time in Bihar',), ('House No. 44',), (\"Vaibhav Sethia: Don't\",), ('Adharm',), ('Jeevan Dhaara',), ('Showbiz',), ('Andarivaadu',), ('Aadmi',), ('Sadda Adda',), ('Jaan Tere Naam',), ('Loafer',), ('Jigar',), ('Antarnaad',), ('Pazhani',), ('Kis Kis Ki Kismat',), ('Sapnon Ka Saudagar',), ('Shaheed',), ('Khandhar',), ('Parineeta',), ('Chase',), ('Dahek: A Burning Passion',), ('Palkon Ki Chhaon Mein',), ('Sardari Begum',), ('New Delhi',), ('Gudgudee',), ('Kalaignan',), ('Mujhe Jeene Do',), ('Hotel Very Welcome',), ('Ab Ayega Mazaa',), ('Shadow',), ('Aashiq Hoon Baharon Ka',), ('A Film by Aravind',), ('Subhash Chandra Bose',), ('Achanak',), ('Jogan',), ('Duvidha',), ('Jaan',), ('Mera Gaon Mera Desh',), ('Marte Dam Tak',), ('Banjaran',), ('Nagina',), ('7 Welcome to London',), ('Yatra',), ('Inspector Balram',), ('Baat Bann Gayi',), ('Go',), ('Do Jasoos',), ('Drishti',), ('Vazhve Mayam',), ('Safari',), ('Chaahat Ek Nasha...',), ('Dadar Kirti',), ('Hari-Bhari',), ('Naseem',), ('Satyameva Jayate',), ('Dekh Tamasha Dekh',), ('Attahaasa',), ('Veettilekkulla Vazhi',), ('Chhailla Babu',), ('Balwaan',), ('Bandh Darwaza',), ('Vijaypath',), ('Mujrim',), ('Love You Hamesha',), ('Majhli Didi',), ('The Film Emotional Atyachar',), ('Chann Pardesee',), ('Alavuddinum Athbutha Vilakkum',), ('Bach ke Zara',), ('Raghavendra',), ('Ikke Pe Ikka',), ('Man On Mission Jaanbaaz',), ('Ek Hota Vidushak',), ('Gangoobai',), ('Anjaam',), ('Shriman Shrimati',), ('Maan Gaye Mughall-E-Azam',), ('Chudail Story',), ('Do Kaliyaan',), ('Yeh Dil Aashiqanaa',), ('Julie',), ('Sweetiee Weds NRI',), ('Sapne Saajan Ke',), ('Barefoot to Goa',), ('Jugaad',), ('Tarpan The Absolution',), ('Zara Si Zindagi',), ('Station',), ('Vakratunda Mahakaaya',), ('Chhodo Kal Ki Baatein',), ('Vihir',), ('Anjaane: The Unkown',), ('Via Darjeeling',), ('Monica',), ('Afsana Dilwalon Ka',), ('Khap',), ('Kudiyon Ka Hai Zamaana',), ('Mera Dil Leke Dekho',), ('Ugly Aur Pagli',), ('Thayillamal Nannilai',), (\"Jo Dooba So Paar: It's Love in Bihar!\",), ('Anand Math',), ('Grahan',), ('Genesis',), ('Ab Dilli Dur Nahin',), ('Jagir',), ('Bol Radha Bol',), ('Swarg',), ('Bunny',), ('Be-Careful',), ('Prem Kaa Game',), ('Dilwale:The Brave Heart',), ('Lezioni di volo',), ('Damul',), ('Sadiyaan: Boundaries Divide... Love Unites',), ('Supari',), ('Uttar Dakshin',), ('Radio: Love on Air',), ('Gospel Movie: Who Is My Lord',), ('Badmashiyaan',), ('Jigyaasa',), ('Oraalppokkam',), ('Life Mein Kabhie Kabhiee',), ('Chorus',), ('Zaalim',), ('Ghar Ghar Ki Kahani',), ('Tum Se Achcha Kaun Hai',), ('Maidan-E-Jung',), ('Budtameez',), ('Suno Sasurjee',), ('Paisa Vasool',), ('Pasand Apni Apni',), ('Soch Lo',), ('Prematho Raa',), ('The Train',), ('Sankham',), ('Subah Subah',), ('Hari Om',), ('Walkaway',), ('Veer Savarkar',), ('Chintu Ji',), ('Do Dooni Char',), ('Future to Bright Hai Ji',), ('Chandu',), ('Heeralal Pannalal',), ('Mohandas',), ('Awara Baap',), ('Sivappathikaaram',), ('Naughty Boy',), ('Veerabhadra',), ('Madly Bangali',), ('Muqabla',), ('Sachaa Jhutha',), ('Koi Mere Dil Mein Hai',), ('Jung',), ('Mohabbat',), ('Chhota Bheem and the Curse of Damyaan',), ('Noorie',), ('88 Antop Hill',), ('Shanti Kranti',), ('Eduruleni Manishi',), ('Curry Munchers',), ('Jaali Note',), ('Ek Din Achanak',), ('Goopi Gawaiya Bagha Bajaiya',), ('Thodasa Roomani Ho Jaayen',), ('Jo Hum Chahein',), (\"Shahrukh Bola 'Khoobsurat Hai Tu'... And She Believed in It\",), ('Chhodon Naa Yaar',), ('Bhavni Bhavai',), ('Kajarya',), ('Shaadi Ka Laddoo',), ('Thee',), ('God Only Knows!',), ('Nandhi',), ('Uski Roti',), ('Ravoyi Chandamama',), ('Pyar Jhukta Nahin',), ('Haatim Tai',), ('Raghuveer',), ('Naseeb Apna Apna',), ('50 Lakh',), ('Gooli',), ('Popcorn Khao! Mast Ho Jao',), ('Sau Jhooth Ek Sach',), ('Sanshodhan',), ('Kiccha Huccha',), ('Meri Biwi Ki Shaadi',), ('The Silent Heroes',), ('Raakh: A Poem Masked in Blood',), ('Uyirile Kalanthathu',), ('C U at 9',), ('Umar',), ('Neel Kamal',), ('Aashiqui.in',), ('Dr. Kotnis Ki Amar Kahani',), ('Azaad',), ('Ginny Aur Johny',), ('5 Ghantey Mien 5 Crore',), ('Hip Hip Hurray',), ('Killer',), ('Enakkul Oruvan',), ('Diksha',), ('Kaisay Kahein...',), ('Shreemaan Aashique',), ('Tahalka',), ('Barood',), ('Aag Hi Aag',), ('Sins',), ('Elaan-E-Jung',), ('Khwaabb',), ('Chupke Se',), ('Man on Mission Fauladi',), ('The Great Indian Butterfly',), ('Padatik',), ('Phir Kabhi',), ('Love at Times Square',), ('Straight',), ('Lakhon Ki Baat',), ('Stoovertpuram Police Station',), ('Dil Ka Kya Kasoor',), ('Daag: The Fire',), ('Zapatlela',), ('Arjun Pandit',), ('Khokababu',), ('Muskaan',), ('Aamaar Bhuvan',), ('Ghar Grihasti',), ('Mohre',), ('Madholal Keep Walking',), ('Meerabai Not Out',), ('Payback',), ('Lal Darja',), ('Aavida Maa Aavide',), ('Bolo Raam',), ('Damadamm!',), ('Kucch Luv Jaisaa',), ('Hum Dum',), ('Kalloori Vaasal',), ('Visakha Express',), ('Edegarike',), ('Ranga S.S.L.C',), ('Sazaye Maut',), ('Mr. Theertha',), ('Sahi Dhandhe Galat Bande',), ('Love Birds',), ('Yaad Rakhegi Duniya',), ('Narasimha',), ('Maa',), ('Ankhiyon Ke Jharokhon Se',), ('Ab Ke Baras',), ('Ajay',), ('Ek Duuje Ke Liye',), ('Ganga',), ('Rudranetra',), ('Satrangee Parachute',), ('Pestonjee',), ('Once We Were Strangers',), ('Oru Kaidhiyin Diary',), ('Kairee',), ('Morning Walk',), ('Miss India: The Mystery',), ('Phir Subha Hogi',), ('Ooops a Desi',), ('Snip!',), ('The Prince of Light',), ('Doodh Ka Karz',), ('Izzat',), ('Dushmani: A Violent Love Story',), ('Teri Meherbaniyan',), ('Dance of the Wind',), ('Mumbai Matinee',), ('Mahakali Ka Insaaf',), ('Ek Din 24 Ghante',), ('The Taste of Relation',), ('Bhootnath',), ('White Rainbow',), ('Ottran',), (\"Let's Enjoy\",), ('Hamari Bahu Alka',), ('Masala Republic',), ('Bagh Bahadur',), ('Target',), ('Ghatotkachudu',), ('Mohabbat',), ('Raja Ki Ayegi Baraat',), ('Captain Prabhakaran',), ('Nigahen: Nagina Part II',), ('Sathyam',), ('Impatient Vivek',), ('Life! Camera Action...',), ('Daayen Ya Baayen',), ('Shuttlecock Boys',), ('Surkhaab',), ('Sirf....: Life Looks Greener on the Other Side',), ('Laadli Laila',), ('Siddharth: The Prisoner',), ('Kal: Yesterday and Tomorrow',), ('Thirupathi',), ('Tandoori Love',), (\"It's Breaking News\",), ('Maya Darpan',), ('Kaadhal Parisu',), ('Boss',), ('Hei ma lai ah sing',), ('Ashwamedham',), ('Kvelaferi kargad iqneba',), ('Seema Simham',), ('Angel',), ('Junglee',), ('Phool Bane Angaarey',), ('Fakira',), ('Maa Tujhhe Salaam',), ('Le Halua Le',), ('Shohrat the Trap',), ('The Untitled Kartik Krishnan Project',), ('Chettaniki Kallu Levu',), ('Dansh',), ('Ram Raaj',), ('Aaj Ka Arjun',), ('Goa Dalli CID 999',), ('Yeh Hai Malegaon Ka Superman',), ('Military Officer',), ('Man on Mission Taqatwar',), ('Raasaiyya',), ('Finding Preet',), ('Ek Hi Raasta',), ('Mehndi',), ('Raja Aur Rangeeli',), ('Thoongathey Tambi Thoongathey',), ('Bewafa Sanam',), ('Geet Gaata Chal',), ('Allah-Rakha',), ('Anari',), ('Come December',), ('Kala Jigar',), ('Kanoon',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(\n",
        "        -1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the English question\n",
        "query_sentence = input(\"Enter English Question: \")\n",
        "\n",
        "# Function Semantically find WHERE clause in the query sentence and return True if found else False\n",
        "def find_where_clause(query_sentence):\n",
        "    # Tokenize the query sentence\n",
        "    query_sentence_encoded = tokenizer(\n",
        "        [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "    # Compute token embeddings for the query sentence\n",
        "    with torch.no_grad():\n",
        "        query_sentence_output = model(**query_sentence_encoded)\n",
        "    # Perform mean pooling on the output of the language model for the query sentence\n",
        "    query_sentence_embedding = mean_pooling(\n",
        "        query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "    # Normalize the embeddings for the query sentence\n",
        "    query_sentence_embedding = F.normalize(\n",
        "        query_sentence_embedding, p=2, dim=1)\n",
        "    # Tokenize the word 'where'\n",
        "    where_clause_encoded = tokenizer(\n",
        "        ['where'], padding=True, truncation=True, return_tensors='pt')\n",
        "    # Compute token embeddings for the word 'where'\n",
        "    with torch.no_grad():\n",
        "        where_clause_output = model(**where_clause_encoded)\n",
        "    # Perform mean pooling on the output of the language model for the word 'where'\n",
        "    where_clause_embedding = mean_pooling(\n",
        "        where_clause_output, where_clause_encoded['attention_mask'])\n",
        "    # Normalize the embeddings for the word 'where'\n",
        "    where_clause_embedding = F.normalize(\n",
        "        where_clause_embedding, p=2, dim=1)\n",
        "    # Compute cosine similarity between the query sentence embedding and the word 'where' embedding\n",
        "    cosine_similarities = torch.nn.functional.cosine_similarity(\n",
        "        query_sentence_embedding, where_clause_embedding, dim=1)\n",
        "    # If cosine similarity is greater than 0.5, return True else False\n",
        "    if cosine_similarities[0] > 0.5:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Print whether the word 'where' is present in the query sentence semantically or not\n",
        "print(find_where_clause(query_sentence))\n",
        "\n",
        "\n",
        "\n",
        "# Connect to database and fetch table names and column names\n",
        "# Update the path to the database file\n",
        "conn = sqlite3.connect('/content/Db-IMDB.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "table_names = [table_info[0] for table_info in cursor.execute(\n",
        "    \"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()]\n",
        "column_names = []\n",
        "for table_name in table_names:\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
        "    column_names.extend([column_info[1] for column_info in cursor.fetchall()])\n",
        "\n",
        "# Tokenize query sentence, table names, and column names\n",
        "query_sentence_encoded = tokenizer(\n",
        "    [query_sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "table_names_encoded = tokenizer(\n",
        "    table_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings for query sentence, table names, and column names\n",
        "with torch.no_grad():\n",
        "    query_sentence_output = model(**query_sentence_encoded)\n",
        "    table_names_output = model(**table_names_encoded)\n",
        "\n",
        "# Perform pooling for query sentence, table names, and column names\n",
        "query_sentence_embedding = mean_pooling(\n",
        "    query_sentence_output, query_sentence_encoded['attention_mask'])\n",
        "table_names_embeddings = mean_pooling(\n",
        "    table_names_output, table_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize embeddings for query sentence, table names, and column names\n",
        "query_sentence_embedding = F.normalize(query_sentence_embedding, p=2, dim=1)\n",
        "table_names_embeddings = F.normalize(table_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Find the most similar table names and column names by computing the cosine similarity between the query sentence embedding and the table names and column names embeddings\n",
        "cosine_similarities_tables = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, table_names_embeddings, dim=1)\n",
        "most_similar_table_names_indices = cosine_similarities_tables.argsort(\n",
        "    descending=True)\n",
        "most_similar_table_names = [table_names[i]\n",
        "                            for i in most_similar_table_names_indices]\n",
        "\n",
        "# Print the most similar table names with there cosine similarity scores in descending order\n",
        "for i in range(len(most_similar_table_names)):\n",
        "    print(\n",
        "        f\"Table name: {most_similar_table_names[i]}, cosine similarity score: {cosine_similarities_tables[most_similar_table_names_indices[i]]}\")\n",
        "\n",
        "# Find the index of the highest matching table name by finding the maximum value in the list of cosine similarities for the table names\n",
        "max_similarity_table_index = cosine_similarities_tables.argmax()\n",
        "\n",
        "# Get the highest matching table name by using the index obtained above\n",
        "highest_matching_table_name = table_names[max_similarity_table_index]\n",
        "\n",
        "# Find the column names of the highest matching table by querying the database\n",
        "cursor.execute(f\"PRAGMA table_info({highest_matching_table_name});\")\n",
        "highest_matching_table_column_names = [\n",
        "    column_info[1] for column_info in cursor.fetchall()]\n",
        "\n",
        "# Tokenize the column names of the highest matching table\n",
        "highest_matching_table_column_names_encoded = tokenizer(\n",
        "    highest_matching_table_column_names, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute the token embeddings for the column names of the highest matching table\n",
        "with torch.no_grad():\n",
        "    highest_matching_table_column_names_output = model(\n",
        "        **highest_matching_table_column_names_encoded)\n",
        "\n",
        "# Perform mean pooling on the output of the language model for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = mean_pooling(\n",
        "    highest_matching_table_column_names_output, highest_matching_table_column_names_encoded['attention_mask'])\n",
        "\n",
        "# Normalize the embeddings for the column names of the highest matching table\n",
        "highest_matching_table_column_names_embeddings = F.normalize(\n",
        "    highest_matching_table_column_names_embeddings, p=2, dim=1)\n",
        "\n",
        "# Compute the cosine similarity between the query sentence embedding and the column names embeddings of the highest matching table\n",
        "cosine_similarities_highest_matching_table_columns = torch.nn.functional.cosine_similarity(\n",
        "    query_sentence_embedding, highest_matching_table_column_names_embeddings, dim=1)\n",
        "\n",
        "# Find the most similar column name in the highest matching table by sorting the cosine similarities in descending order\n",
        "most_similar_highest_matching_table_column_name_index = cosine_similarities_highest_matching_table_columns.argmax()\n",
        "most_similar_highest_matching_table_column_name = highest_matching_table_column_names[\n",
        "    most_similar_highest_matching_table_column_name_index]\n",
        "\n",
        "\n",
        "# Generate an SQL SELECT query based on the most similar table names and column names\n",
        "query = f\"SELECT {most_similar_highest_matching_table_column_name} FROM {most_similar_table_names[0]}\"\n",
        "\n",
        "# Iterate through the list of possible queries and execute each one\n",
        "\n",
        "# Print the generated SQL query\n",
        "print(query)\n",
        "\n",
        "try:\n",
        "    cursor.execute(query)\n",
        "    results = cursor.fetchall()\n",
        "    print(f'Query: {query}')\n",
        "    print(f'Results: {results}')\n",
        "except Exception as e:\n",
        "    print(f'Error: {e}')\n",
        "\n",
        "\n",
        "# close the connection to the database\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZUlAmdqB7L3",
        "outputId": "cf0b60ad-4544-4188-fda7-c95f8b9ff75f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter English Question: list the names of films which are released in 2018?\n",
            "False\n",
            "Table name: Movie, cosine similarity score: 0.3608573377132416\n",
            "Table name: Genre, cosine similarity score: 0.2993427813053131\n",
            "Table name: M_Genre, cosine similarity score: 0.241574227809906\n",
            "Table name: M_Director, cosine similarity score: 0.20498503744602203\n",
            "Table name: M_Producer, cosine similarity score: 0.18535827100276947\n",
            "Table name: M_Cast, cosine similarity score: 0.1720818728208542\n",
            "Table name: Location, cosine similarity score: 0.15005813539028168\n",
            "Table name: Country, cosine similarity score: 0.138449028134346\n",
            "Table name: Language, cosine similarity score: 0.11577567458152771\n",
            "Table name: M_Country, cosine similarity score: 0.10471761226654053\n",
            "Table name: M_Location, cosine similarity score: 0.0661163479089737\n",
            "Table name: M_Language, cosine similarity score: 0.0559806302189827\n",
            "Table name: Person, cosine similarity score: 0.05206707864999771\n",
            "SELECT year FROM Movie\n",
            "Query: SELECT year FROM Movie\n",
            "Results: [('2018',), ('2018',), ('2018',), ('2012',), ('2018',), ('2018',), ('2016',), ('2018',), ('2016',), ('2018',), ('2017',), ('2008',), ('I 2009',), ('2016',), ('1977',), ('2018',), ('2012',), ('2013',), ('2015',), ('2018',), ('2018',), ('2018',), ('2018',), ('2007',), ('2018',), ('2002',), ('2016',), ('2018',), ('1951',), ('2009',), ('2018',), ('2014',), ('2004',), ('2009',), ('2007',), ('2015',), ('2014',), ('2014',), ('2012',), ('1997',), ('1983',), ('1994',), ('2018',), ('2017',), ('2018',), ('2018',), ('2018',), ('2014',), ('2008',), ('2011',), ('2004',), ('1996',), ('2018',), ('2018',), ('2017',), ('2001',), ('2018',), ('2010',), ('2018',), ('2018',), ('2018',), ('2011',), ('2006',), ('2018',), ('2013',), ('2015',), ('1971',), ('2017',), ('2018',), ('2018',), ('2007',), ('I 2018',), ('2018',), ('2018',), ('2018',), ('2018',), ('2008',), ('2014',), ('2013',), ('2013',), ('2017',), ('XVII 2016',), ('2018',), ('2012',), ('1958',), ('2017',), ('2015',), ('2017',), ('2004',), ('2018',), ('I 2017',), ('1984',), ('2018',), ('2001',), ('2015',), ('2017',), ('2018',), ('2017',), ('2016',), ('2017',), ('2018',), ('1987',), ('2018',), ('2018',), ('2015',), ('II 2018',), ('2015',), ('2012',), ('2017',), ('2012',), ('2017',), ('2014',), ('1996',), ('2015',), ('2015',), ('2017',), ('2012',), ('2014',), ('2016',), ('2017',), ('2009',), ('2005',), ('2018',), ('2016',), ('2006',), ('2016',), ('I 2002',), ('2017',), ('2016',), ('I 2018',), ('III 2016',), ('2013',), ('2015',), ('2008',), ('2003',), ('2018',), ('2018',), ('2013',), ('2017',), ('2008',), ('2013',), ('2012',), ('2016',), ('2017',), ('I 2018',), ('1995',), ('2009',), ('I 2018',), ('2006',), ('2017',), ('2003',), ('2016',), ('2018',), ('2017',), ('2016',), ('2015',), ('2001',), ('2017',), ('2012',), ('2015',), ('2016',), ('2011',), ('2007',), ('1998',), ('2007',), ('2016',), ('2017',), ('2014',), ('2013',), ('2017',), ('2018',), ('2004',), ('2011',), ('2015',), ('2008',), ('2017',), ('2011',), ('2017',), ('2018',), ('2011',), ('2008',), ('2016',), ('2012',), ('2017',), ('2013',), ('2013',), ('2016',), ('2014',), ('2004',), ('2018',), ('2017',), ('2012',), ('2018',), ('I 2015',), ('2018',), ('2018',), ('1975',), ('2017',), ('2018',), ('1939',), ('2018',), ('I 2018',), ('2007',), ('2013',), ('2013',), ('2004',), ('2009',), ('2015',), ('2006',), ('2006',), ('2015',), ('2016',), ('2017',), ('I 2018',), ('2015',), ('2017',), ('2002',), ('2016',), ('2018',), ('2003',), ('I 2015',), ('2014',), ('2017',), ('2000',), ('I 2015',), ('2017',), ('2016',), ('2017',), ('2013',), ('2018',), ('2014',), ('1988',), ('2016',), ('2017',), ('2007',), ('2006',), ('2003',), ('2013',), ('2008',), ('2013',), ('2018',), ('2008',), ('I 2016',), ('I 2016',), ('2018',), ('2011',), ('2016',), ('2012',), ('2018',), ('2016',), ('2011',), ('2009',), ('2011',), ('I 2006',), ('2014',), ('I 2010',), ('2015',), ('2014',), ('I 2013',), ('2013',), ('2010',), ('2000',), ('I 2014',), ('2006',), ('2015',), ('2011',), ('2009',), ('I 2018',), ('2016',), ('2012',), ('I 2018',), ('2013',), ('2012',), ('2017',), ('2001',), ('2012',), ('2011',), ('2015',), ('2017',), ('II 2017',), ('2012',), ('2016',), ('2018',), ('2004',), ('2012',), ('2015',), ('2004',), ('2015',), ('2012',), ('2016',), ('I 2014',), ('2002',), ('I 2016',), ('2008',), ('2017',), ('2015',), ('2015',), ('2006',), ('2014',), ('2012',), ('2013',), ('2004',), ('2007',), ('2017',), ('2009',), ('2018',), ('2017',), ('2018',), ('2003',), ('2016',), ('2000',), ('2013',), ('1996',), ('2007',), ('2016',), ('2009',), ('2017',), ('1998',), ('2014',), ('2008',), ('2013',), ('2006',), ('2011',), ('2014',), ('2000',), ('2016',), ('2007',), ('2013',), ('2013',), ('2012',), ('2002',), ('2006',), ('2013',), ('2010',), ('2017',), ('2016',), ('2009',), ('2010',), ('2017',), ('2016',), ('2000',), ('2016',), ('I 2018',), ('2005',), ('2015',), ('1996',), ('2016',), ('2005',), ('2018',), ('1996',), ('2015',), ('2015',), ('2016',), ('I 2016',), ('I 2014',), ('1994',), ('2010',), ('1997',), ('1991',), ('2016',), ('I 2015',), ('2016',), ('2010',), ('2017',), ('2016',), ('2010',), ('1993',), ('2016',), ('2015',), ('2015',), ('2012',), ('I 2016',), ('2005',), ('2012',), ('2017',), ('2017',), ('1981',), ('2017',), ('2002',), ('2016',), ('2008',), ('2010',), ('2014',), ('2014',), ('2011',), ('2018',), ('1989',), ('2018',), ('2017',), ('1976',), ('2017',), ('1998',), ('2017',), ('2017',), ('2017',), ('2007',), ('2017',), ('2005',), ('2015',), ('2015',), ('2009',), ('2005',), ('2010',), ('1959',), ('2015',), ('2014',), ('2017',), ('1970',), ('2008',), ('2017',), ('2007',), ('2018',), ('VI 2015',), ('2011',), ('2010',), ('2010',), ('2011',), ('2016',), ('2012',), ('2011',), ('2010',), ('2014',), ('1979',), ('2015',), ('I 1964',), ('2018',), ('2007',), ('1993',), ('2014',), ('2018',), ('2013',), ('2016',), ('2000',), ('2009',), ('2013',), ('2007',), ('2014',), ('2015',), ('2012',), ('2015',), ('2004',), ('2014',), ('2017',), ('2014',), ('2001',), ('2017',), ('2016',), ('2016',), ('2012',), ('1994',), ('2005',), ('2005',), ('2004',), ('2009',), ('2013',), ('1999',), ('2015',), ('2018',), ('2012',), ('2010',), ('2017',), ('I 2003',), ('2013',), ('2015',), ('2011',), ('2018',), ('2014',), ('2017',), ('I 2014',), ('2017',), ('2015',), ('2008',), ('I 2014',), ('2016',), ('2013',), ('1999',), ('2016',), ('I 2017',), ('2001',), ('2014',), ('2004',), ('2014',), ('2014',), ('III 2017',), ('2018',), ('I 2005',), ('2002',), ('2016',), ('2018',), ('I 2013',), ('2007',), ('2017',), ('2017',), ('I 2008',), ('2018',), ('2018',), ('2003',), ('2018',), ('2010',), ('1999',), ('2011',), ('1995',), ('2016',), ('I 2016',), ('2000',), ('2015',), ('2003',), ('2016',), ('2007',), ('2014',), ('2017',), ('2014',), ('2014',), ('1983',), ('2017',), ('1990',), ('2017',), ('1992',), ('1993',), ('2013',), ('2015',), ('2013',), ('2009',), ('1992',), ('2007',), ('2010',), ('2006',), ('2014',), ('2002',), ('1959',), ('I 2007',), ('2005',), ('2010',), ('2017',), ('2005',), ('2009',), ('2007',), ('I 2013',), ('2005',), ('1996',), ('2013',), ('1997',), ('2004',), ('1995',), ('2012',), ('2014',), ('2013',), ('2015',), ('2016',), ('2012',), ('2013',), ('2006',), ('2004',), ('2013',), ('2013',), ('2011',), ('2010',), ('2011',), ('2008',), ('2015',), ('2012',), ('2011',), ('2005',), ('2013',), ('2001',), ('1975',), ('2014',), ('2016',), ('2010',), ('2005',), ('2007',), ('2012',), ('2002',), ('1990',), ('2006',), ('2010',), ('2012',), ('2016',), ('2006',), ('1994',), ('2012',), ('2013',), ('2010',), ('I 2011',), ('I 2013',), ('2010',), ('2013',), ('2014',), ('2017',), ('2009',), ('2003',), ('2017',), ('2010',), ('2010',), ('1995',), ('1983',), ('2005',), ('2014',), ('2014',), ('2009',), ('2005',), ('2010',), ('1987',), ('2015',), ('2015',), ('2016',), ('2016',), ('2010',), ('2012',), ('2006',), ('2009',), ('1991',), ('1998',), ('1957',), ('2013',), ('2016',), ('2015',), ('1999',), ('2001',), ('2005',), ('2011',), ('2008',), ('1999',), ('1999',), ('I 2009',), ('2001',), ('2012',), ('2016',), ('2007',), ('2008',), ('2018',), ('2017',), ('2012',), ('1996',), ('2011',), ('2008',), ('2007',), ('2013',), ('2002',), ('2005',), ('1994',), ('2014',), ('1995',), ('2005',), ('2016',), ('2012',), ('2015',), ('1994',), ('2013',), ('2017',), ('2008',), ('2015',), ('2002',), ('1980',), ('2014',), ('1989',), ('1966',), ('2015',), ('2006',), ('2013',), ('1967',), ('1973',), ('1970',), ('2013',), ('1998',), ('1994',), ('2003',), ('1999',), ('1994',), ('1992',), ('1988',), ('1993',), ('2001',), ('2012',), ('1995',), ('2007',), ('2014',), ('2007',), ('2011',), ('1995',), ('1991',), ('1970',), ('1990',), ('2010',), ('2006',), ('2008',), ('2016',), ('1998',), ('1994',), ('2002',), ('I 2010',), ('2003',), ('2008',), ('1996',), ('2014',), ('2004',), ('1975',), ('2005',), ('2016',), ('2008',), ('2006',), ('I 1997',), ('1993',), ('2008',), ('2015',), ('2003',), ('1991',), ('1994',), ('2009',), ('1994',), ('1981',), ('1968',), ('1980',), ('2017',), ('1999',), ('1969',), ('2014',), ('2002',), ('2018',), ('1957',), ('2013',), ('1982',), ('1993',), ('I 2016',), ('2004',), ('2009',), ('III 2015',), ('1978',), ('1965',), ('2008',), ('1975',), ('2003',), ('1973',), ('2003',), ('1988',), ('2009',), ('1992',), ('2015',), ('2008',), ('2016',), ('2000',), ('1994',), ('2018',), ('2011',), ('1973',), ('2001',), ('2015',), ('2013',), ('2013',), ('1995',), ('1988',), ('2012',), ('1987',), ('1965',), ('2010',), ('1982',), ('2013',), ('1972',), ('1991',), ('1998',), ('2017',), ('2009',), ('2007',), ('1956',), ('2006',), ('2016',), ('2013',), ('2016',), ('1996',), ('2000',), ('2014',), ('2007',), ('1996',), ('1982',), ('2008',), ('2002',), ('2017',), ('1971',), ('2013',), ('2008',), ('2011',), ('1982',), ('1974',), ('1992',), ('2017',), ('2014',), ('1995',), ('2007',), ('1960',), ('2003',), ('2013',), ('1977',), ('2014',), ('1987',), ('2017',), ('1996',), ('2011',), ('1965',), ('2015',), ('2014',), ('2008',), ('1978',), ('2017',), ('2010',), ('1971',), ('2004',), ('1998',), ('2005',), ('I 2016',), ('2008',), ('1999',), ('1985',), ('1975',), ('2000',), ('1982',), ('2011',), ('1999',), ('2016',), ('2005',), ('I 2003',), ('1981',), ('2011',), ('2000',), ('2014',), ('1974',), ('2016',), ('1996',), ('1981',), ('2005',), ('1991',), ('2006',), ('1993',), ('II 2012',), ('2010',), ('1993',), ('1972',), ('2004',), ('2000',), ('III 2015',), ('1986',), ('2005',), ('1982',), ('1949',), ('1970',), ('2005',), ('2001',), ('2000',), ('2010',), ('2013',), ('1998',), ('1993',), ('2002',), ('2002',), ('1991',), ('1976',), ('1992',), ('2011',), ('2005',), ('2002',), ('2017',), ('1985',), ('2012',), ('1991',), ('1992',), ('1993',), ('2016',), ('2012',), ('2007',), ('1994',), ('2017',), ('2013',), ('1979',), ('1964',), ('2000',), ('2007',), ('1992',), ('III 2007',), ('1995',), ('1970',), ('2003',), ('1994',), ('2011',), ('1994',), ('2001',), ('2016',), ('2013',), ('2009',), ('2003',), ('2016',), ('2017',), ('1974',), ('1998',), ('1982',), ('2010',), ('2013',), ('2011',), ('2014',), ('2008',), ('2001',), ('1997',), ('2013',), ('2005',), ('2007',), ('2012',), ('2005',), ('2010',), ('1982',), ('1995',), ('1989',), ('2007',), ('2010',), ('2012',), ('1998',), ('2012',), ('1977',), ('2003',), ('2002',), ('2010',), ('2004',), ('2005',), ('1993',), ('2004',), ('I 2014',), ('2017',), ('1977',), ('1999',), ('1983',), ('2007',), ('1991',), ('2009',), ('2002',), ('1966',), ('2009',), ('2006',), ('2014',), ('1997',), ('1998',), ('1999',), ('2009',), ('1997',), ('2003',), ('2017',), ('1973',), ('2001',), ('2005',), ('1995',), ('2003',), ('1992',), ('2010',), ('2006',), ('2009',), ('1998',), ('2002',), ('2014',), ('1993',), ('2015',), ('2011',), ('2013',), ('1978',), ('2012',), ('2017',), ('2003',), ('2017',), ('1991',), ('2003',), ('1967',), ('2018',), ('1967',), ('2016',), ('2010',), ('1976',), ('2013',), ('2013',), ('2004',), ('1982',), ('2002',), ('1996',), ('1997',), ('1994',), ('2014',), ('2003',), ('2003',), ('1964',), ('1993',), ('1969',), ('2007',), ('1981',), ('2015',), ('2015',), ('2011',), ('2015',), ('1978',), ('1974',), ('2014',), ('2007',), ('2016',), ('2000',), ('1974',), ('1976',), ('2014',), ('1997',), ('1992',), ('2002',), ('2018',), ('2014',), ('1999',), ('2017',), ('2008',), ('1982',), ('1983',), ('1997',), ('2006',), ('1988',), ('2014',), ('2010',), ('I 2011',), ('1977',), ('2017',), ('2012',), ('2009',), ('1973',), ('1971',), ('1976',), ('1985',), ('2004',), ('1993',), ('2005',), ('1990',), ('1997',), ('2006',), ('2009',), ('1955',), ('2003',), ('2002',), ('1987',), ('2003',), ('1986',), ('2014',), ('1958',), ('2007',), ('2013',), ('1966',), ('1966',), ('2003',), ('2010',), ('2012',), ('2017',), ('1993',), ('1971',), ('1999',), ('2010',), ('1949',), ('1965',), ('1998',), ('1976',), ('2016',), ('2000',), ('2000',), ('2010',), ('2018',), ('2010',), ('1989',), ('1988',), ('2010',), ('2000',), ('2003',), ('2010',), ('1964',), ('1996',), ('2011',), ('2009',), ('2015',), ('2016',), ('2001',), ('2010',), ('1992',), ('2014',), ('2014',), ('2004',), ('1967',), ('II 2010',), ('2004',), ('1972',), ('1956',), ('1972',), ('1987',), ('1961',), ('2009',), ('2018',), ('2002',), ('2012',), ('1997',), ('1997',), ('2013',), ('2014',), ('I 2009',), ('2010',), ('2006',), ('2003',), ('2009',), ('1998',), ('2003',), ('2013',), ('2014',), ('2015',), ('2000',), ('2012',), ('2016',), ('2013',), ('1992',), ('2006',), ('1962',), ('1988',), ('2015',), ('2016',), ('2004',), ('2000',), ('2003',), ('1998',), ('1992',), ('2015',), ('2018',), ('2008',), ('2002',), ('1993',), ('1969',), ('1983',), ('1995',), ('1966',), ('2004',), ('2007',), ('1995',), ('2004',), ('2016',), ('2017',), ('1985',), ('2015',), ('2008',), ('1997',), ('2002',), ('2010',), ('1958',), ('2011',), ('I 2013',), ('1997',), ('2007',), ('2008',), ('2016',), ('2015',), ('2017',), ('2013',), ('1989',), ('2010',), ('2003',), ('1964',), ('1965',), ('1988',), ('2010',), ('2004',), ('2014',), ('2010',), ('2009',), ('1994',), ('2015',), ('1990',), ('2007',), ('1994',), ('2011',), ('1998',), ('2009',), ('2014',), ('1999',), ('1982',), ('1998',), ('2006',), ('2007',), ('2003',), ('2014',), ('2013',), ('II 2013',), ('1993',), ('2005',), ('1975',), ('2016',), ('1987',), ('1991',), ('2000',), ('I 2013',), ('2001',), ('1962',), ('2011',), ('1966',), ('2017',), ('2013',), ('2015',), ('2011',), ('2013',), ('2005',), ('2012',), ('2007',), ('2007',), ('1995',), ('2001',), ('1990',), ('2013',), ('2017',), ('1980',), ('2004',), ('1991',), ('2006',), ('2011',), ('2008',), ('2003',), ('1980',), ('2001',), ('1986',), ('2005',), ('2010',), ('1984',), ('1984',), ('1995',), ('2014',), ('I 2008',), ('1989',), ('1975',), ('1986',), ('2008',), ('2007',), ('2012',), ('2015',), ('1997',), ('1995',), ('2017',), ('2004',), ('I 1986',), ('2006',), ('1992',), ('2002',), ('1985',), ('2002',), ('1999',), ('2006',), ('1988',), ('I 1968',), ('2014',), ('1971',), ('1999',), ('2016',), ('2017',), ('2012',), ('2001',), ('2001',), ('1954',), ('I 2017',), ('1993',), ('2009',), ('2016',), ('2016',), ('1999',), ('2004',), ('2016',), ('1993',), ('2004',), ('2014',), ('1993',), ('2013',), ('2005',), ('2011',), ('1999',), ('2003',), ('2014',), ('2000',), ('1982',), ('2012',), ('2014',), ('2003',), ('1990',), ('2010',), ('1986',), ('1994',), ('2011',), ('2015',), ('2004',), ('1995',), ('2017',), ('2009',), ('2001',), ('2004',), ('2010',), ('2004',), ('1999',), ('1999',), ('1997',), ('1998',), ('2006',), ('2007',), ('2001',), ('1986',), ('I 1980',), ('1998',), ('2014',), ('1986',), ('2008',), ('2017',), ('2000',), ('1985',), ('1967',), ('2000',), ('1984',), ('2006',), ('2017',), ('2012',), ('1994',), ('1987',), ('2003',), ('2009',), ('1998',), ('1989',), ('2005',), ('1998',), ('1993',), ('2002',), ('2016',), ('1993',), ('I 2009',), ('1993',), ('1989',), ('2016',), ('2014',), ('1951',), ('2002',), ('2006',), ('2003',), ('2014',), ('2007',), ('2006',), ('2001',), ('2003',), ('1978',), ('1979',), ('1980',), ('2005',), ('1991',), ('2018',), ('1982',), ('2010',), ('1988',), ('2001',), ('2013',), ('2014',), ('2002',), ('1982',), ('2013',), ('1966',), ('1974',), ('1982',), ('1996',), ('2016',), ('2015',), ('2014',), ('2007',), ('1994',), ('2009',), ('1960',), ('2005',), ('2005',), ('2005',), ('1999',), ('1991',), ('1975',), ('1966',), ('2006',), ('1970',), ('2016',), ('2017',), ('2005',), ('2009',), ('2008',), ('1989',), ('1981',), ('2002',), ('1999',), ('1990',), ('1981',), ('2008',), ('1941',), ('1996',), ('2014',), ('2005',), ('2006',), ('2011',), ('1996',), ('2013',), ('1993',), ('1999',), ('2007',), ('2015',), ('1986',), ('2009',), ('1972',), ('2002',), ('1995',), ('1981',), ('2014',), ('2017',), ('1995',), ('1989',), ('2002',), ('1977',), ('1998',), ('1993',), ('1991',), ('2005',), ('2009',), ('2005',), ('1996',), ('2015',), ('1992',), ('2011',), ('2010',), ('1986',), ('1995',), ('2002',), ('2009',), ('2004',), ('1988',), ('1998',), ('2001',), ('2006',), ('2012',), ('1960',), ('2014',), ('2017',), ('1980',), ('2013',), ('2016',), ('2012',), ('2000',), ('1956',), ('2004',), ('1999',), ('1993',), ('1996',), ('1975',), ('2002',), ('1983',), ('2018',), ('2015',), ('1970',), ('2016',), ('1957',), ('1996',), ('2001',), ('1999',), ('2006',), ('1974',), ('2011',), ('2010',), ('1994',), ('1999',), ('2013',), ('2013',), ('1951',), ('1994',), ('2014',), ('2012',), ('1994',), ('2013',), ('1997',), ('1980',), ('2000',), ('1972',), ('1987',), ('1983',), ('2018',), ('1993',), ('1987',), ('2013',), ('1984',), ('1990',), ('2014',), ('2018',), ('2001',), ('I 2007',), ('1996',), ('1999',), ('2006',), ('2010',), ('2002',), ('1994',), ('II 2008',), ('I 1983',), ('1982',), ('2015',), ('2013',), ('2004',), ('2007',), ('2012',), ('1992',), ('1999',), ('1988',), ('2017',), ('I 2010',), ('1993',), ('2002',), ('2012',), ('1993',), ('2007',), ('2003',), ('2008',), ('I 2015',), ('1962',), ('2008',), ('2017',), ('2014',), ('2009',), ('2013',), ('2017',), ('2011',), ('1986',), ('1983',), ('1990',), ('1988',), ('2007',), ('2003',), ('1987',), ('1968',), ('2015',), ('1990',), ('1984',), ('1985',), ('2014',), ('2014',), ('1976',), ('2013',), ('2008',), ('2005',), ('2014',), ('2014',), ('2017',), ('1983',), ('1990',), ('2001',), ('2006',), ('2015',), ('2017',), ('2003',), ('2001',), ('2005',), ('V 2015',), ('1994',), ('2005',), ('1999',), ('1978',), ('2003',), ('2002',), ('1982',), ('2013',), ('2012',), ('1995',), ('1996',), ('2004',), ('2001',), ('1990',), ('1972',), ('2016',), ('1998',), ('2007',), ('2017',), ('1988',), ('2009',), ('1994',), ('2004',), ('1989',), ('2002',), ('2001',), ('2000',), ('2018',), ('2005',), ('1983',), ('1993',), ('1999',), ('2016',), ('I 2015',), ('1982',), ('2007',), ('2009',), ('2008',), ('2015',), ('2004',), ('2004',), ('2013',), ('2001',), ('I 2002',), ('1972',), ('1992',), ('2003',), ('1990',), ('2014',), ('2009',), ('2014',), ('2002',), ('2011',), ('1992',), ('2002',), ('2006',), ('2004',), ('2003',), ('2004',), ('2013',), ('1966',), ('1951',), ('1992',), ('2005',), ('1987',), ('2004',), ('1988',), ('1983',), ('1982',), ('2013',), ('1980',), ('1983',), ('2006',), ('1995',), ('2011',), ('2005',), ('2004',), ('1983',), ('2007',), ('1994',), ('1990',), ('1963',), ('1983',), ('1974',), ('2007',), ('1988',), ('2013',), ('1974',), ('2015',), ('2017',), ('2001',), ('2005',), ('2004',), ('2002',), ('2010',), ('1978',), ('1994',), ('2011',), ('2008',), ('1962',), ('2006',), ('1993',), ('1996',), ('2005',), ('2003',), ('1990',), ('2009',), ('2005',), ('2010',), ('1989',), ('1994',), ('1976',), ('2015',), ('I 2009',), ('2015',), ('1992',), ('2001',), ('1984',), ('2016',), ('1998',), ('1976',), ('2007',), ('2000',), ('1971',), ('1987',), ('2016',), ('1972',), ('1994',), ('2002',), ('1991',), ('2003',), ('1997',), ('2006',), ('2007',), ('1973',), ('1995',), ('2004',), ('2007',), ('1991',), ('1998',), ('1965',), ('2008',), ('1971',), ('2010',), ('1993',), ('2012',), ('2000',), ('2016',), ('2003',), ('1975',), ('1965',), ('2005',), ('1974',), ('1980',), ('1968',), ('1970',), ('1971',), ('1994',), ('1981',), ('1931',), ('2005',), ('1971',), ('2006',), ('2005',), ('1997',), ('1976',), ('2017',), ('2013',), ('2014',), ('2001',), ('I 2017',), ('1997',), ('1986',), ('1980',), ('2001',), ('1961',), ('1963',), ('2017',), ('1967',), ('2001',), ('2011',), ('1973',), ('2007',), ('2018',), ('2008',), ('1975',), ('2016',), ('2010',), ('1970',), ('2005',), ('2005',), ('2011',), ('2000',), ('1997',), ('1980',), ('1977',), ('1998',), ('1966',), ('1969',), ('1983',), ('2013',), ('1996',), ('1990',), ('2006',), ('1989',), ('2007',), ('2000',), ('1955',), ('2015',), ('1972',), ('1993',), ('2002',), ('1961',), ('1993',), ('2017',), ('2005',), ('2010',), ('2016',), ('2000',), ('1969',), ('2003',), ('1973',), ('1971',), ('2014',), ('2010',), ('1994',), ('1995',), ('1985',), ('2016',), ('2005',), ('1991',), ('2002',), ('2013',), ('2014',), ('1969',), ('1985',), ('1985',), ('1967',), ('2003',), ('2013',), ('1993',), ('1980',), ('2000',), ('1953',), ('2006',), ('1998',), ('1981',), ('1980',), ('2003',), ('1969',), ('1992',), ('2008',), ('2010',), ('2009',), ('1993',), ('1949',), ('2015',), ('2011',), ('2017',), ('2008',), ('1996',), ('2011',), ('2018',), ('1967',), ('1972',), ('1999',), ('2001',), ('1968',), ('1986',), ('1993',), ('2015',), ('1979',), ('2001',), ('2011',), ('2009',), ('1970',), ('1979',), ('2009',), ('2000',), ('1964',), ('1972',), ('2012',), ('2012',), ('2012',), ('1959',), ('1989',), ('2005',), ('1981',), ('1965',), ('2008',), ('2014',), ('2012',), ('2014',), ('2004',), ('2003',), ('1996',), ('2006',), ('1973',), ('1972',), ('1999',), ('1995',), ('I 2001',), ('1991',), ('1995',), ('2005',), ('2015',), ('2008',), ('1983',), ('2004',), ('2014',), ('1996',), ('1974',), ('1985',), ('2015',), ('2005',), ('1996',), ('1988',), ('1990',), ('2011',), ('2002',), ('2003',), ('2005',), ('2016',), ('2013',), ('2003',), ('2013',), ('1980',), ('1974',), ('1979',), ('1995',), ('1978',), ('1999',), ('1986',), ('2005',), ('1971',), ('1995',), ('2004',), ('1983',), ('2005',), ('1969',), ('2003',), ('2005',), ('2008',), ('1981',), ('2007',), ('1988',), ('1995',), ('1997',), ('2003',), ('2010',), ('1989',), ('1970',), ('1954',), ('2002',), ('2007',), ('1958',), ('2004',), ('2002',), ('1976',), ('2001',), ('1991',), ('2013',), ('1988',), ('1998',), ('1992',), ('1999',), ('2007',), ('1987',), ('1963',), ('2003',), ('2004',), ('1970',), ('1999',), ('2002',), ('1976',), ('1986',), ('1999',), ('1993',), ('2000',), ('1997',), ('1970',), ('1998',), ('1989',), ('1990',), ('1989',), ('2013',), ('2013',), ('1996',), ('1985',), ('1999',), ('1993',), ('2002',), ('2018',), ('2015',), ('1955',), ('1977',), ('2004',), ('1991',), ('1986',), ('2009',), ('1958',), ('2004',), ('2013',), ('2014',), ('2010',), ('1971',), ('2011',), ('2009',), ('2008',), ('1965',), ('1976',), ('2008',), ('2014',), ('2005',), ('2005',), ('1989',), ('2010',), ('1998',), ('1988',), ('2009',), ('2016',), ('1993',), ('2013',), ('2006',), ('1977',), ('1996',), ('1990',), ('1960',), ('1981',), ('2005',), ('2001',), ('2004',), ('1977',), ('1979',), ('1988',), ('2012',), ('1978',), ('1996',), ('1999',), ('III 2007',), ('1969',), ('1953',), ('1992',), ('2014',), ('2006',), ('1987',), ('2003',), ('1975',), ('1999',), ('2006',), ('2012',), ('1996',), ('2014',), ('2007',), ('1977',), ('2015',), ('1996',), ('I 2010',), ('1984',), ('2005',), ('1992',), ('2011',), ('I 1989',), ('2015',), ('2005',), ('2013',), ('2005',), ('2005',), ('1964',), ('1993',), ('2004',), ('2017',), ('2013',), ('2003',), ('2004',), ('2011',), ('2010',), ('2005',), ('I 2013',), ('2006',), ('1992',), ('1960',), ('1948',), ('1997',), ('2006',), ('1995',), ('2008',), ('2003',), ('1952',), ('1983',), ('2002',), ('1976',), ('1964',), ('1990',), ('2000',), ('1978',), ('1957',), ('2017',), ('2014',), ('1982',), ('2007',), ('1993',), ('2006',), ('2010',), ('1992',), ('1988',), ('2009',), ('1963',), ('1953',), ('1966',), ('2013',), ('2014',), ('2009',), ('1954',), ('1957',), ('1983',), ('1999',), ('1972',), ('2012',), ('2003',), ('1980',), ('1978',), ('II 1998',), ('1977',), ('2014',), ('1989',), ('2003',), ('1971',), ('1968',), ('1994',), ('1990',), ('2013',), ('1965',), ('1997',), ('1984',), ('1993',), ('2010',), ('2002',), ('2001',), ('1952',), ('2006',), ('1961',), ('1960',), ('2015',), ('1997',), ('1973',), ('1999',), ('1982',), ('2005',), ('II 2013',), ('1978',), ('1990',), ('2007',), ('2009',), ('1955',), ('1989',), ('1999',), ('1996',), ('1968',), ('2004',), ('2002',), ('1995',), ('1969',), ('2006',), ('2008',), ('1970',), ('1968',), ('1984',), ('1979',), ('2004',), ('1996',), ('1975',), ('1972',), ('1974',), ('2004',), ('2005',), ('1968',), ('I 2018',), ('1977',), ('1962',), ('1999',), ('2011',), ('2016',), ('2007',), ('2016',), ('2008',), ('2018',), ('2010',), ('2016',), ('2013',), ('1996',), ('2003',), ('2006',), ('2004',), ('2007',), ('I 2014',), ('1989',), ('2012',), ('1995',), ('1976',), ('1972',), ('1977',), ('1984',), ('1995',), ('2014',), ('2008',), ('2007',), ('1953',), ('2005',), ('2010',), ('1981',), ('1996',), ('1977',), ('1981',), ('1994',), ('2011',), ('2010',), ('1955',), ('2010',), ('2009',), ('2009',), ('1992',), ('2010',), ('1997',), ('2000',), ('2009',), ('2013',), ('2015',), ('1997',), ('2008',), ('2012',), ('1992',), ('2003',), ('1968',), ('2000',), ('2004',), ('2017',), ('1997',), ('1985',), ('2000',), ('2002',), ('1994',), ('2003',), ('1992',), ('2010',), ('2015',), ('1962',), ('1995',), ('1964',), ('1977',), ('2000',), ('2014',), ('1996',), ('2002',), ('2008',), ('2004',), ('2005',), ('2005',), ('1997',), ('2016',), ('1967',), ('1971',), ('1967',), ('1971',), ('2008',), ('1995',), ('1985',), ('2005',), ('2014',), ('2011',), ('1984',), ('2011',), ('2005',), ('2016',), ('1990',), ('2007',), ('2010',), ('1980',), ('2016',), ('1998',), ('2008',), ('2001',), ('1952',), ('2004',), ('1978',), ('1996',), ('1981',), ('1952',), ('1980',), ('1994',), ('2003',), ('2012',), ('2005',), ('1998',), ('2010',), ('2009',), ('1997',), ('2005',), ('2001',), ('2012',), ('2005',), ('1994',), ('2006',), ('1995',), ('2005',), ('2007',), ('2016',), ('2006',), ('2008',), ('1972',), ('1954',), ('2008',), ('1981',), ('2002',), ('2016',), ('1979',), ('1962',), ('2011',), ('1978',), ('1999',), ('2004',), ('2015',), ('1974',), ('1978',), ('1986',), ('1953',), ('2006',), ('2012',), ('1972',), ('2011',), ('2010',), ('2007',), ('1984',), ('1977',), ('1994',), ('1988',), ('1996',), ('2006',), ('1964',), ('1980',), ('1982',), ('1983',), ('2012',), ('1981',), ('2010',), ('2007',), ('2004',), ('1987',), ('1983',), ('1973',), ('2005',), ('2000',), ('2009',), ('1999',), ('2015',), ('2006',), ('2015',), ('2008',), ('2012',), ('1973',), ('1994',), ('1966',), ('1985',), ('2003',), ('1974',), ('1960',), ('2014',), ('2012',), ('1992',), ('1986',), ('1998',), ('1973',), ('2006',), ('1973',), ('1990',), ('2015',), ('1994',), ('1999',), ('1988',), ('1970',), ('2001',), ('1976',), ('1999',), ('2000',), ('2011',), ('1970',), ('2010',), ('2014',), ('2006',), ('1986',), ('2008',), ('1997',), ('1954',), ('1973',), ('1990',), ('1973',), ('2010',), ('2016',), ('2000',), ('2003',), ('1989',), ('1967',), ('1993',), ('1974',), ('2006',), ('2007',), ('2004',), ('I 2010',), ('2007',), ('2000',), ('1984',), ('2012',), ('1987',), ('2011',), ('1996',), ('1991',), ('2006',), ('2009',), ('2001',), ('2011',), ('2005',), ('1989',), ('2009',), ('2013',), ('1947',), ('1975',), ('2012',), ('1994',), ('1989',), ('2015',), ('1953',), ('2008',), ('1999',), ('1959',), ('1968',), ('2007',), ('1993',), ('2010',), ('2010',), ('I 1992',), ('2013',), ('2005',), ('1982',), ('1988',), ('1969',), ('I 2008',), ('1977',), ('1999',), ('1987',), ('I 2009',), ('1977',), ('2006',), ('1951',), ('2012',), ('2010',), ('1997',), ('1971',), ('1998',), ('1972',), ('1989',), ('2005',), ('1967',), ('2008',), ('2006',), ('2012',), ('2006',), ('1981',), ('1978',), ('2001',), ('1973',), ('1979',), ('2003',), ('2006',), ('1984',), ('2005',), ('1982',), ('1992',), ('1987',), ('2008',), ('1983',), ('2012',), ('1989',), ('2015',), ('1991',), ('2006',), ('1991',), ('2009',), ('1973',), ('I 2012',), ('2004',), ('2017',), ('2013',), ('2000',), ('2012',), ('2010',), ('2008',), ('2011',), ('2010',), ('1998',), ('1995',), ('1975',), ('2003',), ('2011',), ('1993',), ('2008',), ('1988',), ('2017',), ('1981',), ('2008',), ('2016',), ('I 2009',), ('2002',), ('2008',), ('I 2017',), ('2003',), ('1970',), ('1992',), ('1986',), ('1966',), ('1978',), ('1994',), ('2007',), ('1981',), ('2010',), ('2004',), ('2008',), ('1975',), ('2003',), ('1997',), ('2006',), ('1985',), ('2008',), ('2014',), ('2016',), ('2001',), ('1994',), ('1983',), ('1980',), ('2012',), ('2002',), ('1936',), ('1966',), ('1976',), ('1963',), ('1968',), ('1987',), ('1972',), ('2010',), ('1983',), ('2006',), ('1991',), ('I 1986',), ('2012',), ('2016',), ('1960',), ('2015',), ('1980',), ('1994',), ('1990',), ('2005',), ('1983',), ('2016',), ('1987',), ('1989',), ('2001',), ('1993',), ('1971',), ('1982',), ('2004',), ('1993',), ('1969',), ('1981',), ('1990',), ('1996',), ('2011',), ('2002',), ('1997',), ('2002',), ('1991',), ('2002',), ('2002',), ('1971',), ('2013',), ('1959',), ('1996',), ('2010',), ('1984',), ('1989',), ('1989',), ('2013',), ('2008',), ('1985',), ('2011',), ('1966',), ('1957',), ('1988',), ('1992',), ('1997',), ('1989',), ('1983',), ('2010',), ('1980',), ('1968',), ('2012',), ('1998',), ('1980',), ('2009',), ('2007',), ('2014',), ('1952',), ('1988',), ('1960',), ('1997',), ('1974',), ('1972',), ('1989',), ('2018',), ('1991',), ('1999',), ('1996',), ('1989',), ('1985',), ('I 1996',), ('1985',), ('1997',), ('1955',), ('2013',), ('1986',), ('2010',), ('2016',), ('2011',), ('1998',), ('2010',), ('2011',), ('1991',), ('2002',), ('2006',), ('2011',), ('2005',), ('2011',), ('1964',), ('1961',), ('1962',), ('2017',), ('I 2016',), ('1977',), ('1999',), ('1979',), ('1984',), ('1966',), ('1982',), ('2004',), ('1973',), ('2001',), ('1996',), ('1998',), ('2016',), ('2012',), ('1946',), ('1986',), ('1956',), ('2012',), ('2016',), ('2011',), ('1979',), ('2007',), ('1990',), ('1972',), ('2001',), ('1999',), ('2011',), ('1958',), ('2009',), ('1967',), ('1981',), ('2016',), ('2010',), ('2000',), ('2010',), ('I 2011',), ('1994',), ('1967',), ('2014',), ('2001',), ('1987',), ('1992',), ('1998',), ('2007',), ('1994',), ('2002',), ('2008',), ('1963',), ('2007',), ('2005',), ('2004',), ('1992',), ('2015',), ('2003',), ('2007',), ('1958',), ('1953',), ('2010',), ('2012',), ('1979',), ('2002',), ('2012',), ('1994',), ('2009',), ('2006',), ('1982',), ('2005',), ('1996',), ('1994',), ('2000',), ('2006',), ('2008',), ('2011',), ('1987',), ('1972',), ('2013',), ('1984',), ('1959',), ('1973',), ('2012',), ('2006',), ('1994',), ('1967',), ('2006',), ('2011',), ('1985',), ('2001',), ('1989',), ('1999',), ('1969',), ('1984',), ('1993',), ('2000',), ('2006',), ('1960',), ('2005',), ('1974',), ('2004',), ('1996',), ('2002',), ('1998',), ('2008',), ('1964',), ('2012',), ('1962',), ('2015',), ('2009',), ('1998',), ('1982',), ('2003',), ('1970',), ('1983',), ('2016',), ('1973',), ('2011',), ('2004',), ('2004',), ('1969',), ('2007',), ('1968',), ('1962',), ('1997',), ('1989',), ('1993',), ('2000',), ('1996',), ('2007',), ('1987',), ('1977',), ('1981',), ('1986',), ('1997',), ('1989',), ('2003',), ('1992',), ('2005',), ('1979',), ('1990',), ('1975',), ('2003',), ('1980',), ('1980',), ('2001',), ('1975',), ('2002',), ('1995',), ('2016',), ('1986',), ('2012',), ('1980',), ('2005',), ('2003',), ('2000',), ('1964',), ('2013',), ('2009',), ('1965',), ('1972',), ('1988',), ('2006',), ('1979',), ('2013',), ('2006',), ('1968',), ('2008',), ('2008',), ('1985',), ('2008',), ('2013',), ('1984',), ('2001',), ('1981',), ('1997',), ('2015',), ('1965',), ('1963',), ('2002',), ('2004',), ('2003',), ('1979',), ('1981',), ('1989',), ('2014',), ('2010',), ('2000',), ('1992',), ('2001',), ('1960',), ('1999',), ('2012',), ('2005',), ('1977',), ('1969',), ('2003',), ('2003',), ('2010',), ('1994',), ('2004',), ('2008',), ('1962',), ('1986',), ('1971',), ('1984',), ('1999',), ('1988',), ('2010',), ('1974',), ('2013',), ('1975',), ('1981',), ('1964',), ('1971',), ('1969',), ('2008',), ('1973',), ('2000',), ('1986',), ('2012',), ('1980',), ('2001',), ('1989',), ('1990',), ('2007',), ('2009',), ('1994',), ('2013',), ('1961',), ('1957',), ('1992',), ('1965',), ('1982',), ('2017',), ('1957',), ('2005',), ('2009',), ('2011',), ('1955',), ('2009',), ('1985',), ('2013',), ('1978',), ('1996',), ('1976',), ('2011',), ('1948',), ('2015',), ('IV 2011',), ('2009',), ('1971',), ('1977',), ('I 2011',), ('2014',), ('1967',), ('2009',), ('2003',), ('1991',), ('1992',), ('II 1983',), ('1998',), ('1995',), ('1997',), ('2009',), ('1993',), ('2008',), ('2006',), ('1957',), ('2005',), ('2007',), ('1975',), ('1995',), ('1974',), ('2014',), ('2005',), ('1972',), ('2006',), ('1990',), ('1972',), ('2005',), ('2008',), ('2002',), ('2004',), ('1960',), ('1974',), ('1996',), ('1998',), ('1951',), ('1998',), ('2004',), ('2005',), ('1974',), ('2009',), ('2008',), ('1997',), ('1957',), ('2013',), ('1998',), ('1995',), ('1964',), ('1967',), ('2016',), ('1999',), ('1992',), ('1978',), ('1968',), ('2010',), ('2011',), ('2000',), ('2004',), ('2004',), ('2009',), ('2011',), ('1992',), ('2006',), ('IV 2010',), ('1995',), ('2011',), ('2003',), ('2010',), ('2002',), ('1984',), ('2003',), ('1983',), ('1995',), ('1992',), ('2010',), ('2005',), ('2012',), ('2008',), ('2007',), ('2014',), ('2002',), ('I 2001',), ('1988',), ('1979',), ('2003',), ('2010',), ('1970',), ('2003',), ('1979',), ('I 2009',), ('1982',), ('1973',), ('1978',), ('II 2011',), ('1991',), ('2015',), ('1994',), ('2016',), ('2013',), ('2000',), ('1970',), ('1993',), ('1987',), ('2004',), ('2011',), ('1985',), ('1998',), ('1987',), ('1987',), ('1981',), ('1997',), ('1997',), ('1982',), ('2005',), ('2003',), ('2003',), ('1973',), ('2006',), ('1981',), ('1986',), ('2004',), ('2005',), ('2014',), ('2015',), ('1995',), ('1991',), ('1984',), ('1971',), ('1997',), ('1977',), ('1995',), ('IV 2017',), ('2015',), ('2011',), ('1994',), ('2018',), ('2007',), ('1974',), ('1995',), ('1976',), ('1993',), ('1998',), ('1988',), ('1970',), ('2003',), ('2011',), ('2014',), ('1963',), ('1983',), ('2008',), ('2009',), ('2005',), ('2007',), ('1972',), ('2004',), ('2005',), ('1990',), ('1979',), ('2014',), ('2013',), ('1977',), ('2001',), ('2001',), ('2000',), ('1943',), ('2007',), ('2011',), ('1984',), ('1958',), ('2004',), ('1984',), ('2002',), ('1984',), ('2000',), ('2010',), ('1978',), ('1989',), ('1967',), ('1989',), ('1956',), ('2001',), ('2004',), ('2003',), ('1973',), ('1991',), ('1966',), ('2013',), ('2002',), ('2006',), ('2012',), ('1983',), ('1996',), ('2009',), ('1979',), ('1976',), ('2012',), ('2005',), ('2009',), ('2008',), ('2001',), ('2009',), ('1936',), ('2013',), ('2003',), ('1999',), ('1988',), ('1980',), ('2012',), ('2000',), ('1986',), ('1955',), ('2015',), ('2009',), ('1965',), ('2017',), ('2007',), ('1993',), ('2002',), ('1990',), ('1960',), ('1970',), ('1984',), ('2004',), ('1974',), ('2007',), ('1969',), ('1972',), ('2011',), ('2001',), ('2010',), ('I 2007',), ('2007',), ('1974',), ('1999',), ('1936',), ('1982',), ('1993',), ('1998',), ('1988',), ('I 1989',), ('2016',), ('2006',), ('1993',), ('2005',), ('2011',), ('1994',), ('1979',), ('1990',), ('2005',), ('2011',), ('2005',), ('1950',), ('2009',), ('2004',), ('1992',), ('2009',), ('2002',), ('2008',), ('1985',), ('I 1969',), ('1994',), ('2003',), ('2012',), ('1957',), ('2004',), ('1996',), ('2000',), ('2003',), ('2003',), ('2016',), ('2009',), ('2015',), ('1992',), ('2015',), ('2006',), ('2002',), ('2011',), ('1988',), ('1985',), ('2012',), ('2002',), ('1997',), ('1957',), ('2010',), ('1999',), ('2010',), ('2006',), ('2007',), ('2013',), ('2012',), ('2017',), ('2015',), ('1974',), ('1954',), ('2015',), ('1955',), ('2018',), ('1992',), ('1982',), ('2007',), ('2005',), ('1993',), ('2011',), ('1992',), ('1973',), ('1992',), ('1991',), ('2008',), ('2004',), ('1968',), ('1948',), ('1984',), ('1953',), ('I 2010',), ('1999',), ('1977',), ('1996',), ('1956',), ('1997',), ('1993',), ('1963',), ('2007',), ('1984',), ('I 2009',), ('1977',), ('2005',), ('2005',), ('1973',), ('1950',), ('1973',), ('1996',), ('1971',), ('1987',), ('1991',), ('1986',), ('2012',), ('2006',), ('1991',), ('2013',), ('2007',), ('1975',), ('1990',), ('1982',), ('1999',), ('2005',), ('1980',), ('2000',), ('1995',), ('2000',), ('2014',), ('2013',), ('2011',), ('1977',), ('1992',), ('1990',), ('1994',), ('I 1989',), ('2001',), ('1967',), ('2010',), ('1980',), ('1979',), ('2008',), ('2003',), ('1994',), ('2005',), ('1992',), ('2013',), ('2007',), ('1982',), ('2008',), ('2016',), ('1968',), ('2002',), ('1975',), ('2017',), ('1992',), ('2015',), ('2009',), ('1995',), ('1983',), ('I 2014',), ('2013',), ('2012',), ('2009',), ('2005',), ('2008',), ('2011',), ('2001',), ('2011',), ('2006',), ('2006',), ('2008',), ('1979',), ('2011',), ('1952',), ('2001',), ('1986',), ('1957',), ('1984',), ('1992',), ('1990',), ('2005',), ('2011',), ('2010',), ('2001',), ('2007',), ('1985',), ('2010',), ('2003',), ('1987',), ('2009',), ('2017',), ('2015',), ('2006',), ('2014',), ('2007',), ('1975',), ('1994',), ('1988',), ('2002',), ('1995',), ('1966',), ('2004',), ('2004',), ('1983',), ('2010',), ('2001',), ('2011',), ('2009',), ('1983',), ('2004',), ('2010',), ('2001',), ('2009',), ('1968',), ('2012',), ('2002',), ('1978',), ('2009',), ('1985',), ('2006',), ('1962',), ('2005',), ('2009',), ('1993',), ('1970',), ('2005',), ('1996',), ('1997',), ('2012',), ('1979',), ('2003',), ('1991',), ('2001',), ('2011',), ('1960',), ('1989',), ('2013',), ('1990',), ('2011',), ('2010',), ('2007',), ('1980',), ('2013',), ('2004',), ('1981',), ('2007',), ('2002',), ('1970',), ('1999',), ('1985',), ('1990',), ('1995',), ('1986',), ('2007',), ('2008',), ('2004',), ('2004',), ('1996',), ('2010',), ('1979',), ('2015',), ('2007',), ('2000',), ('2005',), ('2006',), ('1947',), ('2011',), ('1946',), ('2000',), ('1976',), ('2012',), ('1984',), ('1992',), ('1984',), ('1991',), ('2007',), ('1993',), ('1992',), ('1998',), ('1987',), ('2005',), ('1989',), ('2014',), ('2003',), ('2004',), ('2007',), ('1973',), ('2008',), ('2003',), ('II 2009',), ('1984',), ('1991',), ('1992',), ('1999',), ('1993',), ('1999',), ('2012',), ('2004',), ('2002',), ('2004',), ('1987',), ('2009',), ('2008',), ('2010',), ('1997',), ('1998',), ('2009',), ('2011',), ('2011',), ('2005',), ('1996',), ('2008',), ('2012',), ('2004',), ('1981',), ('2010',), ('2011',), ('1997',), ('1992',), ('1991',), ('I 1992',), ('1978',), ('2002',), ('1996',), ('1981',), ('2006',), ('1989',), ('2011',), ('1988',), ('1997',), ('1985',), ('1999',), ('2009',), ('2003',), ('1958',), ('2013',), ('2000',), ('2000',), ('1990',), ('1968',), ('1995',), ('1985',), ('1997',), ('2003',), ('2001',), ('2003',), ('2009',), ('1963',), ('2004',), ('2003',), ('2004',), ('1982',), ('2014',), ('1989',), ('1995',), ('1995',), ('1985',), ('1997',), ('1991',), ('1989',), ('2008',), ('2011',), ('2012',), ('2010',), ('2011',), ('2014',), ('2008',), ('2009',), ('2009',), ('2005',), ('2006',), ('2008',), ('2007',), ('1972',), ('1987',), ('2006',), ('2005',), ('1992',), ('2009',), ('2002',), ('I 2011',), ('1961',), ('1991',), ('1976',), ('2002',), ('2012',), ('2018',), ('2010',), ('1981',), ('2005',), ('2008',), ('1990',), ('1968',), ('2009',), ('1998',), ('2005',), ('1995',), ('2006',), ('1993',), ('1998',), ('1996',), ('1983',), ('1995',), ('1975',), ('1986',), ('1993',), ('2006',), ('1939',), ('1994',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Sentences we want sentence embeddings for\n",
        "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = transformers.AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Tokenize sentences\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Check if \"where\" is present in the input sentence\n",
        "where_present = \"where\" in tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
        "print(f\"Where present in input sentence: {where_present}\")\n",
        "\n",
        "# Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# Perform pooling\n",
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "# Normalize embeddings\n",
        "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "\n",
        "print(\"Sentence embeddings:\")\n",
        "print(sentence_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knwmJPmrDW4P",
        "outputId": "1b089e2b-56ec-4f2b-ea86-8587412a5e7c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Where present in input sentence: False\n",
            "Sentence embeddings:\n",
            "tensor([[ 6.7657e-02,  6.3496e-02,  4.8713e-02,  7.9305e-02,  3.7448e-02,\n",
            "          2.6528e-03,  3.9375e-02, -7.0985e-03,  5.9361e-02,  3.1537e-02,\n",
            "          6.0098e-02, -5.2905e-02,  4.0607e-02, -2.5931e-02,  2.9843e-02,\n",
            "          1.1269e-03,  7.3515e-02, -5.0382e-02, -1.2239e-01,  2.3703e-02,\n",
            "          2.9727e-02,  4.2477e-02,  2.5634e-02,  1.9952e-03, -5.6919e-02,\n",
            "         -2.7160e-02, -3.2904e-02,  6.6025e-02,  1.1901e-01, -4.5879e-02,\n",
            "         -7.2621e-02, -3.2584e-02,  5.2341e-02,  4.5055e-02,  8.2531e-03,\n",
            "          3.6702e-02, -1.3942e-02,  6.5392e-02, -2.6427e-02,  2.0641e-04,\n",
            "         -1.3664e-02, -3.6281e-02, -1.9504e-02, -2.8974e-02,  3.9427e-02,\n",
            "         -8.8409e-02,  2.6243e-03,  1.3671e-02,  4.8306e-02, -3.1157e-02,\n",
            "         -1.1733e-01, -5.1169e-02, -8.8529e-02, -2.1896e-02,  1.4299e-02,\n",
            "          4.4417e-02, -1.3482e-02,  7.4339e-02,  2.6638e-02, -1.9876e-02,\n",
            "          1.7919e-02, -1.0605e-02, -9.0426e-02,  2.1327e-02,  1.4120e-01,\n",
            "         -6.4717e-03, -1.4038e-03, -1.5361e-02, -8.7357e-02,  7.2217e-02,\n",
            "          2.0140e-02,  4.2559e-02, -3.4901e-02,  3.1951e-04, -8.0297e-02,\n",
            "         -3.2747e-02,  2.8527e-02, -5.1366e-02,  1.0939e-01,  8.1933e-02,\n",
            "         -9.8404e-02, -9.3410e-02, -1.5129e-02,  4.5125e-02,  4.9417e-02,\n",
            "         -2.5187e-02,  1.5708e-02, -1.2929e-01,  5.3189e-03,  4.0234e-03,\n",
            "         -2.3457e-02, -6.7298e-02,  2.9228e-02, -2.6085e-02,  1.3063e-02,\n",
            "         -3.1166e-02, -4.8271e-02, -5.5886e-02, -3.8750e-02,  1.2001e-01,\n",
            "         -1.0392e-02,  4.8970e-02,  5.5354e-02,  4.4936e-02, -4.0098e-03,\n",
            "         -1.0296e-01, -2.9297e-02, -5.8340e-02,  2.7047e-02, -2.2017e-02,\n",
            "         -7.2224e-02, -4.1387e-02, -1.9330e-02,  2.7333e-03,  2.7700e-04,\n",
            "         -9.6759e-02, -1.0057e-01, -1.4192e-02, -8.0789e-02,  4.5392e-02,\n",
            "          2.4504e-02,  5.9761e-02, -7.3818e-02,  1.1984e-02, -6.6340e-02,\n",
            "         -7.6905e-02,  3.8516e-02, -5.5936e-33,  2.8001e-02, -5.6078e-02,\n",
            "         -4.8660e-02,  2.1557e-02,  6.0198e-02, -4.8140e-02, -3.5025e-02,\n",
            "          1.9331e-02, -1.7515e-02, -3.8921e-02, -3.8107e-03, -1.7029e-02,\n",
            "          2.8210e-02,  1.2829e-02,  4.7160e-02,  6.2103e-02, -6.4359e-02,\n",
            "          1.2929e-01, -1.3123e-02,  5.2307e-02, -3.7368e-02,  2.8909e-02,\n",
            "         -1.6898e-02, -2.3733e-02, -3.3349e-02, -5.1676e-02,  1.5536e-02,\n",
            "          2.0880e-02, -1.2537e-02,  4.5958e-02,  3.7272e-02,  2.8057e-02,\n",
            "         -5.9001e-02, -1.1699e-02,  4.9218e-02,  4.7033e-02,  7.3549e-02,\n",
            "         -3.7053e-02,  3.9846e-03,  1.0641e-02, -1.6148e-04, -5.2717e-02,\n",
            "          2.7593e-02, -3.9292e-02,  8.4472e-02,  4.8686e-02, -4.8587e-03,\n",
            "          1.7995e-02, -4.2857e-02,  1.2338e-02,  6.3995e-03,  4.0482e-02,\n",
            "          1.4889e-02, -1.5394e-02,  7.6295e-02,  2.3704e-02,  4.4524e-02,\n",
            "          5.0820e-02, -2.3125e-03, -1.8874e-02, -1.2334e-02,  4.6600e-02,\n",
            "         -5.6344e-02,  6.2993e-02, -3.1554e-02,  3.2491e-02,  2.3467e-02,\n",
            "         -6.5544e-02,  2.0171e-02,  2.5708e-02, -1.2387e-02, -8.3649e-03,\n",
            "         -6.6438e-02,  9.4307e-02, -3.5709e-02, -3.4248e-02, -6.6636e-03,\n",
            "         -8.0153e-03, -3.0971e-02,  4.3301e-02, -8.2140e-03, -1.5079e-01,\n",
            "          3.0769e-02,  4.0072e-02, -3.7929e-02,  1.9322e-03,  4.0053e-02,\n",
            "         -8.7708e-02, -3.6849e-02,  8.5796e-03, -3.1925e-02, -1.2526e-02,\n",
            "          7.3554e-02,  1.3473e-03,  2.0592e-02,  2.7110e-33, -5.1858e-02,\n",
            "          5.7836e-02, -9.1898e-02,  3.9442e-02,  1.0558e-01, -1.9691e-02,\n",
            "          6.1840e-02, -7.6347e-02,  2.4088e-02,  9.4005e-02, -1.1654e-01,\n",
            "          3.7120e-02,  5.2243e-02, -3.9586e-03,  5.7221e-02,  5.3285e-03,\n",
            "          1.2402e-01,  1.3902e-02, -1.1025e-02,  3.5605e-02, -3.3075e-02,\n",
            "          8.1657e-02, -1.5200e-02,  6.0559e-02, -6.0140e-02,  3.2610e-02,\n",
            "         -3.4830e-02, -1.6988e-02, -9.7491e-02, -2.7148e-02,  1.7471e-03,\n",
            "         -7.6898e-02, -4.3186e-02, -1.8998e-02, -2.9166e-02,  5.7749e-02,\n",
            "          2.4182e-02, -1.1690e-02, -6.2144e-02,  2.8435e-02, -2.3752e-04,\n",
            "         -2.5178e-02,  4.3964e-03,  8.1284e-02,  3.6418e-02, -6.0401e-02,\n",
            "         -3.6552e-02, -7.9375e-02, -5.0853e-03,  6.6970e-02, -1.1778e-01,\n",
            "          3.2374e-02, -4.7125e-02, -1.3446e-02, -9.4844e-02,  8.2496e-03,\n",
            "         -1.0675e-02, -6.8188e-02,  1.1182e-03,  2.4802e-02, -6.3589e-02,\n",
            "          2.8449e-02, -2.6130e-02,  8.5811e-02,  1.1468e-01, -5.3535e-02,\n",
            "         -5.6359e-02,  4.2601e-02,  1.0945e-02,  2.0958e-02,  1.0013e-01,\n",
            "          3.2605e-02, -1.8421e-01, -3.9321e-02, -6.9145e-02, -6.3810e-02,\n",
            "         -6.5639e-02, -6.4125e-03, -4.7961e-02, -7.6813e-02,  2.9538e-02,\n",
            "         -2.2995e-02,  4.1704e-02, -2.5005e-02, -4.5450e-03, -4.1714e-02,\n",
            "         -1.3229e-02, -6.3836e-02, -2.4647e-03, -1.3734e-02,  1.6898e-02,\n",
            "         -6.3040e-02,  8.9888e-02,  4.1817e-02, -1.8569e-02, -1.8044e-08,\n",
            "         -1.6800e-02, -3.2158e-02,  6.3038e-02, -4.1309e-02,  4.4482e-02,\n",
            "          2.0246e-03,  6.2959e-02, -5.1737e-03, -1.0044e-02, -3.0564e-02,\n",
            "          3.5267e-02,  5.5858e-02, -4.6712e-02,  3.4510e-02,  3.2958e-02,\n",
            "          4.3011e-02,  2.9436e-02, -3.0316e-02, -1.7111e-02,  7.3748e-02,\n",
            "         -5.4791e-02,  2.7752e-02,  6.2017e-03,  1.5880e-02,  3.4298e-02,\n",
            "         -5.1575e-03,  2.3508e-02,  7.5314e-02,  1.9284e-02,  3.3620e-02,\n",
            "          5.0910e-02,  1.5250e-01,  1.6421e-02,  2.7053e-02,  3.7516e-02,\n",
            "          2.1855e-02,  5.6633e-02, -3.9575e-02,  7.1231e-02, -5.4138e-02,\n",
            "          1.0376e-03,  2.1185e-02, -3.5631e-02,  1.0902e-01,  2.7653e-03,\n",
            "          3.1400e-02,  1.3842e-03, -3.4574e-02, -4.5928e-02,  2.8808e-02,\n",
            "          7.1690e-03,  4.8469e-02,  2.6102e-02, -9.4407e-03,  2.8217e-02,\n",
            "          3.4872e-02,  3.6910e-02, -8.5895e-03, -3.5321e-02, -2.4786e-02,\n",
            "         -1.9192e-02,  3.8071e-02,  5.9965e-02, -4.2229e-02],\n",
            "        [ 8.6439e-02,  1.0276e-01,  5.3946e-03,  2.0445e-03, -9.9633e-03,\n",
            "          2.5385e-02,  4.9288e-02, -3.0627e-02,  6.8725e-02,  1.0137e-02,\n",
            "          7.7540e-02, -9.0081e-02,  6.1062e-03, -5.6990e-02,  1.4171e-02,\n",
            "          2.8049e-02, -8.6846e-02,  7.6440e-02, -1.0349e-01, -6.7744e-02,\n",
            "          6.9995e-02,  8.4425e-02, -7.2491e-03,  1.0477e-02,  1.3402e-02,\n",
            "          6.7758e-02, -9.4209e-02, -3.7169e-02,  5.2262e-02, -3.1085e-02,\n",
            "         -9.6341e-02,  1.5772e-02,  2.5787e-02,  7.8525e-02,  7.8995e-02,\n",
            "          1.9152e-02,  1.6436e-02,  3.1009e-03,  3.8131e-02,  2.3709e-02,\n",
            "          1.0539e-02, -4.4064e-02,  4.4174e-02, -2.5873e-02,  6.1538e-02,\n",
            "         -4.0543e-02, -8.6414e-02,  3.1972e-02, -8.9065e-04, -2.4444e-02,\n",
            "         -9.1972e-02,  2.3394e-02, -8.3029e-02,  4.4151e-02, -2.4969e-02,\n",
            "          6.2302e-02, -1.3035e-03,  7.5140e-02,  2.4639e-02, -6.4724e-02,\n",
            "         -1.1773e-01,  3.8339e-02, -9.1177e-02,  6.3545e-02,  7.6274e-02,\n",
            "         -8.8024e-02,  9.5456e-03, -4.6972e-02, -8.4174e-02,  3.8882e-02,\n",
            "         -1.1439e-01,  6.2886e-03, -3.4936e-02,  2.3975e-02, -3.3132e-02,\n",
            "         -1.5724e-02, -3.7896e-02, -8.8125e-03,  7.0612e-02,  3.2807e-02,\n",
            "          2.0367e-03, -1.1228e-01,  6.7972e-03,  1.2277e-02,  3.3530e-02,\n",
            "         -1.3620e-02, -2.2549e-02, -2.2523e-02, -2.0319e-02,  5.0430e-02,\n",
            "         -7.4865e-02, -8.2282e-02,  7.6596e-02,  4.9339e-02, -3.7555e-02,\n",
            "          1.4463e-02, -5.7246e-02, -1.7995e-02,  1.0970e-01,  1.1946e-01,\n",
            "          8.0925e-04,  6.1706e-02,  3.2632e-02, -1.3078e-01, -1.4864e-01,\n",
            "         -6.1623e-02,  4.3389e-02,  2.6713e-02,  1.3979e-02, -3.9400e-02,\n",
            "         -2.5271e-02,  3.8774e-03,  3.5866e-02, -6.1542e-02,  3.7666e-02,\n",
            "          2.6757e-02, -3.8266e-02, -3.5479e-02, -2.3923e-02,  8.6798e-02,\n",
            "         -1.8406e-02,  7.7104e-02,  1.3987e-03,  7.0038e-02, -4.7788e-02,\n",
            "         -7.8982e-02,  5.1081e-02, -2.9987e-33, -3.9165e-02, -2.5621e-03,\n",
            "          1.6521e-02,  9.4894e-03, -5.6622e-02,  6.5778e-02, -4.7700e-02,\n",
            "          1.1166e-02, -5.7356e-02, -9.1626e-03, -2.1752e-02, -5.5953e-02,\n",
            "         -1.1142e-02,  9.3279e-02,  1.6677e-02, -1.3672e-02,  4.3439e-02,\n",
            "          1.8724e-03,  7.2995e-03,  5.1633e-02,  4.8061e-02,  1.3534e-01,\n",
            "         -1.7174e-02, -1.2970e-02, -7.5011e-02,  2.6111e-02,  2.6980e-02,\n",
            "          7.8306e-04, -4.8727e-02,  1.1784e-02, -4.5958e-02, -4.8321e-02,\n",
            "         -1.9567e-02,  1.9389e-02,  1.9881e-02,  1.6743e-02,  9.8780e-02,\n",
            "         -2.7409e-02,  2.3481e-02,  3.7023e-03, -6.1451e-02, -1.2123e-03,\n",
            "         -9.5047e-03,  9.2515e-03,  2.3844e-02,  8.6123e-02,  2.2679e-02,\n",
            "          5.4512e-04,  3.4713e-02,  6.2546e-03, -6.9278e-03,  3.9240e-02,\n",
            "          1.1567e-02,  3.2628e-02,  6.2216e-02,  2.7611e-02,  1.8688e-02,\n",
            "          3.5581e-02,  4.1180e-02,  1.5478e-02,  4.2269e-02,  3.8225e-02,\n",
            "          1.0031e-02, -2.8325e-02,  4.4705e-02, -4.1046e-02, -4.5055e-03,\n",
            "         -5.4473e-02,  2.6232e-02,  1.7986e-02, -1.2312e-01, -4.6695e-02,\n",
            "         -1.3591e-02,  6.4671e-02,  3.5735e-03, -1.2223e-02, -1.7938e-02,\n",
            "         -2.5550e-02,  2.3722e-02,  4.0866e-03, -6.5148e-02,  4.4365e-02,\n",
            "          4.6860e-02, -3.2517e-02,  4.0226e-03, -3.9761e-03,  1.1194e-02,\n",
            "         -9.9560e-02,  3.3317e-02,  8.0106e-02,  9.4269e-02, -6.3829e-02,\n",
            "          3.2315e-02, -5.1355e-02, -7.4988e-03,  5.3005e-34, -4.1320e-02,\n",
            "          9.4965e-02, -1.0640e-01,  4.9659e-02, -3.4191e-02, -3.1675e-02,\n",
            "         -1.7156e-02,  1.7010e-03,  5.7976e-02, -1.2178e-03, -1.6854e-02,\n",
            "         -5.1691e-02,  5.5300e-02, -3.4265e-02,  3.0818e-02, -3.1048e-02,\n",
            "          9.2753e-02,  3.7266e-02, -2.3740e-02,  4.4589e-02,  1.4615e-02,\n",
            "          1.1624e-01, -5.0011e-02,  3.8872e-02,  4.2474e-03,  2.5698e-02,\n",
            "          3.2724e-02,  4.2991e-02, -1.3614e-02,  2.5612e-02,  1.0626e-02,\n",
            "         -8.4686e-02, -9.5298e-02,  1.0840e-01, -7.5160e-02, -1.3777e-02,\n",
            "          6.3734e-02, -4.4967e-03, -3.2532e-02,  6.2361e-02,  3.4805e-02,\n",
            "         -3.5492e-02, -2.0022e-02,  3.6661e-02, -2.4884e-02,  1.0182e-02,\n",
            "         -7.0123e-02, -4.3195e-02,  2.9533e-02, -2.9490e-04, -3.4539e-02,\n",
            "          1.4668e-02, -9.8397e-02, -4.7049e-02, -8.8550e-03, -8.8991e-02,\n",
            "          3.5100e-02, -1.2960e-01, -4.9887e-02, -6.1205e-02, -5.9780e-02,\n",
            "          9.4632e-03,  4.9122e-02, -7.7503e-02,  8.0973e-02, -4.7926e-02,\n",
            "          2.3438e-03,  7.5703e-02, -2.4018e-02, -1.5255e-02,  4.8674e-02,\n",
            "         -3.8597e-02, -7.0483e-02, -1.2035e-02, -3.8879e-02, -7.7602e-02,\n",
            "         -1.0724e-02,  1.0419e-02, -2.1375e-02, -9.1739e-02, -1.1134e-02,\n",
            "         -2.9607e-02,  2.4646e-02,  4.6571e-03, -1.6345e-02, -3.9522e-02,\n",
            "          7.7337e-02, -2.8473e-02, -3.6994e-03,  8.2767e-02, -1.1041e-02,\n",
            "          3.1398e-02,  5.3509e-02,  5.7515e-02, -3.1762e-02, -1.5291e-08,\n",
            "         -7.9966e-02, -4.7680e-02, -8.5979e-02,  5.6962e-02, -4.0887e-02,\n",
            "          2.2383e-02, -4.6445e-03, -3.8013e-02, -3.1067e-02, -1.0728e-02,\n",
            "          1.9770e-02,  7.7700e-03, -6.0947e-03, -3.8638e-02,  2.8027e-02,\n",
            "          6.7814e-02, -2.3535e-02,  3.2175e-02,  8.0254e-03, -2.3911e-02,\n",
            "         -1.2200e-03,  3.1460e-02, -5.2492e-02, -8.0682e-03,  3.1478e-03,\n",
            "          5.1150e-02, -4.4410e-02,  6.3601e-02,  3.8508e-02,  3.3043e-02,\n",
            "         -4.1873e-03,  4.9559e-02, -5.6961e-02, -6.4971e-03, -2.4979e-02,\n",
            "         -1.6087e-02,  6.6229e-02, -2.0631e-02,  1.0805e-01,  1.6855e-02,\n",
            "          1.4381e-02, -1.3213e-02, -1.2939e-01,  6.9522e-02, -5.5577e-02,\n",
            "         -6.7541e-02, -5.4582e-03, -6.1359e-03,  3.9084e-02, -6.2878e-02,\n",
            "          3.7406e-02, -1.1657e-02,  1.2915e-02, -5.5250e-02,  5.1608e-02,\n",
            "         -4.3084e-03,  5.8025e-02,  1.8694e-02,  2.2781e-02,  3.2167e-02,\n",
            "          5.3798e-02,  7.0285e-02,  7.4931e-02, -8.4178e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")  # Load the English language model\n",
        "\n",
        "def find_where_clause(sentence):\n",
        "    doc = nlp(sentence)  # Parse the sentence using the NLP model\n",
        "    \n",
        "    # Iterate over the tokens in the document\n",
        "    for token in doc:\n",
        "        # Check if the token is a preposition and its lemma is \"where\"\n",
        "        if token.pos_ == \"ADP\" and token.lemma_ == \"where\":\n",
        "            # Find the noun phrase following the \"where\" preposition\n",
        "            np = None\n",
        "            for child in token.children:\n",
        "                if child.pos_ == \"NOUN\" or child.pos_ == \"PROPN\":\n",
        "                    np = child\n",
        "                    break\n",
        "            if np:\n",
        "                # Return the noun phrase as the WHERE clause\n",
        "                return np.text\n",
        "    # If no \"where\" preposition was found, return None\n",
        "    return None\n",
        "\n",
        "# Test the function\n",
        "sentence = \"Show me the products where the price is greater than $100\"\n",
        "where_clause = find_where_clause(sentence)\n",
        "print(where_clause)  # Output: \"the price is greater than $100\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "q9LkEaWjE7QW",
        "outputId": "2e23d4e6-f422-460b-8c6a-601a391f9b3c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-c7f82c7e38f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_md\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load the English language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_where_clause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Tokenize the input sentence\n",
        "sentence = \"Show me the products where the price is greater than $100\"\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "\n",
        "# Pass the input through the model to get the output\n",
        "output = model(input_ids)[0]\n",
        "\n",
        "# Find the index of the \"WHERE\" token\n",
        "where_index = tokenizer.encode([\"WHERE\"]).index(\"WHERE\")\n",
        "\n",
        "# Get the output for the \"WHERE\" token\n",
        "where_output = output[0][where_index]\n",
        "\n",
        "# Check if the output for the \"WHERE\" token is related to the semantic meaning of \"where\" in a SQL clause\n",
        "if where_output.sum() > 0:\n",
        "    print(\"The sentence contains a WHERE clause\")\n",
        "else:\n",
        "    print(\"The sentence does not contain a WHERE clause\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "fhE8QnezGCZm",
        "outputId": "b198adff-4945-446a-b5dc-85902fafa77e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d575c44fa7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Find the index of the \"WHERE\" token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mwhere_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"WHERE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WHERE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Get the output for the \"WHERE\" token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'WHERE' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load a pre-trained language model\n",
        "model = transformers.AutoModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "def find_where_clause(sentence):\n",
        "  # Tokenize the input sentence using the language model's tokenizer\n",
        "  input_ids = model.tokenizer.encode(sentence, return_tensors='pt').squeeze(0)\n",
        "\n",
        "  # Pass the input through the language model to get the hidden states\n",
        "  hidden_states, _ = model(input_ids)\n",
        "\n",
        "  # Get the last hidden state of the last token in the input\n",
        "  last_hidden_state = hidden_states[-1][-1]\n",
        "\n",
        "  # Use the last hidden state to predict whether the sentence contains a WHERE clause\n",
        "  where_prediction = model.classifier(last_hidden_state)\n",
        "\n",
        "  # Check if the prediction is positive (i.e. the sentence contains a WHERE clause)\n",
        "  if where_prediction > 0.5:\n",
        "    print(\"WHERE present\")\n",
        "  else:\n",
        "    print(\"WHERE is not present\")\n",
        "\n",
        "# Test the function on some sample sentences\n",
        "find_where_clause(\"what are the names of movies?\")\n",
        "find_where_clause(\"what are the names of movies released in year 2018?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "RhiZ4quLGdiH",
        "outputId": "2407973b-d97c-4486-e090-499447134027"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-cb0edaa0be0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Test the function on some sample sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfind_where_clause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what are the names of movies?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mfind_where_clause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what are the names of movies released in year 2018?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-cb0edaa0be0b>\u001b[0m in \u001b[0;36mfind_where_clause\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_where_clause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Tokenize the input sentence using the language model's tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Pass the input through the language model to get the hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Load a pre-trained transformer model\n",
        "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def find_where_clause(sentence: str):\n",
        "  # Tokenize the sentence and encode it using the transformer model\n",
        "  inputs = transformers.BertTokenizer.from_pretrained('bert-base-uncased').encode_plus(\n",
        "      sentence,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  # Pass the encoded sentence through the transformer model to get the hidden states\n",
        "  hidden_states = model(**inputs)[2]\n",
        "\n",
        "  # The hidden state for the first token of the first attention head for the first layer\n",
        "  # will be used to determine the WHERE clause\n",
        "  where_clause_hidden_state = hidden_states[0][0][0]\n",
        "\n",
        "  # Use a simple linear layer to classify the hidden state as either \"WHERE present\" or \"WHERE not present\"\n",
        "  linear_layer = torch.nn.Linear(where_clause_hidden_state.size(-1), 2)\n",
        "  where_clause_prediction = linear_layer(where_clause_hidden_state).argmax().item()\n",
        "\n",
        "  if where_clause_prediction == 0:\n",
        "    return \"WHERE not present\"\n",
        "  else:\n",
        "    return \"WHERE present\"\n",
        "\n",
        "# Test the function\n",
        "sentence1 = \"what are the names of movies?\"\n",
        "print(find_where_clause(sentence1))  # Output: WHERE not present\n",
        "\n",
        "sentence2 = \"what are the names of movies released in year 2018?\"\n",
        "print(find_where_clause(sentence2))  # Output: WHERE present\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580,
          "referenced_widgets": [
            "5030bc8118c743958f32a9e76336ffc2",
            "d84011a8b7594ca694ff37e8f64d78a1",
            "7b1a4963110a496eb61a362ae1b766a0",
            "4be47353540a4c6e841c15194a0a8ae2",
            "354a3e36bac540bdb01a6ce04e3ade14",
            "8bc5d3ba03834bdabe4d9f9be532adf3",
            "364c0595dc9f49d0b62f741c319341a1",
            "7f54c7938e5b4ee68332d3f4319f496b",
            "a752ea9e529045089a470ac979c1f1c4",
            "13ee1c54cd3d440fae2d7c801541cbd4",
            "9504af680c434ad3b17b66b62006fc5f",
            "7a6ea9327b634858b7f043d77e983458",
            "15288bc0ff064f849d6bb491577dee49",
            "8d3f5b9b569249bba284f2776710cd9d",
            "bc4d16a7a011449d84a8c5a75a001db8",
            "a35653cfb8ec47e3ac4899d6fd6ba3b3",
            "cb09f2523ae242948019346dfed16228",
            "083ccbaf1fdf47ff95638e7c7e61ff12",
            "b0d01a0cb3af4af8994f3477c3c6521a",
            "1b5d2ae421ae4d738ba47061577f7f1a",
            "8d45c9546a01470aa3e1964e2c280bef",
            "159d61fe1c5441ce98a012af1c178721",
            "ca85b8e80fb7429f8efdaedaf61806e8",
            "3cf274e14eaf4e35bf1c7c3a0e213eac",
            "c69862baf5d8435fae75762e90f14ecc",
            "12d5479b783c4d8b857a9e7a1341edbb",
            "1bffa87f2dfc4a4d971d40b5beca9d42",
            "103533b527bf46fbb1cbb18dac58e3fc",
            "cdfa0f406cb54f499975ca60911a6f7c",
            "cd755acb61fb4511aa30879193e6da5e",
            "8b81792cc8c84b13b87015252ab93e57",
            "e88cff068b2646f4b33046e8ccd32fbf",
            "97add7d7ffcf4183a346720a73237599",
            "24b99b821fa948d29d8e783c053f070a",
            "e3d627bc58c6437298b36cd277bda49f",
            "757a877b524e497883cc5a42f095ae17",
            "8f1bdabba0f84e429c356ef06583b539",
            "287df84a5882497abc96d6c82e1a7356",
            "35c866db67664840a2ec135fedbd2cac",
            "92e2db6ec2a944d683195299d07316e5",
            "89344cefc0af4d92b75900790f795820",
            "f4f6195cdc554e0ca60ab59a77352f3f",
            "1498fc88dbb442f5b0fa4d2d9f422e78",
            "c9dd0a4d000242ac9631d9d45caae116"
          ]
        },
        "id": "aIGHGV20Hauf",
        "outputId": "afcab77a-486a-455d-89b2-59eae0a7fed5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5030bc8118c743958f32a9e76336ffc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a6ea9327b634858b7f043d77e983458"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca85b8e80fb7429f8efdaedaf61806e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24b99b821fa948d29d8e783c053f070a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-70ab07faa47e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Test the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"what are the names of movies?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_where_clause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Output: WHERE not present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"what are the names of movies released in year 2018?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-70ab07faa47e>\u001b[0m in \u001b[0;36mfind_where_clause\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Pass the encoded sentence through the transformer model to get the hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# The hidden state for the first token of the first attention head for the first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUrGCvBAHbSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}